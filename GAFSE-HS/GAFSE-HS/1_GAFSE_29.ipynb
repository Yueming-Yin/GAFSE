{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "torch.manual_seed(8)\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ki_P29274_0.3333333333333333_300\n",
      "model_file/1_GAFSE_Ki_P29274_0.3333333333333333_300_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/Ki_P29274_0.3333333333333333_300_train.csv\"\n",
    "test_filename = \"./data/benchmark/Ki_P29274_0.3333333333333333_300_test.csv\"\n",
    "test_active = 300\n",
    "val_rate = 0.2\n",
    "random_seed = 68\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/1_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"1_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/1_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0              CN1C2C(C3=C1C=NC=C3)N(C(=O)N(C2=O)C)C -3.622214\n",
      "1          C1=COC(=C1)C2=NC3=C(C=C2C4=NC=NC=C4)NC=N3 -3.556905\n",
      "2  CC(C)(C)OC(=O)NCCNC1=NC2=NC(=NN2C(=N1)NCCNC(=O... -3.639486\n",
      "3  CCCN1C2=C(C(=O)N(C1=O)C)NC(=C2)C3=CC=C(C=C3)OC... -2.810115\n",
      "4  CC(C)(C)OC(=O)NCCOCCOCCNC1=NC2=NC(=NN2C(=N1)N)... -1.250420\n",
      "number of all smiles:  1017\n",
      "number of successfully processed smiles:  1017\n",
      "                                              smiles     value  \\\n",
      "0              CN1C2C(C3=C1C=NC=C3)N(C(=O)N(C2=O)C)C -3.622214   \n",
      "1          C1=COC(=C1)C2=NC3=C(C=C2C4=NC=NC=C4)NC=N3 -3.556905   \n",
      "2  CC(C)(C)OC(=O)NCCNC1=NC2=NC(=NN2C(=N1)NCCNC(=O... -3.639486   \n",
      "3  CCCN1C2=C(C(=O)N(C1=O)C)NC(=C2)C3=CC=C(C=C3)OC... -2.810115   \n",
      "4  CC(C)(C)OC(=O)NCCOCCOCCNC1=NC2=NC(=NN2C(=N1)N)... -1.250420   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0                   CN1C(=O)C2C(c3ccncc3N2C)N(C)C1=O  \n",
      "1                c1coc(-c2nc3nc[nH]c3cc2-c2ccncn2)c1  \n",
      "2  CC(C)(C)OC(=O)NCCNc1nc(NCCNC(=O)OC(C)(C)C)n2nc...  \n",
      "3  CCCn1c(=O)n(C)c(=O)c2[nH]c(-c3ccc(OCC(=O)N4CCN...  \n",
      "4  CC(C)(C)OC(=O)NCCOCCOCCNc1nc(N)n2nc(-c3ccco3)n...  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  1353\n",
      "number of successfully processed smiles:  1353\n",
      "(1353, 3)\n",
      "                                              smiles     value  \\\n",
      "0  CN1C2=C(C(=O)N(C1=O)C)NC(=C2)C3=CC=C(C=C3)OCC(... -3.009999   \n",
      "1   C1CCC(CC1)NC2=NC3=C(C=C2)C(=O)C=C(N3)C4=CC=CC=C4 -3.322219   \n",
      "2        C1CN(CC=C1)CC2=CC3=C(N=C(N=C3S2)C4=COC=N4)N -2.380211   \n",
      "3  C1=CC=C(C=C1)CC(=O)NC2=NC3=CC=CC=C3N4C2=NC(=N4... -2.200002   \n",
      "4           CCC1=C(C=C2C(=C1)C(=O)C(=CO2)C3=CSC=N3)O -3.199999   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  CN(Cc1ccccc1)C(=O)COc1ccc(-c2cc3c([nH]2)c(=O)n...  \n",
      "1          O=c1cc(-c2ccccc2)[nH]c2nc(NC3CCCCC3)ccc12  \n",
      "2               Nc1nc(-c2cocn2)nc2sc(CN3CC=CCC3)cc12  \n",
      "3       O=C(Cc1ccccc1)Nc1nc2ccccc2n2nc(-c3ccco3)nc12  \n",
      "4                    CCc1cc2c(=O)c(-c3cscn3)coc2cc1O  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/Ki_P29274_0.3333333333333333_300_train.pickle\n",
      "./data/benchmark/Ki_P29274_0.3333333333333333_300_train\n",
      "2370\n",
      "feature dicts file saved as ./data/benchmark/Ki_P29274_0.3333333333333333_300_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(814, 3) (203, 3) (1353, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    [d,e,f,g] = bond_neighbor.shape\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    one_hot_loss = 0\n",
    "    interger_loss = 0\n",
    "    binary_loss = 0\n",
    "    counter_i = 0\n",
    "    counter_j = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        one_hot_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-6)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)+\\\n",
    "                         weighted_CE_loss(atom_list[i,:l,16:22], x_atom[i,:l,16:22])+ \\\n",
    "                         weighted_CE_loss(atom_list[i,:l,24:30], x_atom[i,:l,24:30])+ \\\n",
    "                         weighted_CE_loss(atom_list[i,:l,31:36], x_atom[i,:l,31:36])\n",
    "        interger_loss += loss_function(atom_list[i,:l,23], x_atom[i,:l,23])+ \\\n",
    "                        loss_function(atom_list[i,:l,24], x_atom[i,:l,24])\n",
    "        binary_loss += CE(atom_list[i,:l,30], x_atom[i,:l,30])+ \\\n",
    "                        CE(atom_list[i,:l,36], x_atom[i,:l,36])+ \\\n",
    "                        CE(atom_list[i,:l,37], x_atom[i,:l,37])+ \\\n",
    "                        CE(atom_list[i,:l,38], x_atom[i,:l,38])\n",
    "        counter_i += 1\n",
    "        for j in range(l):\n",
    "            n = (bond_neighbor[i,j].sum(-1)!=0).sum(-1)\n",
    "            if n==0:\n",
    "                continue\n",
    "            one_hot_loss += weighted_CE_loss(bond_list[i,j,:n,:4], bond_neighbor[i,j,:n,:4])+ \\\n",
    "                             weighted_CE_loss(bond_list[i,j,:n,6:], bond_neighbor[i,j,:n,6:])\n",
    "            binary_loss += CE(bond_neighbor[i,j,:n,4], bond_list[i,j,:n,4])+ \\\n",
    "                           CE(bond_neighbor[i,j,:n,5], bond_list[i,j,:n,5])\n",
    "            counter_j += 1\n",
    "    one_hot_loss = one_hot_loss/(5*counter_i+2*counter_j)\n",
    "    interger_loss = interger_loss/(2*counter_i)\n",
    "    binary_loss = binary_loss/(4*counter_i+2*counter_j)\n",
    "    total_loss = (one_hot_loss + interger_loss + binary_loss)/3\n",
    "    return total_loss, one_hot_loss, interger_loss, binary_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "#         atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "#                                       torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "#         success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "#                             bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "#                                                      refer_atom_list, refer_bond_list,topn=1)\n",
    "#         reconstruction_loss, one_hot_loss, interger_loss,binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, \n",
    "#                                                                                               bond_neighbor, validity_mask, atom_list, \n",
    "#                                                                                               bond_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "#         atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "#                                                 torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "#                                                 mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "#         refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "#                                                             torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "#                                                             mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "#         success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "#                             bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "#                                                      refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "#         test_reconstruction_loss, test_one_hot_loss, test_interger_loss,test_binary_loss = generate_loss_function(atom_list_test, x_atom_test, bond_list_test, bond_neighbor_test, validity_mask_test, atom_list_test, bond_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        max_lr = 1e-3\n",
    "        conv_lr = conv_lr - conv_lr**2 + 0.06 * punish_lr\n",
    "        if conv_lr < max_lr and conv_lr >= 0:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = conv_lr.detach()\n",
    "                AFSE_lr = conv_lr    \n",
    "        elif conv_lr < 0:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = 0\n",
    "                AFSE_lr = 0\n",
    "        elif conv_lr >= max_lr:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = max_lr\n",
    "                AFSE_lr = max_lr\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_one_hot', one_hot_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_interger', interger_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_binary', binary_loss, global_step)\n",
    "        logger.add_scalar('lr/max_lr', conv_lr, global_step)\n",
    "        logger.add_scalar('lr/punish_lr', punish_lr, global_step)\n",
    "        logger.add_scalar('lr/AFSE_lr', AFSE_lr, global_step)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "#         optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6*(vat_loss + test_vat_loss) # + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "#         optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 10\n",
    "patience = 100\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/1_GAFSE_Ki_P29274_0.3333333333333333_300_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 135 Index:-0.9996 R2:0.0413 0.0466 0.0635 RMSE:1.1028 1.1506 1.1070 Tau:0.1454 0.1510 0.0783\n",
      "Epoch: 2 Step: 270 Index:-0.8977 R2:0.1060 0.0826 0.1317 RMSE:1.0675 1.1194 1.0762 Tau:0.2334 0.2217 0.0898\n",
      "Epoch: 3 Step: 405 Index:-0.8851 R2:0.1560 0.1011 0.1679 RMSE:1.0530 1.1183 1.0676 Tau:0.2811 0.2332 0.0948\n",
      "Epoch: 4 Step: 540 Index:-0.7891 R2:0.2200 0.1597 0.2675 RMSE:0.9999 1.0745 0.9946 Tau:0.3388 0.2854 0.1551\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 5 Step: 675 Index:-0.7921 R2:0.2813 0.1979 0.3150 RMSE:0.9828 1.0901 0.9700 Tau:0.3817 0.2980 0.1585\n",
      "Epoch: 6 Step: 810 Index:-0.7755 R2:0.3157 0.2408 0.3605 RMSE:0.9904 1.0951 0.9704 Tau:0.3967 0.3197 0.1796\n",
      "Epoch: 7 Step: 945 Index:-0.7674 R2:0.3156 0.2473 0.3645 RMSE:0.9857 1.0932 0.9655 Tau:0.3962 0.3258 0.1805\n",
      "Epoch: 8 Step: 1080 Index:-0.6908 R2:0.3336 0.2598 0.3780 RMSE:0.9236 1.0215 0.9077 Tau:0.4024 0.3307 0.1835\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 9 Step: 1215 Index:-0.7106 R2:0.3432 0.2763 0.3863 RMSE:0.9478 1.0499 0.9316 Tau:0.4123 0.3393 0.1893\n",
      "Epoch: 10 Step: 1350 Index:-0.6686 R2:0.3500 0.2728 0.3889 RMSE:0.9076 1.0071 0.8948 Tau:0.4150 0.3385 0.1881\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 11 Step: 1485 Index:-0.6721 R2:0.3544 0.2798 0.3890 RMSE:0.9034 1.0086 0.8928 Tau:0.4172 0.3366 0.1871\n",
      "Epoch: 12 Step: 1620 Index:-0.6336 R2:0.3679 0.2880 0.3996 RMSE:0.8997 0.9857 0.8913 Tau:0.4282 0.3521 0.1912\n",
      "Epoch: 13 Step: 1755 Index:-0.6289 R2:0.3713 0.2924 0.3969 RMSE:0.8886 0.9811 0.8843 Tau:0.4281 0.3522 0.1890\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 14 Step: 1890 Index:-0.6350 R2:0.3761 0.2915 0.3996 RMSE:0.8966 0.9858 0.8938 Tau:0.4331 0.3508 0.1893\n",
      "Epoch: 15 Step: 2025 Index:-0.6134 R2:0.3891 0.3193 0.4064 RMSE:0.8844 0.9803 0.8852 Tau:0.4428 0.3669 0.1976\n",
      "Epoch: 16 Step: 2160 Index:-0.6070 R2:0.3837 0.3087 0.4031 RMSE:0.8810 0.9761 0.8806 Tau:0.4431 0.3691 0.1968\n",
      "Epoch: 17 Step: 2295 Index:-0.5955 R2:0.3996 0.3198 0.4174 RMSE:0.8747 0.9616 0.8734 Tau:0.4520 0.3661 0.1971\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 18 Step: 2430 Index:-0.5964 R2:0.4003 0.3179 0.4209 RMSE:0.8679 0.9633 0.8656 Tau:0.4511 0.3668 0.1949\n",
      "Epoch: 19 Step: 2565 Index:-0.5950 R2:0.4020 0.3221 0.4159 RMSE:0.8845 0.9666 0.8858 Tau:0.4544 0.3716 0.1943\n",
      "Epoch: 20 Step: 2700 Index:-0.5691 R2:0.4223 0.3435 0.4321 RMSE:0.8549 0.9566 0.8610 Tau:0.4675 0.3874 0.2036\n",
      "Epoch: 21 Step: 2835 Index:-0.5428 R2:0.4229 0.3544 0.4136 RMSE:0.8608 0.9392 0.8761 Tau:0.4669 0.3964 0.2053\n",
      "Epoch: 22 Step: 2970 Index:-0.5326 R2:0.4336 0.3551 0.4393 RMSE:0.8477 0.9366 0.8529 Tau:0.4745 0.4039 0.2103\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 23 Step: 3105 Index:-0.5405 R2:0.4351 0.3519 0.4191 RMSE:0.8420 0.9392 0.8667 Tau:0.4756 0.3987 0.2103\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 24 Step: 3240 Index:-0.5407 R2:0.4427 0.3519 0.4301 RMSE:0.8505 0.9425 0.8676 Tau:0.4854 0.4019 0.2068\n",
      "Epoch: 25 Step: 3375 Index:-0.5059 R2:0.4562 0.3806 0.4432 RMSE:0.8351 0.9195 0.8511 Tau:0.4889 0.4137 0.2139\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 26 Step: 3510 Index:-0.5161 R2:0.4623 0.3799 0.4483 RMSE:0.8337 0.9417 0.8598 Tau:0.4941 0.4256 0.2145\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 27 Step: 3645 Index:-0.5195 R2:0.4510 0.3742 0.4207 RMSE:0.8335 0.9387 0.8763 Tau:0.4840 0.4192 0.2188\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 28 Step: 3780 Index:-0.5059 R2:0.4682 0.3726 0.4491 RMSE:0.8287 0.9245 0.8482 Tau:0.5031 0.4186 0.2096\n",
      "Epoch: 29 Step: 3915 Index:-0.5044 R2:0.4836 0.3898 0.4540 RMSE:0.8203 0.9429 0.8641 Tau:0.5070 0.4385 0.2206\n",
      "Epoch: 30 Step: 4050 Index:-0.4984 R2:0.4769 0.3784 0.4527 RMSE:0.8102 0.9209 0.8402 Tau:0.5067 0.4225 0.2138\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 31 Step: 4185 Index:-0.5098 R2:0.5021 0.4032 0.4619 RMSE:0.8232 0.9545 0.8801 Tau:0.5198 0.4447 0.2213\n",
      "Epoch: 32 Step: 4320 Index:-0.4965 R2:0.5087 0.4095 0.4573 RMSE:0.8185 0.9403 0.8756 Tau:0.5223 0.4438 0.2246\n",
      "Epoch: 33 Step: 4455 Index:-0.4936 R2:0.4950 0.3850 0.4554 RMSE:0.8246 0.9284 0.8581 Tau:0.5183 0.4348 0.2192\n",
      "Epoch: 34 Step: 4590 Index:-0.4297 R2:0.5201 0.4367 0.4543 RMSE:0.8011 0.9049 0.8634 Tau:0.5271 0.4752 0.2365\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 35 Step: 4725 Index:-0.4411 R2:0.5279 0.4166 0.4713 RMSE:0.7767 0.8974 0.8305 Tau:0.5350 0.4562 0.2250\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 36 Step: 4860 Index:-0.4567 R2:0.5171 0.4148 0.4517 RMSE:0.7793 0.9045 0.8535 Tau:0.5239 0.4478 0.2243\n",
      "Epoch: 37 Step: 4995 Index:-0.4222 R2:0.5402 0.4327 0.4763 RMSE:0.7788 0.8828 0.8285 Tau:0.5423 0.4606 0.2334\n",
      "Epoch: 38 Step: 5130 Index:-0.4138 R2:0.5449 0.4345 0.4831 RMSE:0.7565 0.8767 0.8163 Tau:0.5442 0.4630 0.2333\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 39 Step: 5265 Index:-0.4294 R2:0.5497 0.4458 0.4843 RMSE:0.7774 0.9033 0.8502 Tau:0.5476 0.4739 0.2388\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 40 Step: 5400 Index:-0.4267 R2:0.5464 0.4353 0.4794 RMSE:0.7649 0.8952 0.8358 Tau:0.5453 0.4684 0.2344\n",
      "Epoch: 41 Step: 5535 Index:-0.3834 R2:0.5576 0.4611 0.4862 RMSE:0.7492 0.8678 0.8275 Tau:0.5465 0.4844 0.2446\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 42 Step: 5670 Index:-0.4181 R2:0.5546 0.4377 0.4817 RMSE:0.7632 0.8896 0.8314 Tau:0.5476 0.4715 0.2364\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 43 Step: 5805 Index:-0.4747 R2:0.5582 0.4478 0.4686 RMSE:0.8188 0.9436 0.9046 Tau:0.5491 0.4689 0.2389\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 44 Step: 5940 Index:-0.4168 R2:0.5702 0.4573 0.4908 RMSE:0.7722 0.8983 0.8525 Tau:0.5602 0.4814 0.2457\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 45 Step: 6075 Index:-0.4171 R2:0.5592 0.4385 0.4875 RMSE:0.7528 0.8776 0.8173 Tau:0.5535 0.4604 0.2322\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 46 Step: 6210 Index:-0.4044 R2:0.5697 0.4490 0.4939 RMSE:0.7464 0.8704 0.8131 Tau:0.5608 0.4660 0.2374\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 47 Step: 6345 Index:-0.4192 R2:0.5807 0.4657 0.4916 RMSE:0.7736 0.9029 0.8605 Tau:0.5645 0.4838 0.2424\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 48 Step: 6480 Index:-0.4093 R2:0.5815 0.4608 0.5077 RMSE:0.7481 0.8860 0.8261 Tau:0.5664 0.4766 0.2433\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 49 Step: 6615 Index:-0.3988 R2:0.5745 0.4636 0.4907 RMSE:0.7492 0.8816 0.8397 Tau:0.5622 0.4829 0.2481\n",
      "Epoch: 50 Step: 6750 Index:-0.3714 R2:0.5895 0.4669 0.5085 RMSE:0.7265 0.8546 0.8009 Tau:0.5721 0.4832 0.2463\n",
      "Epoch: 51 Step: 6885 Index:-0.3688 R2:0.5841 0.4762 0.4992 RMSE:0.7323 0.8545 0.8167 Tau:0.5631 0.4857 0.2540\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 52 Step: 7020 Index:-0.3751 R2:0.5917 0.4792 0.5184 RMSE:0.7255 0.8626 0.8093 Tau:0.5716 0.4876 0.2532\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 53 Step: 7155 Index:-0.4017 R2:0.5264 0.4590 0.5025 RMSE:0.8064 0.8914 0.8316 Tau:0.5338 0.4897 0.2521\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 54 Step: 7290 Index:-0.4085 R2:0.5467 0.4667 0.5144 RMSE:0.7879 0.8988 0.8319 Tau:0.5444 0.4903 0.2560\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 55 Step: 7425 Index:-0.4074 R2:0.5614 0.4640 0.5237 RMSE:0.7704 0.8970 0.8226 Tau:0.5556 0.4896 0.2573\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 56 Step: 7560 Index:-0.3716 R2:0.5690 0.4638 0.5168 RMSE:0.7570 0.8597 0.8000 Tau:0.5611 0.4882 0.2583\n",
      "Epoch: 57 Step: 7695 Index:-0.3524 R2:0.5783 0.4788 0.5088 RMSE:0.7324 0.8517 0.8048 Tau:0.5661 0.4993 0.2627\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 58 Step: 7830 Index:-0.3738 R2:0.5855 0.4789 0.5184 RMSE:0.7313 0.8644 0.8083 Tau:0.5703 0.4906 0.2572\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 59 Step: 7965 Index:-0.4272 R2:0.5936 0.4887 0.5150 RMSE:0.7894 0.9295 0.8901 Tau:0.5723 0.5023 0.2646\n",
      "Epoch: 60 Step: 8100 Index:-0.3324 R2:0.5985 0.4964 0.5138 RMSE:0.7188 0.8402 0.8037 Tau:0.5764 0.5078 0.2628\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 61 Step: 8235 Index:-0.3636 R2:0.6021 0.4864 0.5140 RMSE:0.7184 0.8630 0.8193 Tau:0.5809 0.4994 0.2621\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 62 Step: 8370 Index:-0.3688 R2:0.6040 0.4843 0.5176 RMSE:0.7212 0.8648 0.8179 Tau:0.5819 0.4961 0.2564\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 63 Step: 8505 Index:-0.3856 R2:0.6096 0.4945 0.5073 RMSE:0.7603 0.8912 0.8647 Tau:0.5850 0.5055 0.2605\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 64 Step: 8640 Index:-0.3740 R2:0.6147 0.4912 0.5166 RMSE:0.7276 0.8733 0.8362 Tau:0.5889 0.4993 0.2610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 Step: 8775 Index:-0.3270 R2:0.6131 0.4942 0.5199 RMSE:0.6991 0.8310 0.7898 Tau:0.5854 0.5040 0.2656\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 66 Step: 8910 Index:-0.3498 R2:0.6157 0.4847 0.5144 RMSE:0.7004 0.8459 0.8005 Tau:0.5893 0.4961 0.2613\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 67 Step: 9045 Index:-0.3750 R2:0.5564 0.4670 0.4564 RMSE:0.7812 0.8692 0.8487 Tau:0.5566 0.4942 0.2575\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 68 Step: 9180 Index:-0.3743 R2:0.6187 0.5020 0.5254 RMSE:0.7344 0.8799 0.8465 Tau:0.5907 0.5056 0.2677\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 69 Step: 9315 Index:-0.3592 R2:0.6114 0.4750 0.5178 RMSE:0.6998 0.8469 0.7890 Tau:0.5843 0.4878 0.2593\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 70 Step: 9450 Index:-0.3579 R2:0.6265 0.4978 0.5286 RMSE:0.7103 0.8612 0.8183 Tau:0.5940 0.5033 0.2664\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 71 Step: 9585 Index:-0.3800 R2:0.6172 0.4882 0.5221 RMSE:0.7130 0.8706 0.8231 Tau:0.5919 0.4906 0.2607\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 72 Step: 9720 Index:-0.3395 R2:0.6210 0.4981 0.5137 RMSE:0.7072 0.8360 0.8013 Tau:0.5932 0.4966 0.2610\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 73 Step: 9855 Index:-0.3412 R2:0.6260 0.4937 0.5280 RMSE:0.6974 0.8369 0.7848 Tau:0.5952 0.4957 0.2612\n",
      "Epoch: 74 Step: 9990 Index:-0.3083 R2:0.6289 0.5114 0.5228 RMSE:0.6869 0.8153 0.7863 Tau:0.5982 0.5070 0.2665\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 75 Step: 10125 Index:-0.3404 R2:0.6238 0.4906 0.5131 RMSE:0.6915 0.8320 0.7926 Tau:0.5923 0.4916 0.2654\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 76 Step: 10260 Index:-0.3121 R2:0.6333 0.5116 0.5341 RMSE:0.6942 0.8180 0.7774 Tau:0.5982 0.5058 0.2697\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 77 Step: 10395 Index:-0.3344 R2:0.6371 0.4987 0.5340 RMSE:0.6751 0.8373 0.7858 Tau:0.6019 0.5029 0.2691\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 78 Step: 10530 Index:-0.3111 R2:0.6360 0.5126 0.5159 RMSE:0.6839 0.8245 0.8035 Tau:0.5993 0.5134 0.2724\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 79 Step: 10665 Index:-0.3142 R2:0.6375 0.5111 0.5241 RMSE:0.6824 0.8158 0.7834 Tau:0.6027 0.5016 0.2657\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 80 Step: 10800 Index:-0.3408 R2:0.6408 0.5030 0.5239 RMSE:0.6739 0.8379 0.8000 Tau:0.6056 0.4970 0.2648\n",
      "Epoch: 81 Step: 10935 Index:-0.2970 R2:0.6402 0.5190 0.5244 RMSE:0.6841 0.8097 0.7853 Tau:0.6063 0.5128 0.2734\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 82 Step: 11070 Index:-0.3373 R2:0.6274 0.5036 0.5133 RMSE:0.7089 0.8493 0.8256 Tau:0.5924 0.5120 0.2834\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 83 Step: 11205 Index:-0.3504 R2:0.6428 0.5218 0.5248 RMSE:0.7094 0.8630 0.8464 Tau:0.6071 0.5126 0.2778\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 84 Step: 11340 Index:-0.3239 R2:0.6531 0.5195 0.5272 RMSE:0.6848 0.8369 0.8152 Tau:0.6119 0.5130 0.2740\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 85 Step: 11475 Index:-0.3141 R2:0.6526 0.5095 0.5197 RMSE:0.6614 0.8184 0.7893 Tau:0.6144 0.5043 0.2735\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 86 Step: 11610 Index:-0.3478 R2:0.6486 0.5018 0.5455 RMSE:0.6659 0.8437 0.7810 Tau:0.6106 0.4959 0.2712\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 87 Step: 11745 Index:-0.3258 R2:0.6481 0.5189 0.5324 RMSE:0.6788 0.8315 0.7916 Tau:0.6118 0.5057 0.2788\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 88 Step: 11880 Index:-0.3060 R2:0.6491 0.5187 0.5094 RMSE:0.6720 0.8209 0.8141 Tau:0.6132 0.5149 0.2768\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 89 Step: 12015 Index:-0.3232 R2:0.6480 0.5080 0.5141 RMSE:0.6709 0.8289 0.8063 Tau:0.6139 0.5056 0.2802\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 90 Step: 12150 Index:-0.3228 R2:0.6645 0.5197 0.5353 RMSE:0.6895 0.8272 0.7905 Tau:0.6240 0.5045 0.2733\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 91 Step: 12285 Index:-0.3539 R2:0.6542 0.5173 0.5062 RMSE:0.7137 0.8652 0.8610 Tau:0.6159 0.5113 0.2804\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 92 Step: 12420 Index:-0.3101 R2:0.6570 0.5120 0.5364 RMSE:0.6721 0.8219 0.7770 Tau:0.6166 0.5118 0.2781\n",
      "Epoch: 93 Step: 12555 Index:-0.2919 R2:0.6688 0.5311 0.5411 RMSE:0.6453 0.8068 0.7779 Tau:0.6252 0.5149 0.2826\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 94 Step: 12690 Index:-0.3724 R2:0.6602 0.5287 0.5295 RMSE:0.7356 0.8902 0.8793 Tau:0.6200 0.5178 0.2886\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 95 Step: 12825 Index:-0.3217 R2:0.6707 0.5279 0.5345 RMSE:0.6742 0.8349 0.8134 Tau:0.6272 0.5133 0.2822\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 96 Step: 12960 Index:-0.4005 R2:0.6668 0.5355 0.5269 RMSE:0.7691 0.9239 0.9295 Tau:0.6218 0.5234 0.2911\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 97 Step: 13095 Index:-0.3215 R2:0.6632 0.5076 0.5414 RMSE:0.6638 0.8267 0.7736 Tau:0.6230 0.5051 0.2766\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 98 Step: 13230 Index:-0.3355 R2:0.6659 0.5296 0.5242 RMSE:0.6883 0.8527 0.8433 Tau:0.6225 0.5173 0.2863\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 99 Step: 13365 Index:-0.3145 R2:0.6735 0.5123 0.5411 RMSE:0.6400 0.8219 0.7716 Tau:0.6296 0.5074 0.2843\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 100 Step: 13500 Index:-0.2945 R2:0.6757 0.5388 0.5320 RMSE:0.6561 0.8243 0.8139 Tau:0.6264 0.5298 0.2982\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 101 Step: 13635 Index:-0.3182 R2:0.6766 0.5183 0.5559 RMSE:0.6379 0.8353 0.7763 Tau:0.6296 0.5171 0.2882\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 102 Step: 13770 Index:-0.3131 R2:0.6531 0.5200 0.5354 RMSE:0.6808 0.8257 0.7948 Tau:0.6135 0.5126 0.2801\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 103 Step: 13905 Index:-0.3185 R2:0.6706 0.5137 0.5465 RMSE:0.6686 0.8239 0.7735 Tau:0.6277 0.5054 0.2832\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 104 Step: 14040 Index:-0.2964 R2:0.6785 0.5216 0.5515 RMSE:0.6585 0.8130 0.7648 Tau:0.6324 0.5166 0.2850\n",
      "Epoch: 105 Step: 14175 Index:-0.2801 R2:0.6835 0.5309 0.5443 RMSE:0.6486 0.8027 0.7686 Tau:0.6377 0.5225 0.2896\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 106 Step: 14310 Index:-0.3536 R2:0.6833 0.5276 0.5476 RMSE:0.6826 0.8743 0.8396 Tau:0.6372 0.5207 0.2894\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 107 Step: 14445 Index:-0.3412 R2:0.6843 0.5143 0.5426 RMSE:0.6578 0.8474 0.8081 Tau:0.6395 0.5062 0.2903\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 108 Step: 14580 Index:-0.3183 R2:0.6787 0.5160 0.5386 RMSE:0.6394 0.8272 0.7868 Tau:0.6347 0.5090 0.2879\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 109 Step: 14715 Index:-0.3011 R2:0.6830 0.5282 0.5280 RMSE:0.6393 0.8135 0.7972 Tau:0.6357 0.5124 0.2947\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 110 Step: 14850 Index:-0.3055 R2:0.6804 0.5198 0.5411 RMSE:0.6342 0.8171 0.7759 Tau:0.6344 0.5116 0.2866\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 111 Step: 14985 Index:-0.3266 R2:0.6784 0.5126 0.5533 RMSE:0.6482 0.8342 0.7700 Tau:0.6341 0.5076 0.2846\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 112 Step: 15120 Index:-0.3137 R2:0.6883 0.5079 0.5424 RMSE:0.6278 0.8238 0.7713 Tau:0.6423 0.5100 0.2759\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 113 Step: 15255 Index:-0.3116 R2:0.6786 0.5073 0.5383 RMSE:0.6555 0.8193 0.7733 Tau:0.6377 0.5077 0.2798\n",
      "Epoch: 114 Step: 15390 Index:-0.2774 R2:0.6897 0.5321 0.5303 RMSE:0.6330 0.7992 0.7818 Tau:0.6409 0.5218 0.2998\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 115 Step: 15525 Index:-0.3678 R2:0.6764 0.5265 0.5097 RMSE:0.7009 0.8885 0.8815 Tau:0.6319 0.5207 0.3049\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 116 Step: 15660 Index:-0.3286 R2:0.6917 0.5001 0.5400 RMSE:0.6272 0.8310 0.7750 Tau:0.6450 0.5024 0.2830\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 117 Step: 15795 Index:-0.3272 R2:0.6997 0.5254 0.5522 RMSE:0.6405 0.8466 0.8040 Tau:0.6480 0.5193 0.2990\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 118 Step: 15930 Index:-0.2992 R2:0.6980 0.5352 0.5367 RMSE:0.6411 0.8293 0.8118 Tau:0.6462 0.5300 0.2977\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 119 Step: 16065 Index:-0.3503 R2:0.6878 0.5186 0.5277 RMSE:0.6895 0.8757 0.8554 Tau:0.6438 0.5254 0.2946\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 120 Step: 16200 Index:-0.3188 R2:0.6926 0.5151 0.5449 RMSE:0.6221 0.8327 0.7817 Tau:0.6449 0.5138 0.2801\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 121 Step: 16335 Index:-0.2994 R2:0.6943 0.5201 0.5392 RMSE:0.6227 0.8176 0.7795 Tau:0.6443 0.5182 0.2875\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 122 Step: 16470 Index:-0.3340 R2:0.6958 0.5158 0.5298 RMSE:0.6297 0.8434 0.8074 Tau:0.6478 0.5094 0.2837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 123 Step: 16605 Index:-0.3044 R2:0.7055 0.5251 0.5435 RMSE:0.6227 0.8270 0.7886 Tau:0.6516 0.5225 0.2954\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 124 Step: 16740 Index:-0.3418 R2:0.7032 0.5096 0.5488 RMSE:0.6182 0.8489 0.7858 Tau:0.6528 0.5071 0.2893\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 125 Step: 16875 Index:-0.3069 R2:0.7067 0.5191 0.5556 RMSE:0.6063 0.8219 0.7604 Tau:0.6534 0.5150 0.2983\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Epoch: 126 Step: 17010 Index:-0.3285 R2:0.6977 0.5085 0.5492 RMSE:0.6143 0.8412 0.7729 Tau:0.6496 0.5127 0.2869\n",
      "Epoch: 127 Step: 17145 Index:-0.2707 R2:0.6955 0.5417 0.5379 RMSE:0.6197 0.8069 0.7884 Tau:0.6404 0.5362 0.3077\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 128 Step: 17280 Index:-0.3339 R2:0.6884 0.5148 0.5232 RMSE:0.6378 0.8484 0.8165 Tau:0.6397 0.5144 0.2990\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 129 Step: 17415 Index:-0.3295 R2:0.6905 0.5039 0.5145 RMSE:0.6310 0.8371 0.8055 Tau:0.6442 0.5077 0.2884\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 130 Step: 17550 Index:-0.2971 R2:0.7087 0.5288 0.5336 RMSE:0.6172 0.8202 0.7986 Tau:0.6550 0.5231 0.2954\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 131 Step: 17685 Index:-0.3121 R2:0.7160 0.5305 0.5593 RMSE:0.6529 0.8375 0.7846 Tau:0.6606 0.5254 0.2986\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 132 Step: 17820 Index:-0.3225 R2:0.7140 0.5131 0.5461 RMSE:0.6046 0.8380 0.7845 Tau:0.6592 0.5155 0.2961\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 133 Step: 17955 Index:-0.3189 R2:0.7117 0.5231 0.5490 RMSE:0.6031 0.8361 0.7782 Tau:0.6552 0.5173 0.3039\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 134 Step: 18090 Index:-0.3056 R2:0.7087 0.5149 0.5581 RMSE:0.6060 0.8217 0.7565 Tau:0.6549 0.5161 0.2809\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 135 Step: 18225 Index:-0.3195 R2:0.7170 0.5186 0.5458 RMSE:0.5957 0.8364 0.7811 Tau:0.6602 0.5170 0.2935\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 136 Step: 18360 Index:-0.3460 R2:0.7087 0.5047 0.5572 RMSE:0.6077 0.8557 0.7753 Tau:0.6563 0.5097 0.2867\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 137 Step: 18495 Index:-0.2925 R2:0.7207 0.5252 0.5445 RMSE:0.5924 0.8140 0.7721 Tau:0.6624 0.5216 0.3024\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 138 Step: 18630 Index:-0.3446 R2:0.7093 0.5329 0.5278 RMSE:0.6606 0.8768 0.8612 Tau:0.6562 0.5322 0.3161\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Epoch: 139 Step: 18765 Index:-0.3768 R2:0.7152 0.5070 0.5601 RMSE:0.6824 0.8924 0.8103 Tau:0.6582 0.5156 0.3040\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Epoch: 140 Step: 18900 Index:-0.3309 R2:0.7268 0.5273 0.5465 RMSE:0.6259 0.8586 0.8229 Tau:0.6680 0.5277 0.3006\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Epoch: 141 Step: 19035 Index:-0.4016 R2:0.7261 0.5257 0.5227 RMSE:0.7123 0.9250 0.9140 Tau:0.6687 0.5234 0.3062\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Epoch: 142 Step: 19170 Index:-0.3217 R2:0.7150 0.5215 0.5446 RMSE:0.6259 0.8473 0.7998 Tau:0.6585 0.5257 0.3133\n",
      "EarlyStopping counter: 16 out of 100\n",
      "Epoch: 143 Step: 19305 Index:-0.3055 R2:0.7184 0.5205 0.5217 RMSE:0.6195 0.8401 0.8228 Tau:0.6615 0.5345 0.3016\n",
      "EarlyStopping counter: 17 out of 100\n",
      "Epoch: 144 Step: 19440 Index:-0.2959 R2:0.7187 0.5261 0.5516 RMSE:0.5949 0.8208 0.7668 Tau:0.6601 0.5250 0.3052\n",
      "EarlyStopping counter: 18 out of 100\n",
      "Epoch: 145 Step: 19575 Index:-0.2974 R2:0.7176 0.5136 0.5265 RMSE:0.6071 0.8141 0.7811 Tau:0.6600 0.5168 0.3096\n",
      "EarlyStopping counter: 19 out of 100\n",
      "Epoch: 146 Step: 19710 Index:-0.2826 R2:0.7284 0.5282 0.5495 RMSE:0.5893 0.8104 0.7683 Tau:0.6683 0.5278 0.3059\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Epoch: 147 Step: 19845 Index:-0.3164 R2:0.7269 0.5122 0.5482 RMSE:0.5887 0.8365 0.7767 Tau:0.6680 0.5201 0.3045\n",
      "EarlyStopping counter: 21 out of 100\n",
      "Epoch: 148 Step: 19980 Index:-0.3091 R2:0.7341 0.5207 0.5437 RMSE:0.5838 0.8293 0.7823 Tau:0.6729 0.5203 0.3027\n",
      "EarlyStopping counter: 22 out of 100\n",
      "Epoch: 149 Step: 20115 Index:-0.3650 R2:0.7161 0.4979 0.5518 RMSE:0.6218 0.8732 0.7921 Tau:0.6616 0.5083 0.3151\n",
      "EarlyStopping counter: 23 out of 100\n",
      "Epoch: 150 Step: 20250 Index:-0.3232 R2:0.7272 0.5093 0.5365 RMSE:0.6106 0.8369 0.7969 Tau:0.6687 0.5136 0.3005\n",
      "EarlyStopping counter: 24 out of 100\n",
      "Epoch: 151 Step: 20385 Index:-0.4095 R2:0.7170 0.5089 0.5092 RMSE:0.6988 0.9313 0.9072 Tau:0.6638 0.5217 0.3097\n",
      "EarlyStopping counter: 25 out of 100\n",
      "Epoch: 152 Step: 20520 Index:-0.3635 R2:0.7348 0.5162 0.5633 RMSE:0.6117 0.8808 0.8067 Tau:0.6741 0.5173 0.3075\n",
      "EarlyStopping counter: 26 out of 100\n",
      "Epoch: 153 Step: 20655 Index:-0.3634 R2:0.7226 0.5097 0.5480 RMSE:0.6082 0.8791 0.8109 Tau:0.6626 0.5157 0.3107\n",
      "EarlyStopping counter: 27 out of 100\n",
      "Epoch: 154 Step: 20790 Index:-0.3289 R2:0.7390 0.5192 0.5520 RMSE:0.5919 0.8504 0.7935 Tau:0.6761 0.5216 0.3146\n",
      "EarlyStopping counter: 28 out of 100\n",
      "Epoch: 155 Step: 20925 Index:-0.4053 R2:0.7159 0.4953 0.5409 RMSE:0.6510 0.9182 0.8479 Tau:0.6620 0.5129 0.3119\n",
      "EarlyStopping counter: 29 out of 100\n",
      "Epoch: 156 Step: 21060 Index:-0.3191 R2:0.7259 0.5169 0.5440 RMSE:0.5858 0.8442 0.7882 Tau:0.6687 0.5251 0.3120\n",
      "EarlyStopping counter: 30 out of 100\n",
      "Epoch: 157 Step: 21195 Index:-0.3314 R2:0.7281 0.5103 0.5552 RMSE:0.5841 0.8460 0.7740 Tau:0.6670 0.5146 0.3030\n",
      "EarlyStopping counter: 31 out of 100\n",
      "Epoch: 158 Step: 21330 Index:-0.3183 R2:0.7277 0.5039 0.5480 RMSE:0.5963 0.8268 0.7693 Tau:0.6695 0.5085 0.2886\n",
      "EarlyStopping counter: 32 out of 100\n",
      "Epoch: 159 Step: 21465 Index:-0.3252 R2:0.7379 0.5174 0.5631 RMSE:0.5896 0.8467 0.7776 Tau:0.6737 0.5215 0.3008\n",
      "EarlyStopping counter: 33 out of 100\n",
      "Epoch: 160 Step: 21600 Index:-0.3231 R2:0.7434 0.5067 0.5369 RMSE:0.5691 0.8330 0.7781 Tau:0.6798 0.5099 0.2992\n",
      "EarlyStopping counter: 34 out of 100\n",
      "Epoch: 161 Step: 21735 Index:-0.3078 R2:0.7374 0.5219 0.5592 RMSE:0.5891 0.8345 0.7649 Tau:0.6748 0.5266 0.3023\n",
      "EarlyStopping counter: 35 out of 100\n",
      "Epoch: 162 Step: 21870 Index:-0.3189 R2:0.7361 0.5091 0.5468 RMSE:0.5926 0.8332 0.7689 Tau:0.6736 0.5143 0.2993\n",
      "EarlyStopping counter: 36 out of 100\n",
      "Epoch: 163 Step: 22005 Index:-0.3328 R2:0.7291 0.5138 0.5408 RMSE:0.5937 0.8478 0.7997 Tau:0.6668 0.5150 0.3042\n",
      "EarlyStopping counter: 37 out of 100\n",
      "Epoch: 164 Step: 22140 Index:-0.3649 R2:0.7404 0.5056 0.5435 RMSE:0.6183 0.8780 0.8264 Tau:0.6769 0.5132 0.3006\n",
      "EarlyStopping counter: 38 out of 100\n",
      "Epoch: 165 Step: 22275 Index:-0.3182 R2:0.7436 0.5160 0.5614 RMSE:0.5669 0.8404 0.7669 Tau:0.6779 0.5222 0.3050\n",
      "EarlyStopping counter: 39 out of 100\n",
      "Epoch: 166 Step: 22410 Index:-0.3485 R2:0.7452 0.5063 0.5440 RMSE:0.6327 0.8607 0.7960 Tau:0.6810 0.5122 0.3031\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Epoch: 167 Step: 22545 Index:-0.3296 R2:0.7405 0.4995 0.5494 RMSE:0.5751 0.8410 0.7672 Tau:0.6784 0.5114 0.3016\n",
      "EarlyStopping counter: 41 out of 100\n",
      "Epoch: 168 Step: 22680 Index:-0.3587 R2:0.7387 0.4944 0.5498 RMSE:0.5753 0.8641 0.7837 Tau:0.6764 0.5054 0.3048\n",
      "EarlyStopping counter: 42 out of 100\n",
      "Epoch: 169 Step: 22815 Index:-0.3613 R2:0.7451 0.5136 0.5549 RMSE:0.5953 0.8853 0.8127 Tau:0.6802 0.5240 0.2979\n",
      "EarlyStopping counter: 43 out of 100\n",
      "Epoch: 170 Step: 22950 Index:-0.3074 R2:0.7503 0.5259 0.5487 RMSE:0.5919 0.8393 0.7834 Tau:0.6849 0.5319 0.3104\n",
      "EarlyStopping counter: 44 out of 100\n",
      "Epoch: 171 Step: 23085 Index:-0.3202 R2:0.7439 0.5165 0.5442 RMSE:0.5673 0.8368 0.7811 Tau:0.6810 0.5167 0.3024\n",
      "EarlyStopping counter: 45 out of 100\n",
      "Epoch: 172 Step: 23220 Index:-0.3541 R2:0.7363 0.4929 0.5480 RMSE:0.5752 0.8632 0.7771 Tau:0.6742 0.5091 0.2991\n",
      "EarlyStopping counter: 46 out of 100\n",
      "Epoch: 173 Step: 23355 Index:-0.3237 R2:0.7517 0.5105 0.5531 RMSE:0.5586 0.8437 0.7662 Tau:0.6856 0.5200 0.3105\n",
      "EarlyStopping counter: 47 out of 100\n",
      "Epoch: 174 Step: 23490 Index:-0.3144 R2:0.7435 0.5099 0.5389 RMSE:0.5794 0.8323 0.7747 Tau:0.6773 0.5178 0.3178\n",
      "EarlyStopping counter: 48 out of 100\n",
      "Epoch: 175 Step: 23625 Index:-0.3422 R2:0.7420 0.5012 0.5550 RMSE:0.5727 0.8522 0.7645 Tau:0.6789 0.5100 0.2959\n",
      "EarlyStopping counter: 49 out of 100\n",
      "Epoch: 176 Step: 23760 Index:-0.4307 R2:0.7472 0.5211 0.5322 RMSE:0.7083 0.9659 0.9325 Tau:0.6835 0.5351 0.3159\n",
      "EarlyStopping counter: 50 out of 100\n",
      "Epoch: 177 Step: 23895 Index:-0.3711 R2:0.7392 0.4837 0.5513 RMSE:0.5769 0.8685 0.7711 Tau:0.6767 0.4973 0.2978\n",
      "EarlyStopping counter: 51 out of 100\n",
      "Epoch: 178 Step: 24030 Index:-0.3308 R2:0.7563 0.5157 0.5678 RMSE:0.5641 0.8517 0.7602 Tau:0.6889 0.5210 0.3104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 52 out of 100\n",
      "Epoch: 179 Step: 24165 Index:-0.3568 R2:0.7497 0.5216 0.5258 RMSE:0.6080 0.8875 0.8520 Tau:0.6853 0.5307 0.3161\n",
      "EarlyStopping counter: 53 out of 100\n",
      "Epoch: 180 Step: 24300 Index:-0.3336 R2:0.7540 0.5143 0.5487 RMSE:0.5560 0.8535 0.7864 Tau:0.6871 0.5199 0.3063\n",
      "EarlyStopping counter: 54 out of 100\n",
      "Epoch: 181 Step: 24435 Index:-0.3872 R2:0.7512 0.5001 0.5515 RMSE:0.6041 0.8977 0.8259 Tau:0.6848 0.5105 0.3081\n",
      "EarlyStopping counter: 55 out of 100\n",
      "Epoch: 182 Step: 24570 Index:-0.4284 R2:0.7579 0.4945 0.5449 RMSE:0.6484 0.9389 0.8681 Tau:0.6919 0.5105 0.2987\n",
      "EarlyStopping counter: 56 out of 100\n",
      "Epoch: 183 Step: 24705 Index:-0.3741 R2:0.7461 0.5085 0.5472 RMSE:0.6561 0.8921 0.8219 Tau:0.6837 0.5179 0.3176\n",
      "EarlyStopping counter: 57 out of 100\n",
      "Epoch: 184 Step: 24840 Index:-0.3615 R2:0.7546 0.5030 0.5422 RMSE:0.5736 0.8767 0.8120 Tau:0.6904 0.5152 0.3033\n",
      "EarlyStopping counter: 58 out of 100\n",
      "Epoch: 185 Step: 24975 Index:-0.3514 R2:0.7615 0.5076 0.5448 RMSE:0.5668 0.8653 0.8031 Tau:0.6938 0.5139 0.3120\n",
      "EarlyStopping counter: 59 out of 100\n",
      "Epoch: 186 Step: 25110 Index:-0.4030 R2:0.7585 0.4864 0.5458 RMSE:0.5799 0.9001 0.8064 Tau:0.6934 0.4970 0.3110\n",
      "EarlyStopping counter: 60 out of 100\n",
      "Epoch: 187 Step: 25245 Index:-0.3405 R2:0.7566 0.5102 0.5511 RMSE:0.5663 0.8631 0.7944 Tau:0.6881 0.5225 0.2977\n",
      "EarlyStopping counter: 61 out of 100\n",
      "Epoch: 188 Step: 25380 Index:-0.3109 R2:0.7593 0.5234 0.5405 RMSE:0.5498 0.8363 0.7872 Tau:0.6913 0.5254 0.3240\n",
      "EarlyStopping counter: 62 out of 100\n",
      "Epoch: 189 Step: 25515 Index:-0.3361 R2:0.7605 0.5161 0.5505 RMSE:0.5969 0.8558 0.7885 Tau:0.6928 0.5197 0.3141\n",
      "EarlyStopping counter: 63 out of 100\n",
      "Epoch: 190 Step: 25650 Index:-0.3468 R2:0.7569 0.4947 0.5546 RMSE:0.5584 0.8568 0.7662 Tau:0.6919 0.5100 0.3175\n",
      "EarlyStopping counter: 64 out of 100\n",
      "Epoch: 191 Step: 25785 Index:-0.3650 R2:0.7543 0.4882 0.5286 RMSE:0.5694 0.8692 0.8066 Tau:0.6910 0.5042 0.2996\n",
      "EarlyStopping counter: 65 out of 100\n",
      "Epoch: 192 Step: 25920 Index:-0.3329 R2:0.7623 0.5059 0.5372 RMSE:0.5595 0.8496 0.7931 Tau:0.6951 0.5167 0.3131\n",
      "EarlyStopping counter: 66 out of 100\n",
      "Epoch: 193 Step: 26055 Index:-0.3376 R2:0.7625 0.5059 0.5475 RMSE:0.5732 0.8490 0.7765 Tau:0.6941 0.5114 0.3192\n",
      "EarlyStopping counter: 67 out of 100\n",
      "Epoch: 194 Step: 26190 Index:-0.3624 R2:0.7647 0.4894 0.5443 RMSE:0.5433 0.8673 0.7812 Tau:0.6977 0.5050 0.3056\n",
      "EarlyStopping counter: 68 out of 100\n",
      "Epoch: 195 Step: 26325 Index:-0.3622 R2:0.7694 0.4982 0.5522 RMSE:0.5458 0.8764 0.7852 Tau:0.6995 0.5142 0.3211\n",
      "EarlyStopping counter: 69 out of 100\n",
      "Epoch: 196 Step: 26460 Index:-0.3215 R2:0.7555 0.5112 0.5445 RMSE:0.5688 0.8431 0.7747 Tau:0.6901 0.5216 0.3222\n",
      "EarlyStopping counter: 70 out of 100\n",
      "Epoch: 197 Step: 26595 Index:-0.3711 R2:0.7436 0.4900 0.5146 RMSE:0.5733 0.8691 0.8146 Tau:0.6774 0.4980 0.3123\n",
      "EarlyStopping counter: 71 out of 100\n",
      "Epoch: 198 Step: 26730 Index:-0.3614 R2:0.7713 0.4993 0.5574 RMSE:0.5369 0.8697 0.7699 Tau:0.7007 0.5084 0.3182\n",
      "EarlyStopping counter: 72 out of 100\n",
      "Epoch: 199 Step: 26865 Index:-0.3167 R2:0.7577 0.5159 0.5405 RMSE:0.5500 0.8422 0.7845 Tau:0.6906 0.5256 0.3224\n",
      "EarlyStopping counter: 73 out of 100\n",
      "Epoch: 200 Step: 27000 Index:-0.3536 R2:0.7764 0.5014 0.5367 RMSE:0.5609 0.8674 0.8109 Tau:0.7048 0.5138 0.3165\n",
      "EarlyStopping counter: 74 out of 100\n",
      "Epoch: 201 Step: 27135 Index:-0.3169 R2:0.7686 0.5038 0.5431 RMSE:0.5462 0.8339 0.7709 Tau:0.6997 0.5170 0.3237\n",
      "EarlyStopping counter: 75 out of 100\n",
      "Epoch: 202 Step: 27270 Index:-0.3182 R2:0.7723 0.5062 0.5536 RMSE:0.5390 0.8338 0.7610 Tau:0.7021 0.5157 0.3161\n",
      "EarlyStopping counter: 76 out of 100\n",
      "Epoch: 203 Step: 27405 Index:-0.3627 R2:0.7752 0.5023 0.5523 RMSE:0.5483 0.8777 0.7970 Tau:0.7047 0.5150 0.3203\n",
      "EarlyStopping counter: 77 out of 100\n",
      "Epoch: 204 Step: 27540 Index:-0.3730 R2:0.7645 0.4873 0.5448 RMSE:0.5790 0.8733 0.7878 Tau:0.6971 0.5003 0.3098\n",
      "EarlyStopping counter: 78 out of 100\n",
      "Epoch: 205 Step: 27675 Index:-0.3529 R2:0.7721 0.4885 0.5622 RMSE:0.5356 0.8538 0.7578 Tau:0.7007 0.5010 0.3105\n",
      "EarlyStopping counter: 79 out of 100\n",
      "Epoch: 206 Step: 27810 Index:-0.3708 R2:0.7772 0.4965 0.5646 RMSE:0.5449 0.8787 0.7841 Tau:0.7052 0.5080 0.3179\n",
      "EarlyStopping counter: 80 out of 100\n",
      "Epoch: 207 Step: 27945 Index:-0.3818 R2:0.7643 0.4777 0.5395 RMSE:0.5426 0.8845 0.7899 Tau:0.6966 0.5027 0.3110\n",
      "EarlyStopping counter: 81 out of 100\n",
      "Epoch: 208 Step: 28080 Index:-0.3519 R2:0.7788 0.4933 0.5493 RMSE:0.5429 0.8652 0.7901 Tau:0.7057 0.5133 0.3183\n",
      "EarlyStopping counter: 82 out of 100\n",
      "Epoch: 209 Step: 28215 Index:-0.4014 R2:0.7693 0.4973 0.5330 RMSE:0.5985 0.9182 0.8599 Tau:0.6994 0.5168 0.3204\n",
      "EarlyStopping counter: 83 out of 100\n",
      "Epoch: 210 Step: 28350 Index:-0.3609 R2:0.7790 0.4868 0.5447 RMSE:0.5256 0.8690 0.7800 Tau:0.7088 0.5081 0.3130\n",
      "EarlyStopping counter: 84 out of 100\n",
      "Epoch: 211 Step: 28485 Index:-0.3454 R2:0.7659 0.5051 0.5363 RMSE:0.5669 0.8707 0.8155 Tau:0.6983 0.5253 0.3236\n",
      "EarlyStopping counter: 85 out of 100\n",
      "Epoch: 212 Step: 28620 Index:-0.3960 R2:0.7764 0.4885 0.5505 RMSE:0.5421 0.9038 0.8047 Tau:0.7046 0.5078 0.3112\n",
      "EarlyStopping counter: 86 out of 100\n",
      "Epoch: 213 Step: 28755 Index:-0.4410 R2:0.7756 0.4887 0.5289 RMSE:0.6406 0.9492 0.8937 Tau:0.7063 0.5083 0.3033\n",
      "EarlyStopping counter: 87 out of 100\n",
      "Epoch: 214 Step: 28890 Index:-0.3742 R2:0.7775 0.4955 0.5522 RMSE:0.5489 0.8919 0.8039 Tau:0.7068 0.5177 0.3258\n",
      "EarlyStopping counter: 88 out of 100\n",
      "Epoch: 215 Step: 29025 Index:-0.4251 R2:0.7734 0.4823 0.5556 RMSE:0.5426 0.9264 0.7998 Tau:0.7055 0.5012 0.3123\n",
      "EarlyStopping counter: 89 out of 100\n",
      "Epoch: 216 Step: 29160 Index:-0.3838 R2:0.7754 0.4898 0.5582 RMSE:0.5320 0.8851 0.7816 Tau:0.7027 0.5012 0.3137\n",
      "EarlyStopping counter: 90 out of 100\n",
      "Epoch: 217 Step: 29295 Index:-0.4460 R2:0.7792 0.4780 0.5500 RMSE:0.5745 0.9487 0.8427 Tau:0.7093 0.5027 0.3141\n",
      "EarlyStopping counter: 91 out of 100\n",
      "Epoch: 218 Step: 29430 Index:-0.3679 R2:0.7865 0.4899 0.5423 RMSE:0.5294 0.8811 0.7958 Tau:0.7115 0.5132 0.3207\n",
      "EarlyStopping counter: 92 out of 100\n",
      "Epoch: 219 Step: 29565 Index:-0.3482 R2:0.7774 0.4976 0.5371 RMSE:0.5312 0.8642 0.7929 Tau:0.7043 0.5160 0.3194\n",
      "EarlyStopping counter: 93 out of 100\n",
      "Epoch: 220 Step: 29700 Index:-0.3435 R2:0.7831 0.4853 0.5265 RMSE:0.5282 0.8530 0.7839 Tau:0.7120 0.5095 0.3184\n",
      "EarlyStopping counter: 94 out of 100\n",
      "Epoch: 221 Step: 29835 Index:-0.3858 R2:0.7874 0.4852 0.5324 RMSE:0.5373 0.8954 0.8172 Tau:0.7141 0.5096 0.3183\n",
      "EarlyStopping counter: 95 out of 100\n",
      "Epoch: 222 Step: 29970 Index:-0.3933 R2:0.7851 0.4803 0.5378 RMSE:0.5339 0.8908 0.7905 Tau:0.7131 0.4975 0.3174\n",
      "EarlyStopping counter: 96 out of 100\n",
      "Epoch: 223 Step: 30105 Index:-0.3417 R2:0.7868 0.4998 0.5620 RMSE:0.5180 0.8551 0.7590 Tau:0.7112 0.5134 0.3236\n",
      "EarlyStopping counter: 97 out of 100\n",
      "Epoch: 224 Step: 30240 Index:-0.3686 R2:0.7607 0.4858 0.5430 RMSE:0.5622 0.8863 0.7796 Tau:0.6942 0.5177 0.3288\n",
      "EarlyStopping counter: 98 out of 100\n",
      "Epoch: 225 Step: 30375 Index:-0.3752 R2:0.7933 0.4870 0.5429 RMSE:0.5246 0.8854 0.8025 Tau:0.7179 0.5102 0.3238\n",
      "EarlyStopping counter: 99 out of 100\n",
      "Epoch: 226 Step: 30510 Index:-0.3732 R2:0.7912 0.4932 0.5585 RMSE:0.5208 0.8833 0.7705 Tau:0.7158 0.5100 0.3251\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Epoch: 227 Step: 30645 Index:-0.3804 R2:0.7835 0.4858 0.5496 RMSE:0.5855 0.8788 0.7899 Tau:0.7110 0.4984 0.3148\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_one_hot', one_hot_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_interger', interger_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_binary', binary_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = - val_MSE**0.5 + val_tau\n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 227 r2:0.5379 RMSE:0.7884 WTI:0.3224 AP:0.4845 Tau:0.3077 \n",
      " \n",
      " Top-1:0.2308 Top-1-fp:0.0769 \n",
      " Top-5:0.5821 Top-5-fp:0.2090 \n",
      " Top-10:0.6222 Top-10-fp:0.3185 \n",
      " Top-15:0.5347 Top-15-fp:0.4307 \n",
      " Top-20:0.5000 Top-20-fp:0.4926 \n",
      " Top-25:0.5167 Top-25-fp:0.5414 \n",
      " Top-30:0.5800 Top-30-fp:0.5704 \n",
      " Top-40:0.6900 Top-40-fp:0.6174 \n",
      " Top-50:0.7500 Top-50-fp:0.6672 \n",
      " \n",
      " Top50:0.5800 Top50-fp:0.1200 \n",
      " Top100:0.6500 Top100-fp:0.2600 \n",
      " Top150:0.6067 Top150-fp:0.3400 \n",
      " Top200:0.5350 Top200-fp:0.4300 \n",
      " Top250:0.5080 Top250-fp:0.4680 \n",
      " Top300:0.4900 Top300-fp:0.5100 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = loss_function(mol_prediction,y)\n",
    "#             loss.backward(retain_graph=True)\n",
    "#             optimizer_AFSE.zero_grad()\n",
    "#             punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "\n",
    "# loss =  regression_loss + vat_loss + test_vat_loss\n",
    "\n",
    "#         init_lr = 1e-4\n",
    "#         max_lr = 10**-(init_lr-1)\n",
    "#         conv_lr = conv_lr - conv_lr**2 + 0.1 * punish_lr\n",
    "#         if conv_lr < max_lr:\n",
    "#             for param_group in optimizer_AFSE.param_groups:\n",
    "#                 param_group[\"lr\"] = conv_lr.detach()\n",
    "#                 AFSE_lr = conv_lr    \n",
    "#         else:\n",
    "#             for param_group in optimizer_AFSE.param_groups:\n",
    "#                 param_group[\"lr\"] = max_lr\n",
    "#                 AFSE_lr = max_lr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
