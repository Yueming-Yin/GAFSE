{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "torch.manual_seed(8)\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping, eval\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_C_CYP1A2_inhibitor\n",
      "model_file/G_ADMET_M_C_CYP1A2_inhibitor_run_0\n"
     ]
    }
   ],
   "source": [
    "task_filename = './data/ADMET/Generation/M_C_CYP1A2_inhibitor_1.csv'\n",
    "random_seed = 68\n",
    "batch_size = 10\n",
    "file_list1 = task_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-6]\n",
    "run_number = '_run_0'\n",
    "model_file = \"model_file/G_ADMET_\"+file1+run_number\n",
    "log_dir = f'log/{\"G_ADMET_\"+file1}'+run_number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12635\n",
      "./features/G_ADMET_M_C_CYP1A2_inhibitor_run_0.pickle\n",
      "feature dicts file saved as ./features/G_ADMET_M_C_CYP1A2_inhibitor_run_0.pickle\n"
     ]
    }
   ],
   "source": [
    "tasks = ['value']\n",
    "task_df = pd.read_csv(task_filename, header=0, names = [\"smiles\", \"dataset\",\"value\",\"cano_smiles\", \"dataset_generate\",\"matched_smiles\",\"matched_value\",\"matched_cano_smiles\"],usecols=[1,2,3,4,5,6,7,8])\n",
    "task_smilesList = task_df['cano_smiles'].values\n",
    "print(len(task_smilesList))\n",
    "feature_filename = './features/'+model_file.split('/')[-1][:-1]+'0.pickle'\n",
    "filename = './features/'+model_file.split('/')[-1]\n",
    "print(feature_filename)\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "#     print('Loading features successfully.')\n",
    "# else:\n",
    "feature_dicts = save_smiles_dicts(task_smilesList,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11339, 8) (1261, 8) (30, 8)\n"
     ]
    }
   ],
   "source": [
    "train_df = task_df[task_df.dataset_generate.values == \"training\"]\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = task_df[task_df.dataset_generate.values == \"val\"]\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = task_df[task_df.dataset_generate.values == \"test\"]\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([task_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None, sigmoid=False):\n",
    "    mol_prediction = model(feature=f, d=0, sigmoid=sigmoid)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda(), sigmoid=sigmoid)\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda(), sigmoid=sigmoid)\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr, sigmoid=sigmoid)\n",
    "    f_p = model(feature=f, d=d_adv, sigmoid=sigmoid)\n",
    "    f_p_ = model(feature=f, d=-d_adv, sigmoid=sigmoid)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "# def generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list):\n",
    "#     [a,b,c] = x_atom.shape\n",
    "#     [d,e,f,g] = bond_neighbor.shape\n",
    "#     ce_loss = nn.CrossEntropyLoss()\n",
    "#     one_hot_loss = 0\n",
    "#     interger_loss = 0\n",
    "#     binary_loss = 0\n",
    "#     counter_i = 0\n",
    "#     counter_j = 0\n",
    "#     validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "#     for i in range(a):\n",
    "#         l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "#         one_hot_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "#                         ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-9)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)+\\\n",
    "#                          weighted_CE_loss(refer_atom_list[i,:l,16:22], x_atom[i,:l,16:22])+ \\\n",
    "#                          weighted_CE_loss(refer_atom_list[i,:l,24:30], x_atom[i,:l,24:30])+ \\\n",
    "#                          weighted_CE_loss(refer_atom_list[i,:l,31:36], x_atom[i,:l,31:36])\n",
    "#         interger_loss += loss_function(refer_atom_list[i,:l,23], x_atom[i,:l,23])+ \\\n",
    "#                         loss_function(refer_atom_list[i,:l,24], x_atom[i,:l,24])\n",
    "#         binary_loss += CE(refer_atom_list[i,:l,30], x_atom[i,:l,30])+ \\\n",
    "#                         CE(refer_atom_list[i,:l,36], x_atom[i,:l,36])+ \\\n",
    "#                         CE(refer_atom_list[i,:l,37], x_atom[i,:l,37])+ \\\n",
    "#                         CE(refer_atom_list[i,:l,38], x_atom[i,:l,38])\n",
    "#         counter_i += 1\n",
    "#         for j in range(l):\n",
    "#             n = (bond_neighbor[i,j].sum(-1)!=0).sum(-1)\n",
    "#             if n==0:\n",
    "#                 continue\n",
    "#             one_hot_loss += weighted_CE_loss(refer_bond_list[i,j,:n,:4], bond_neighbor[i,j,:n,:4])+ \\\n",
    "#                              weighted_CE_loss(refer_bond_list[i,j,:n,6:], bond_neighbor[i,j,:n,6:])\n",
    "#             binary_loss += CE(refer_bond_list[i,j,:n,4], bond_neighbor[i,j,:n,4])+ \\\n",
    "#                            CE(refer_bond_list[i,j,:n,5], bond_neighbor[i,j,:n,5])\n",
    "#             counter_j += 1\n",
    "#     one_hot_loss = one_hot_loss/(5*counter_i+2*counter_j)\n",
    "#     interger_loss = interger_loss/(2*counter_i)\n",
    "#     binary_loss = binary_loss/(4*counter_i+2*counter_j)\n",
    "#     total_loss = (one_hot_loss + interger_loss + binary_loss)/3\n",
    "#     return total_loss, one_hot_loss, interger_loss, binary_loss\n",
    "\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    [d,e,f,g] = bond_neighbor.shape\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    one_hot_loss = 0\n",
    "    run_times = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        atom_weights = 1/x_atom[i,:l,:16].mean(-2)\n",
    "        ce_atom_loss = nn.CrossEntropyLoss(weight=atom_weights)\n",
    "        one_hot_loss += ce_atom_loss(refer_atom_list[i,:l,:16], torch.argmax(x_atom[i,:l,:16],-1))- \\\n",
    "                         ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-9)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)\n",
    "        run_times += 2\n",
    "    total_loss = one_hot_loss/run_times\n",
    "    return total_loss, 0, 0, 0\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features)\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1),\n",
    "                                                                                       sigmoid=True) # 10**-learning_rate     \n",
    "        classification_loss = - torch.Tensor(y_val).view(-1,1) * torch.log(mol_prediction) - \\\n",
    "                                (1-torch.Tensor(y_val).view(-1,1)) * torch.log(1-mol_prediction)\n",
    "        classification_loss = torch.mean(classification_loss)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-9),activated_features=activated_features)\n",
    "        success_smiles_batch, init_smiles, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=1, global_step=global_step)\n",
    "        reconstruction_loss, one_hot_loss, interger_loss,binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, \n",
    "                                                                                              bond_neighbor, validity_mask, atom_list, \n",
    "                                                                                              bond_list)\n",
    "#         x_atom_test = torch.Tensor(x_atom_test)\n",
    "#         x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "#         x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "#         bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "#         bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "#         activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "#                                                           torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "#                                                           torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "#                                                                                     alpha=1, lamda=10**-learning_rate)\n",
    "#         atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "#                                                 torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "#                                                 mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "#         refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "#                                                             torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "#                                                             mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "#         success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "#                             bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "#                                                      refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "#         test_reconstruction_loss, test_one_hot_loss, test_interger_loss,test_binary_loss = generate_loss_function(atom_list_test, x_atom_test, bond_list_test, bond_neighbor_test, validity_mask_test, atom_list_test, bond_list_test)\n",
    "        \n",
    "        if vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())    \n",
    "#         if vat_loss>1 or test_vat_loss>1:\n",
    "#             vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "#             test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        max_lr = 1e-3\n",
    "        conv_lr = conv_lr - conv_lr**2 + 0.06 * punish_lr # 0.06\n",
    "        if conv_lr < max_lr and conv_lr >= 0:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = conv_lr.detach()\n",
    "                AFSE_lr = conv_lr    \n",
    "        elif conv_lr < 0:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = 0\n",
    "                AFSE_lr = 0\n",
    "        elif conv_lr >= max_lr:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = max_lr\n",
    "                AFSE_lr = max_lr\n",
    "        \n",
    "        logger.add_scalar('loss/classification', classification_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "#         logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN_one_hot', one_hot_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN_interger', interger_loss, global_step)\n",
    "        logger.add_scalar('loss/GRN_binary', binary_loss, global_step)\n",
    "        logger.add_scalar('lr/max_lr', conv_lr, global_step)\n",
    "        logger.add_scalar('lr/punish_lr', punish_lr, global_step)\n",
    "        logger.add_scalar('lr/AFSE_lr', AFSE_lr, global_step)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "        optimizer_GRN.zero_grad()\n",
    "        loss =  classification_loss + vat_loss + reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "        optimizer_GRN.step()\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, \n",
    "                 refer_bond_list,topn=1, print_flag=False, global_step=0):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    init_smiles = []\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    recon_rate = 0\n",
    "    total_atom = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "            if print_flag:\n",
    "                print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "                      f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "                     '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#                 print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: refer_atom_list[i,j,k] for k in range(16)},\n",
    "#                       f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: atom_list[i,j,k] for k in range(16)},\n",
    "#                      '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        recon_rate += counter\n",
    "        total_atom += l\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "                if print_flag:\n",
    "                    print('No molecule that meets the conditions is generated.')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            if print_flag:\n",
    "                print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "                if print_flag:\n",
    "                    print('The same element is generated, generating the next element ...')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold or symbol_list[atom_symbol_generated]=='other':\n",
    "                if print_flag:\n",
    "                    print(f'The confidence of generating {symbol_list[atom_symbol_generated]} elements on the {generate_rdkit_index}-th atom is less than {confidence_threshold}, finding the next atom...')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "                    if print_flag:\n",
    "                        print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                init_smiles.append(smiles[i])\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                if print_flag:\n",
    "                    print(\"Molecule known to be highly toxic:\\n>>> \"+smiles[i])\n",
    "                    display(mol_init)\n",
    "                    print(f\"To optimize the above molecule to be less toxic, GAFSE Modifies the <{generate_rdkit_index}>-th atom to the element of <{symbol_list[atom_symbol_generated]}> with the probability of <{np.sort(atom_list[i,generate_index,:16])[-1-rank]:.3f}>:\\n>>> \"+Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                    display(mol_with_atom_index(mol))\n",
    "                    mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                    print(\"Molecule known to have lower toxicity:\\n>>> \"+y_smiles[i])\n",
    "                    display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                if print_flag:\n",
    "                    print(f\"Modifying the {generate_rdkit_index}-th atom to the element of {symbol_list[atom_symbol_generated]} does not meet the specification, generating the next element...\")\n",
    "                rank += 1\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                first_run_flag = False\n",
    "    recon_rate = recon_rate/total_atom\n",
    "    if global_step > 0:\n",
    "        logger.add_scalar('acc/recon_rate', recon_rate, global_step)\n",
    "    return success_smiles, init_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, \n",
    "         viz=False, print_flag=False, validate=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    init_smiles_list = []\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        if generate and validate:\n",
    "            matched_smiles_list = batch_df.matched_cano_smiles.values\n",
    "        else:\n",
    "            matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda, sigmoid=True)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-9),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:     \n",
    "                success_smiles_batch, init_smiles, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn, print_flag=print_flag)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            init_smiles_list.extend(init_smiles)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "            for n in range(topn):\n",
    "                success[n] += success_batch[n]\n",
    "                total[n] += total_batch[n]\n",
    "            if print_flag:\n",
    "                print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "        \n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "        if print_flag:\n",
    "            print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, init_smiles_list, generated_smiles,predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list, predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,predict_list\n",
    "    return predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 2000\n",
    "batch_size = 10\n",
    "patience = 200\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/G_ADMET_M_C_CYP1A2_inhibitor_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Rec: 0.0002 0.0008 0.0000 Vali: 0.3347 0.3333 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.5839 0.5833\n",
      "2 Rec: 0.0002 0.0008 0.0000 Vali: 0.4377 0.4667 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.6096 0.6167\n",
      "3 Rec: 0.3609 0.3838 0.4333 Vali: 0.5147 0.6000 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7246 0.7583\n",
      "EarlyStopping counter: 1 out of 200\n",
      "4 Rec: 0.4677 0.4948 0.5333 Vali: 0.3941 0.3667 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7222 0.7250\n",
      "EarlyStopping counter: 2 out of 200\n",
      "5 Rec: 0.4696 0.4964 0.5333 Vali: 0.3616 0.2333 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7145 0.6917\n",
      "6 Rec: 0.4713 0.4988 0.5333 Vali: 0.4623 0.4667 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7403 0.7500\n",
      "7 Rec: 0.4709 0.4980 0.5333 Vali: 0.6646 0.4667 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7906 0.7500\n",
      "EarlyStopping counter: 1 out of 200\n",
      "8 Rec: 0.4792 0.5059 0.5333 Vali: 0.6075 0.6667 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7784 0.8000\n",
      "EarlyStopping counter: 2 out of 200\n",
      "9 Rec: 0.4792 0.5059 0.5333 Vali: 0.4124 0.3333 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7296 0.7167\n",
      "EarlyStopping counter: 3 out of 200\n",
      "10 Rec: 0.4799 0.5059 0.5333 Vali: 0.4092 0.4000 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7288 0.7333\n",
      "EarlyStopping counter: 4 out of 200\n",
      "11 Rec: 0.4799 0.5059 0.5333 Vali: 0.2355 0.1667 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.6854 0.6750\n",
      "EarlyStopping counter: 5 out of 200\n",
      "12 Rec: 0.4799 0.5059 0.5333 Vali: 0.1594 0.0667 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.6663 0.6500\n",
      "EarlyStopping counter: 6 out of 200\n",
      "13 Rec: 0.7178 0.7201 0.7667 Vali: 0.1848 0.1667 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7262 0.7333\n",
      "EarlyStopping counter: 7 out of 200\n",
      "14 Rec: 0.7178 0.7201 0.7667 Vali: 0.2022 0.1333 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7306 0.7250\n",
      "EarlyStopping counter: 8 out of 200\n",
      "15 Rec: 0.7178 0.7201 0.7667 Vali: 0.3196 0.3000 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7599 0.7667\n",
      "EarlyStopping counter: 9 out of 200\n",
      "16 Rec: 0.7157 0.7177 0.7667 Vali: 0.1420 0.0667 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7149 0.7083\n",
      "EarlyStopping counter: 10 out of 200\n",
      "17 Rec: 0.7157 0.7177 0.7667 Vali: 0.1229 0.0000 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7102 0.6917\n",
      "EarlyStopping counter: 11 out of 200\n",
      "18 Rec: 0.7157 0.7177 0.7667 Vali: 0.1079 0.0000 Uni: 1.00 0.00 Nov: 1.00 0.00 Avg: 0.7064 0.1917\n",
      "EarlyStopping counter: 12 out of 200\n",
      "19 Rec: 0.7157 0.7177 0.7667 Vali: 0.1364 0.0333 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7135 0.7000\n",
      "EarlyStopping counter: 13 out of 200\n",
      "20 Rec: 0.7157 0.7177 0.7667 Vali: 0.2458 0.3333 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7409 0.7750\n",
      "EarlyStopping counter: 14 out of 200\n",
      "21 Rec: 0.8322 0.8200 0.8000 Vali: 0.2181 0.2000 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7595 0.7500\n",
      "EarlyStopping counter: 15 out of 200\n",
      "22 Rec: 0.8322 0.8200 0.8000 Vali: 0.2760 0.4000 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7740 0.8000\n",
      "EarlyStopping counter: 16 out of 200\n",
      "23 Rec: 0.8342 0.8232 0.8000 Vali: 0.3093 0.3333 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7831 0.7833\n",
      "EarlyStopping counter: 17 out of 200\n",
      "24 Rec: 0.8344 0.8232 0.8000 Vali: 0.1301 0.0000 Uni: 1.00 0.00 Nov: 1.00 0.00 Avg: 0.7383 0.2000\n",
      "EarlyStopping counter: 18 out of 200\n",
      "25 Rec: 0.9410 0.9389 1.0000 Vali: 0.1499 0.0667 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7722 0.7667\n",
      "EarlyStopping counter: 19 out of 200\n",
      "26 Rec: 0.9410 0.9389 1.0000 Vali: 0.1435 0.1333 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7706 0.7833\n",
      "EarlyStopping counter: 20 out of 200\n",
      "27 Rec: 0.9410 0.9389 1.0000 Vali: 0.1998 0.1000 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7847 0.7750\n",
      "EarlyStopping counter: 21 out of 200\n",
      "28 Rec: 0.9410 0.9389 1.0000 Vali: 0.1324 0.1000 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7678 0.7750\n",
      "EarlyStopping counter: 22 out of 200\n",
      "29 Rec: 0.9410 0.9389 1.0000 Vali: 0.0658 0.0000 Uni: 1.00 0.00 Nov: 1.00 0.00 Avg: 0.7512 0.2500\n",
      "EarlyStopping counter: 23 out of 200\n",
      "30 Rec: 0.9410 0.9389 1.0000 Vali: 0.0436 0.0667 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7456 0.7667\n",
      "EarlyStopping counter: 24 out of 200\n",
      "31 Rec: 0.9410 0.9389 1.0000 Vali: 0.0309 0.0000 Uni: 1.00 0.00 Nov: 1.00 0.00 Avg: 0.7425 0.2500\n",
      "EarlyStopping counter: 25 out of 200\n",
      "32 Rec: 0.9410 0.9389 1.0000 Vali: 0.0436 0.0667 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7456 0.7667\n",
      "EarlyStopping counter: 26 out of 200\n",
      "33 Rec: 0.9410 0.9389 1.0000 Vali: 0.0325 0.0333 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7429 0.7583\n",
      "EarlyStopping counter: 27 out of 200\n",
      "34 Rec: 0.9410 0.9389 1.0000 Vali: 0.0357 0.1333 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7437 0.7833\n",
      "EarlyStopping counter: 28 out of 200\n",
      "35 Rec: 0.9410 0.9389 1.0000 Vali: 0.0389 0.0333 Uni: 1.00 1.00 Nov: 1.00 1.00 Avg: 0.7444 0.7583\n"
     ]
    }
   ],
   "source": [
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_recon, train_validity, train_unique, train_novelty, _, _, _, train_predict = eval(model, amodel, gmodel, train_df, topn=1, generate=True)\n",
    "    val_recon, val_validity, val_unique, val_novelty, _, _, _, val_predict = eval(model, amodel, gmodel, val_df, topn=1, generate=True)\n",
    "    \n",
    "    test_recon, test_validity, test_unique, test_novelty, _, _, _, test_predict = eval(model, amodel, gmodel, test_df, topn=1, generate=True)\n",
    "\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/reconstruction', val_recon, global_step)\n",
    "    logger.add_scalar('val/validity', val_validity, global_step)\n",
    "    logger.add_scalar('val/unique', val_unique, global_step)\n",
    "    logger.add_scalar('val/novelty', val_novelty, global_step)\n",
    "    logger.add_scalar('test/reconstruction', test_recon, global_step)\n",
    "    logger.add_scalar('test/validity', test_validity, global_step)\n",
    "    logger.add_scalar('test/unique', test_unique, global_step)\n",
    "    logger.add_scalar('test/novelty', test_novelty, global_step)\n",
    "    \n",
    "    val_average = (val_recon+val_validity+val_unique+val_novelty)/4\n",
    "    test_average = (test_recon+test_validity+test_unique+test_novelty)/4\n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    early_stop = stopper.step(val_average, model)\n",
    "    early_stop = stopper_afse.step(val_average, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(val_average, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print(epoch, 'Rec: %.4f'%train_recon,'%.4f'%val_recon,'%.4f'%test_recon, 'Vali: %.4f'%val_validity,'%.4f'%test_validity,\n",
    "         'Uni: %.2f'%val_unique,'%.2f'%test_unique, 'Nov: %.2f'%val_novelty,'%.2f'%test_novelty,\n",
    "         'Avg: %.4f'%val_average,'%.4f'%test_average)\n",
    "    if early_stop:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "\n",
    "torch.save({'model_state_dict': model.state_dict()}, model_file + '_model_test.pth')\n",
    "torch.save({'model_state_dict': amodel.state_dict()}, model_file + '_amodel_test.pth')\n",
    "torch.save({'model_state_dict': gmodel.state_dict()}, model_file + '_gmodel_test.pth')\n",
    "    \n",
    "reconstruction_rate, success_validity, unique_rate, novelty_rate, success_smiles, init_smiles, generated_smiles, test_predict = eval(model, amodel, gmodel, test_df, topn=1, generate=True, validate=True, print_flag=True)\n",
    "\n",
    "test_predict = np.array(test_predict)\n",
    "\n",
    "print(\"----------\")\n",
    "print('Reconstruction: %.4f'%reconstruction_rate,'\\nValidity: %.4f'%success_validity)\n",
    "print('Unique: %.4f'%unique_rate,'\\nNovelty: %.4f'%novelty_rate, \n",
    "      '\\nAverage: %.4f'%((reconstruction_rate+success_validity+unique_rate+novelty_rate)/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(init_smiles))\n",
    "print(init_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(generated_smiles))\n",
    "print(generated_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(success_smiles))\n",
    "print(success_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "import os\n",
    "import os.path as op\n",
    " \n",
    "#get_sa_score start\n",
    "_fscores = None\n",
    " \n",
    "def readFragmentScores(name='fpscores'):\n",
    "    import gzip\n",
    "    global _fscores\n",
    "    # generate the full path filename:\n",
    "    if name == \"fpscores\":\n",
    "        name = op.join(os.getcwd(), name)\n",
    "        # name = op.join(op.dirname(__file__), name)\n",
    "    data = pickle.load(gzip.open('%s.pkl.gz' % name))\n",
    "    outDict = {}\n",
    "    for i in data:\n",
    "        for j in range(1, len(i)):\n",
    "            outDict[i[j]] = float(i[0])\n",
    "    _fscores = outDict\n",
    " \n",
    " \n",
    "def numBridgeheadsAndSpiro(mol, ri=None):\n",
    "    nSpiro = rdMolDescriptors.CalcNumSpiroAtoms(mol)\n",
    "    nBridgehead = rdMolDescriptors.CalcNumBridgeheadAtoms(mol)\n",
    "    return nBridgehead, nSpiro\n",
    " \n",
    "def calculateScore(m):\n",
    "    if _fscores is None:\n",
    "        readFragmentScores()\n",
    " \n",
    "    # fragment score\n",
    "    fp = rdMolDescriptors.GetMorganFingerprint(m,\n",
    "                                            2)  # <- 2 is the *radius* of the circular fingerprint\n",
    "    fps = fp.GetNonzeroElements()\n",
    "    score1 = 0.\n",
    "    nf = 0\n",
    "    for bitId, v in fps.items():\n",
    "        nf += v\n",
    "        sfp = bitId\n",
    "        score1 += _fscores.get(sfp, -4) * v\n",
    "    score1 /= nf\n",
    " \n",
    "    # features score\n",
    "    nAtoms = m.GetNumAtoms()\n",
    "    nChiralCenters = len(Chem.FindMolChiralCenters(m, includeUnassigned=True))\n",
    "    ri = m.GetRingInfo()\n",
    "    nBridgeheads, nSpiro = numBridgeheadsAndSpiro(m, ri)\n",
    "    nMacrocycles = 0\n",
    "    for x in ri.AtomRings():\n",
    "        if len(x) > 8:\n",
    "            nMacrocycles += 1\n",
    " \n",
    "    sizePenalty = nAtoms**1.005 - nAtoms\n",
    "    stereoPenalty = math.log10(nChiralCenters + 1)\n",
    "    spiroPenalty = math.log10(nSpiro + 1)\n",
    "    bridgePenalty = math.log10(nBridgeheads + 1)\n",
    "    macrocyclePenalty = 0.\n",
    "    # ---------------------------------------\n",
    "    # This differs from the paper, which defines:\n",
    "    # macrocyclePenalty = math.log10(nMacrocycles+1)\n",
    "    # This form generates better results when 2 or more macrocycles are present\n",
    "    if nMacrocycles > 0:\n",
    "        macrocyclePenalty = math.log10(2)\n",
    " \n",
    "    score2 = 0. - sizePenalty - stereoPenalty - spiroPenalty - bridgePenalty - macrocyclePenalty\n",
    " \n",
    "    # correction for the fingerprint density\n",
    "    # not in the original publication, added in version 1.1\n",
    "    # to make highly symmetrical molecules easier to synthetise\n",
    "    score3 = 0.\n",
    "    if nAtoms > len(fps):\n",
    "        score3 = math.log(float(nAtoms) / len(fps)) * .5\n",
    " \n",
    "    sascore = score1 + score2 + score3\n",
    " \n",
    "    # need to transform \"raw\" value into scale between 1 and 10\n",
    "    min = -4.0\n",
    "    max = 2.5\n",
    "    sascore = 11. - (sascore - min + 1) / (max - min) * 9.\n",
    "    # smooth the 10-end\n",
    "    if sascore > 8.:\n",
    "        sascore = 8. + math.log(sascore + 1. - 9.)\n",
    "    if sascore > 10.:\n",
    "        sascore = 10.0\n",
    "    elif sascore < 1.:\n",
    "        sascore = 1.0\n",
    " \n",
    "    return sascore\n",
    "\n",
    "def SA_score(mols:list):\n",
    "    ss = []\n",
    "    drop_index = []\n",
    "    readFragmentScores(\"fpscores\")\n",
    "#     print('smiles\\tsa_score')\n",
    "    for i, m in enumerate(mols):\n",
    "        try:\n",
    "            s = calculateScore(m)\n",
    "            smiles = Chem.MolToSmiles(m)\n",
    "            ss.append(s)\n",
    "        except:\n",
    "            print(f'Cannot read {m}')\n",
    "            drop_index.append(i)\n",
    "    return drop_index, ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QED_scores_init = []\n",
    "logP_scores_init = []\n",
    "init_mols = []\n",
    "smiles_num = len(init_smiles)\n",
    "init_smiles_new = init_smiles\n",
    "for i in range(smiles_num):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(init_smiles[i])\n",
    "        init_mols.append(mol)\n",
    "        QED = Chem.QED.default(mol)\n",
    "        QED_scores_init.append(QED)\n",
    "        logP = Chem.Crippen.MolLogP(mol)\n",
    "        logP_scores_init.append(logP)\n",
    "    except:\n",
    "        print(f'Cannot read {init_smiles[i]}')\n",
    "        init_smiles_new.pop(i)\n",
    "init_mols_new = init_mols\n",
    "drop_index, SA_scores_init = np.array(SA_score(init_mols))\n",
    "for i in drop_index:\n",
    "    init_mols_new.pop(i)\n",
    "    init_smiles_new.pop(i)\n",
    "QED_scores_init = np.array(QED_scores_init)\n",
    "QED_scores_init_ranked = np.sort(QED_scores_init)\n",
    "logP_scores_init = np.array(logP_scores_init)\n",
    "\n",
    "QED_scores = []\n",
    "logP_scores = []\n",
    "generated_mols = []\n",
    "smiles_num = len(generated_smiles)\n",
    "generated_smiles_new = generated_smiles\n",
    "for i in range(smiles_num):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(generated_smiles[i])\n",
    "        generated_mols.append(mol)\n",
    "        QED = Chem.QED.default(mol)\n",
    "        QED_scores.append(QED)\n",
    "        logP = Chem.Crippen.MolLogP(mol)\n",
    "        logP_scores.append(logP)\n",
    "    except:\n",
    "        print(f'Cannot read {generated_smiles[i]}')\n",
    "        generated_smiles_new.pop(i)\n",
    "        init_smiles_new.pop(i)\n",
    "        init_mols_new.pop(i)\n",
    "generated_mols_new = generated_mols\n",
    "drop_index, SA_scores = np.array(SA_score(generated_mols))\n",
    "for i in drop_index:\n",
    "    generated_smiles_new.pop(i)\n",
    "    generated_mols_new.pop(i)\n",
    "    init_smiles_new.pop(i)\n",
    "    init_mols_new.pop(i)\n",
    "QED_scores = np.array(QED_scores)\n",
    "QED_scores_ranked = np.sort(QED_scores)\n",
    "logP_scores = np.array(logP_scores)\n",
    "print('QED Rank:', QED_scores_ranked[-5:],\n",
    "      '\\nQED（越大类药性越高）: %.4f'%max(QED_scores), '%.4f'%(sum(QED_scores)/smiles_num),'%.4f'%min(QED_scores),\n",
    "      '\\nSA（越小可合成性越高）: %.4f'%max(SA_scores), '%.4f'%(sum(SA_scores)/smiles_num),'%.4f'%min(SA_scores),\n",
    "      '\\nlogP（越大疏水性越好）: %.4f'%max(logP_scores), '%.4f'%(sum(logP_scores)/smiles_num),'%.4f'%min(logP_scores),\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "I_SMILES_Dict = {init_smiles_new[i]: [QED_scores_init[i], SA_scores_init[i], logP_scores_init[i]] for i in range(len(init_smiles_new))}\n",
    "G_SMILES_Dict = {generated_smiles_new[i]: [QED_scores[i], SA_scores[i], logP_scores[i]] for i in range(len(generated_smiles_new))}\n",
    "for i, smiles in enumerate(success_smiles):\n",
    "    try:\n",
    "        index = generated_smiles_new.index(smiles)\n",
    "        for j in range(len(init_smiles_new)):\n",
    "            same = 0\n",
    "            anchor = smiles\n",
    "            smi = init_smiles_new[j]\n",
    "            length = np.min([len(anchor),len(smi)])\n",
    "            for k in range(length):\n",
    "                if anchor[k] == smi[k]:\n",
    "                    same += 1\n",
    "            if same == length-1 and len(anchor)==len(smi):\n",
    "                matched_init_smiles = smi\n",
    "        print('原始测试集分子：',matched_init_smiles,\n",
    "              '\\n原始测试集分子毒性：%.4f'%(train_df['value'].values[train_df.cano_smiles.values==matched_init_smiles][0].astype(float)),\n",
    "             '\\nQED（越大类药性越高）: %.4f'%I_SMILES_Dict[init_smiles_new[index]][0],\n",
    "              '\\nSA（越小可合成性越高）: %.4f'%I_SMILES_Dict[init_smiles_new[index]][1],\n",
    "              '\\nlogP（越大疏水性越好）: %.4f'%I_SMILES_Dict[init_smiles_new[index]][2],\n",
    "             '\\n\\n成功生成的高活性分子：',smiles,\n",
    "              '\\n毒性（1有毒0无毒）: %.4f'%(test_df['value'].values[test_df.cano_smiles.values==smiles][0].astype(float)),\n",
    "             '\\nQED（越大类药性越高）: %.4f'%G_SMILES_Dict[smiles][0],\n",
    "              '\\nSA（越小可合成性越高）: %.4f'%G_SMILES_Dict[smiles][1],\n",
    "              '\\nlogP（越大疏水性越好）: %.4f'%G_SMILES_Dict[smiles][2],\n",
    "             '\\n------------------------------------------------------------------------------------------------------------')\n",
    "    except ValueError as e:\n",
    "        print(f'Cannot read {smiles}')\n",
    "        print(sys.exc_info())\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_step = 1\n",
    "# test_predict = []\n",
    "# generated_smiles = []\n",
    "# stopper.load_checkpoint(model)\n",
    "# stopper_afse.load_checkpoint(amodel)\n",
    "# stopper_generate.load_checkpoint(gmodel)\n",
    "# total_smilesList = train_df['smiles'].values\n",
    "# feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "# generated_df = train_df\n",
    "# for step in range(total_step):\n",
    "#     _, _, _, _, _, _, generated_smiles_step, _, _, test_predict_step = eval(model, amodel, gmodel, generated_df, topn=1, generate=True)\n",
    "#     test_predict.extend(test_predict_step)\n",
    "#     generated_df = pd.DataFrame([])\n",
    "#     generated_df['smiles'] = generated_smiles_step\n",
    "#     generated_df['value'] = 0\n",
    "#     generated_df = add_canonical_smiles(generated_df)\n",
    "#     feature_dicts = save_smiles_dicts(generated_df['smiles'],filename)\n",
    "#     generated_df = generated_df[generated_df[\"smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "#     generated_df = generated_df.reset_index(drop=True)\n",
    "#     generated_smiles.extend(generated_df['smiles'])\n",
    "# test_predict = np.array(test_predict)\n",
    "# print(len(test_predict),len(generated_smiles))\n",
    "# QED_scores = []\n",
    "# logP_scores = []\n",
    "# generated_mols = []\n",
    "# smiles_num = len(generated_smiles)\n",
    "# for i in range(smiles_num):\n",
    "#     mol = Chem.MolFromSmiles(generated_smiles[i])\n",
    "#     if mol is not None:\n",
    "#         generated_mols.append(mol)\n",
    "#         QED = Chem.QED.default(mol)\n",
    "#         QED_scores.append(QED)\n",
    "#         logP = Chem.Crippen.MolLogP(mol)\n",
    "#         logP_scores.append(logP)\n",
    "# SA_scores = np.array(SA_score(generated_mols))\n",
    "# QED_scores = np.array(QED_scores)\n",
    "# QED_scores_ranked = np.sort(QED_scores)\n",
    "# logP_scores = np.array(logP_scores)\n",
    "# print('QED Rank:', QED_scores_ranked[-5:],\n",
    "#       '\\nQED（越大类药性越高）: %.4f'%max(QED_scores), '%.4f'%(sum(QED_scores)/smiles_num),'%.4f'%min(QED_scores),\n",
    "#       '\\nSA（越小可合成性越高）: %.4f'%max(SA_scores), '%.4f'%(sum(SA_scores)/smiles_num),'%.4f'%min(SA_scores),\n",
    "#       '\\nlogP（越大疏水性越好）: %.4f'%max(logP_scores), '%.4f'%(sum(logP_scores)/smiles_num),'%.4f'%min(logP_scores),\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_df = pd.DataFrame([])\n",
    "# generated_df['smiles'] = generated_smiles\n",
    "# generated_df = add_canonical_smiles(generated_df)\n",
    "# generated_df = generated_df.reset_index(drop=True)\n",
    "# feature_dicts = save_smiles_dicts(generated_df['smiles'],filename)\n",
    "# x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(generated_df['smiles'],feature_dicts)\n",
    "# x_atom = torch.Tensor(x_atom)\n",
    "# x_bonds = torch.Tensor(x_bonds)\n",
    "# x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "# mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask))\n",
    "# eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=10**-learning_rate)\n",
    "# mol_prediction = np.array(mol_prediction.detach().cpu())\n",
    "# test_predict_scores = mol_prediction\n",
    "# generated_df['Predicted_Activity_Score'] = test_predict_scores\n",
    "# generated_df['QED'] = QED_scores\n",
    "# generated_df['SA'] = SA_scores\n",
    "# generated_df['logP'] = logP_scores\n",
    "# generated_df.to_csv(path_or_buf='./result/'+file1+'_generated_molecules.csv')\n",
    "print(len(test_predict),len(generated_mols))\n",
    "# G_SMILES_Dict = {generated_mols[i]: [QED_scores[i], SA_scores[i], logP_scores[i]] for i in range(len(generated_mols))}\n",
    "# for i, mol in enumerate(generated_mols):\n",
    "#     index = generated_mols.index(mol)\n",
    "#     display(mol)\n",
    "#     print('成功生成的高活性分子：',Chem.MolToSmiles(mol),\n",
    "#           '\\nActivity（越大药效越好）: %.4f'%test_predict_scores[index],\n",
    "#          '\\nQED（越大类药性越高）: %.4f'%G_SMILES_Dict[mol][0],\n",
    "#           '\\nSA（越小可合成性越高）: %.4f'%G_SMILES_Dict[mol][1],\n",
    "#           '\\nlogP（越大疏水性越好）: %.4f'%G_SMILES_Dict[mol][2],\n",
    "#          '\\n------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopper.load_checkpoint(model)\n",
    "# stopper_afse.load_checkpoint(amodel)\n",
    "# stopper_generate.load_checkpoint(gmodel)\n",
    "# _, _, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "# test_predict = np.array(test_predict)\n",
    "# test_df['predict_value'] = test_predict\n",
    "# test_df.to_csv(path_or_buf='./result/'+file1+'_test_predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',task_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez('./result/G_AFSE_'+file1, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load('./result/G_AFSE_'+file1+\".npz\")\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重构原子类型\n",
    "# activated_features=activated_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
