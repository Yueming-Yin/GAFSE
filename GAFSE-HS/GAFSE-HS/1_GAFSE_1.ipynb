{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "torch.manual_seed(8)\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ki_Q95136_1_100\n",
      "model_file/1_GAFSE_Ki_Q95136_1_100_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/Ki_Q95136_1_100_train.csv\"\n",
    "test_filename = \"./data/benchmark/Ki_Q95136_1_100_test.csv\"\n",
    "test_active = 100\n",
    "val_rate = 0.2\n",
    "random_seed = 68\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/1_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"1_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/1_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0   C1CN(CCN1CC2=CC3=C(N2)C=CC(=C3)C#N)C4=CC=CC=C4Cl -3.875061\n",
      "1               CN1CCN2C(C1)C3=CC=CC=C3CC4=CC=CC=C42 -3.148911\n",
      "2  CCC(CO)NC(=O)C1CN(C2CC3=CN(C4=CC=CC(=C34)C2=C1... -2.971276\n",
      "3  CC1=CC2=NC(=C(N2C(=N1)C)CN3CCN(CC3)C4=CC(=C(C=... -3.322219\n",
      "4                               C1=CC(=C(C=C1CCN)O)O -2.643453\n",
      "number of all smiles:  74\n",
      "number of successfully processed smiles:  74\n",
      "                                              smiles     value  \\\n",
      "0   C1CN(CCN1CC2=CC3=C(N2)C=CC(=C3)C#N)C4=CC=CC=C4Cl -3.875061   \n",
      "1               CN1CCN2C(C1)C3=CC=CC=C3CC4=CC=CC=C42 -3.148911   \n",
      "2  CCC(CO)NC(=O)C1CN(C2CC3=CN(C4=CC=CC(=C34)C2=C1... -2.971276   \n",
      "3  CC1=CC2=NC(=C(N2C(=N1)C)CN3CCN(CC3)C4=CC(=C(C=... -3.322219   \n",
      "4                               C1=CC(=C(C=C1CCN)O)O -2.643453   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0         N#Cc1ccc2[nH]c(CN3CCN(c4ccccc4Cl)CC3)cc2c1  \n",
      "1                       CN1CCN2c3ccccc3Cc3ccccc3C2C1  \n",
      "2       CCC(CO)NC(=O)C1C=C2c3cccc4c3c(cn4C)CC2N(C)C1  \n",
      "3  Cc1cc2nc(C)c(CN3CCN(c4ccc(Cl)c(Cl)c4)CC3)n2c(C)n1  \n",
      "4                                  NCCc1ccc(O)c(O)c1  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  268\n",
      "number of successfully processed smiles:  268\n",
      "(268, 3)\n",
      "                                              smiles     value  \\\n",
      "0  C1CN(CC=C1N2C3=CC=CC=C3NC2=O)CCCC(=O)C4=CC=C(C... -2.944483   \n",
      "1             CN1CCC2=CC=CC3=C2C1CC4=C3C(=C(C=C4)O)O -0.903090   \n",
      "2  C1CN(CCN1CC2=CNC3=C2C=C(C=C3)C#N)C4=CC(=C(C=C4... -2.653213   \n",
      "3  CCCC1CN(CC1CNC(=O)C2=CC(=C(C=C2OC)NC)Cl)CC3=CC... -3.447158   \n",
      "4  C1CC2=CC(=C(CCC3=CC=C1C=C3)C=C2)N4CCN(CC4)CC5=... -3.414973   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  O=C(CCCN1CC=C(n2c(=O)[nH]c3ccccc32)CC1)c1ccc(F...  \n",
      "1                  CN1CCc2cccc3c2C1Cc1ccc(O)c(O)c1-3  \n",
      "2   N#Cc1ccc2[nH]cc(CN3CCN(c4ccc(Cl)c(Cl)c4)CC3)c2c1  \n",
      "3     CCCC1CN(Cc2ccccc2)CC1CNC(=O)c1cc(Cl)c(NC)cc1OC  \n",
      "4  c1ccc(-n2cc(CN3CCN(c4cc5ccc4CCc4ccc(cc4)CC5)CC...  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/Ki_Q95136_1_100_train.pickle\n",
      "./data/benchmark/Ki_Q95136_1_100_train\n",
      "342\n",
      "feature dicts file saved as ./data/benchmark/Ki_Q95136_1_100_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 3) (15, 3) (268, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    [d,e,f,g] = bond_neighbor.shape\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    one_hot_loss = 0\n",
    "    interger_loss = 0\n",
    "    binary_loss = 0\n",
    "    counter_i = 0\n",
    "    counter_j = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        one_hot_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-6)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)+\\\n",
    "                         weighted_CE_loss(atom_list[i,:l,16:22], x_atom[i,:l,16:22])+ \\\n",
    "                         weighted_CE_loss(atom_list[i,:l,24:30], x_atom[i,:l,24:30])+ \\\n",
    "                         weighted_CE_loss(atom_list[i,:l,31:36], x_atom[i,:l,31:36])\n",
    "        interger_loss += loss_function(atom_list[i,:l,23], x_atom[i,:l,23])+ \\\n",
    "                        loss_function(atom_list[i,:l,24], x_atom[i,:l,24])\n",
    "        binary_loss += CE(atom_list[i,:l,30], x_atom[i,:l,30])+ \\\n",
    "                        CE(atom_list[i,:l,36], x_atom[i,:l,36])+ \\\n",
    "                        CE(atom_list[i,:l,37], x_atom[i,:l,37])+ \\\n",
    "                        CE(atom_list[i,:l,38], x_atom[i,:l,38])\n",
    "        counter_i += 1\n",
    "        for j in range(l):\n",
    "            n = (bond_neighbor[i,j].sum(-1)!=0).sum(-1)\n",
    "            if n==0:\n",
    "                continue\n",
    "            one_hot_loss += weighted_CE_loss(bond_list[i,j,:n,:4], bond_neighbor[i,j,:n,:4])+ \\\n",
    "                             weighted_CE_loss(bond_list[i,j,:n,6:], bond_neighbor[i,j,:n,6:])\n",
    "            binary_loss += CE(bond_neighbor[i,j,:n,4], bond_list[i,j,:n,4])+ \\\n",
    "                           CE(bond_neighbor[i,j,:n,5], bond_list[i,j,:n,5])\n",
    "            counter_j += 1\n",
    "    one_hot_loss = one_hot_loss/(5*counter_i+2*counter_j)\n",
    "    interger_loss = interger_loss/(2*counter_i)\n",
    "    binary_loss = binary_loss/(4*counter_i+2*counter_j)\n",
    "    total_loss = (one_hot_loss + interger_loss + binary_loss)/3\n",
    "    return total_loss, one_hot_loss, interger_loss, binary_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "#         atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "#                                       torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "#         success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "#                             bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "#                                                      refer_atom_list, refer_bond_list,topn=1)\n",
    "#         reconstruction_loss, one_hot_loss, interger_loss,binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, \n",
    "#                                                                                               bond_neighbor, validity_mask, atom_list, \n",
    "#                                                                                               bond_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "#         atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "#                                                 torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "#                                                 mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "#         refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "#                                                             torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "#                                                             mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "#         success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "#                             bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "#                                                      refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "#         test_reconstruction_loss, test_one_hot_loss, test_interger_loss,test_binary_loss = generate_loss_function(atom_list_test, x_atom_test, bond_list_test, bond_neighbor_test, validity_mask_test, atom_list_test, bond_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        max_lr = 1e-3\n",
    "        conv_lr = conv_lr - conv_lr**2 + 0.06 * punish_lr\n",
    "        if conv_lr < max_lr and conv_lr >= 0:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = conv_lr.detach()\n",
    "                AFSE_lr = conv_lr    \n",
    "        elif conv_lr < 0:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = 0\n",
    "                AFSE_lr = 0\n",
    "        elif conv_lr >= max_lr:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = max_lr\n",
    "                AFSE_lr = max_lr\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_one_hot', one_hot_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_interger', interger_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_binary', binary_loss, global_step)\n",
    "        logger.add_scalar('lr/max_lr', conv_lr, global_step)\n",
    "        logger.add_scalar('lr/punish_lr', punish_lr, global_step)\n",
    "        logger.add_scalar('lr/AFSE_lr', AFSE_lr, global_step)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "#         optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6*(vat_loss + test_vat_loss) # + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "#         optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 10\n",
    "patience = 100\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/1_GAFSE_Ki_Q95136_1_100_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 26 Index:-0.7316 R2:0.0019 0.0068 0.0914 RMSE:1.0243 0.7017 0.7864 Tau:0.1002 -0.1810 -0.2784\n",
      "Epoch: 2 Step: 52 Index:-0.4044 R2:0.0595 0.0006 0.2050 RMSE:0.9118 0.5860 0.7076 Tau:0.1975 0.0095 -0.2469\n",
      "Epoch: 3 Step: 78 Index:-0.0705 R2:0.3745 0.0843 0.3025 RMSE:0.7778 0.6564 0.7577 Tau:0.4578 0.2952 -0.1218\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 4 Step: 104 Index:-0.0838 R2:0.4433 0.1171 0.1997 RMSE:0.6886 0.5712 0.7374 Tau:0.4543 0.2381 -0.0829\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 5 Step: 130 Index:-0.0747 R2:0.4556 0.1232 0.2138 RMSE:0.6735 0.5621 0.7249 Tau:0.4613 0.2381 -0.0801\n",
      "Epoch: 6 Step: 156 Index:-0.0568 R2:0.4656 0.1338 0.2188 RMSE:0.6658 0.5663 0.7260 Tau:0.4578 0.2571 -0.0710\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 7 Step: 182 Index:-0.0578 R2:0.4793 0.1484 0.2327 RMSE:0.6666 0.5885 0.7340 Tau:0.4695 0.2762 -0.0627\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 8 Step: 208 Index:-0.0815 R2:0.4864 0.1574 0.2336 RMSE:0.6736 0.6134 0.7495 Tau:0.4730 0.2762 -0.0474\n",
      "Epoch: 9 Step: 234 Index:0.0032 R2:0.5020 0.1723 0.2664 RMSE:0.6456 0.5326 0.6883 Tau:0.4859 0.2762 -0.0548\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 10 Step: 260 Index:-0.0729 R2:0.5057 0.1800 0.2508 RMSE:0.6483 0.5857 0.7267 Tau:0.4836 0.2571 -0.0347\n",
      "Epoch: 11 Step: 286 Index:0.0230 R2:0.5223 0.1922 0.2857 RMSE:0.6349 0.5167 0.6758 Tau:0.4953 0.2762 -0.0411\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 12 Step: 312 Index:0.0067 R2:0.5301 0.2035 0.3262 RMSE:0.7203 0.5418 0.7114 Tau:0.5188 0.2762 -0.0482\n",
      "Epoch: 13 Step: 338 Index:0.0417 R2:0.5422 0.2205 0.2959 RMSE:0.6220 0.5402 0.6820 Tau:0.4988 0.3143 -0.0233\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 14 Step: 364 Index:0.0268 R2:0.5506 0.2321 0.2962 RMSE:0.6118 0.5353 0.6810 Tau:0.5059 0.2952 -0.0160\n",
      "Epoch: 15 Step: 390 Index:0.0789 R2:0.5606 0.2514 0.3079 RMSE:0.6028 0.5141 0.6705 Tau:0.5176 0.3143 -0.0131\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 16 Step: 416 Index:0.0441 R2:0.5719 0.2486 0.3238 RMSE:0.5964 0.5247 0.6623 Tau:0.5188 0.2952 -0.0095\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 17 Step: 442 Index:0.0172 R2:0.5787 0.2564 0.3272 RMSE:0.6000 0.5516 0.6736 Tau:0.5199 0.2952 0.0010\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 18 Step: 468 Index:0.0634 R2:0.5814 0.2631 0.3289 RMSE:0.5919 0.5295 0.6638 Tau:0.5223 0.3143 -0.0016\n",
      "Epoch: 19 Step: 494 Index:0.0902 R2:0.5969 0.2769 0.3616 RMSE:0.5948 0.4826 0.6369 Tau:0.5504 0.2952 -0.0048\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 20 Step: 520 Index:0.0847 R2:0.6057 0.2862 0.3482 RMSE:0.5775 0.5312 0.6559 Tau:0.5340 0.3333 0.0108\n",
      "Epoch: 21 Step: 546 Index:0.1125 R2:0.6155 0.3111 0.3704 RMSE:0.5737 0.4832 0.6363 Tau:0.5633 0.3143 0.0014\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 22 Step: 572 Index:0.1046 R2:0.6266 0.3227 0.3695 RMSE:0.5582 0.4911 0.6373 Tau:0.5586 0.3143 0.0184\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 23 Step: 598 Index:0.0937 R2:0.6423 0.3426 0.3943 RMSE:0.5502 0.5020 0.6254 Tau:0.5797 0.3143 0.0239\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 24 Step: 624 Index:0.0453 R2:0.6421 0.3505 0.3630 RMSE:0.5649 0.5706 0.6783 Tau:0.5598 0.3333 0.0360\n",
      "Epoch: 25 Step: 650 Index:0.1956 R2:0.6610 0.3686 0.3970 RMSE:0.5340 0.4851 0.6218 Tau:0.5821 0.3905 0.0294\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 26 Step: 676 Index:0.0045 R2:0.6671 0.3742 0.3870 RMSE:0.5649 0.5896 0.6758 Tau:0.5739 0.3143 0.0489\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 27 Step: 702 Index:0.1400 R2:0.6812 0.3844 0.4124 RMSE:0.5162 0.4836 0.6153 Tau:0.5903 0.3333 0.0435\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 28 Step: 728 Index:0.1715 R2:0.6943 0.4093 0.4351 RMSE:0.5360 0.4521 0.6146 Tau:0.6079 0.3333 0.0398\n",
      "Epoch: 29 Step: 754 Index:0.2290 R2:0.7073 0.4234 0.4441 RMSE:0.5318 0.4405 0.6075 Tau:0.6114 0.3714 0.0451\n",
      "Epoch: 30 Step: 780 Index:0.2609 R2:0.7104 0.4292 0.4572 RMSE:0.5418 0.4399 0.6102 Tau:0.6149 0.3905 0.0477\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 31 Step: 806 Index:0.2546 R2:0.7212 0.4349 0.4520 RMSE:0.5179 0.4462 0.6074 Tau:0.6172 0.3905 0.0500\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 32 Step: 832 Index:0.2456 R2:0.7331 0.4455 0.4261 RMSE:0.4816 0.4627 0.6109 Tau:0.6161 0.4095 0.0645\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 33 Step: 858 Index:0.2517 R2:0.7383 0.4493 0.4287 RMSE:0.4676 0.4879 0.6141 Tau:0.6254 0.4286 0.0632\n",
      "Epoch: 34 Step: 884 Index:0.2731 R2:0.7390 0.4547 0.4822 RMSE:0.5658 0.4625 0.6333 Tau:0.6419 0.4286 0.0332\n",
      "Epoch: 35 Step: 910 Index:0.3056 R2:0.7564 0.4550 0.4564 RMSE:0.4947 0.4530 0.6112 Tau:0.6430 0.4476 0.0508\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 36 Step: 936 Index:0.2774 R2:0.7603 0.4587 0.4479 RMSE:0.5109 0.4812 0.6446 Tau:0.6395 0.4476 0.0528\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 37 Step: 962 Index:0.1351 R2:0.7318 0.4539 0.5015 RMSE:0.7449 0.5976 0.7774 Tau:0.6512 0.4286 0.0184\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 38 Step: 988 Index:0.2734 R2:0.7612 0.4403 0.4616 RMSE:0.4795 0.4623 0.6005 Tau:0.6454 0.4286 0.0408\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 39 Step: 1014 Index:0.2242 R2:0.7726 0.4520 0.4424 RMSE:0.4433 0.4924 0.6046 Tau:0.6454 0.4095 0.0520\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 40 Step: 1040 Index:0.2423 R2:0.7767 0.4473 0.4565 RMSE:0.5448 0.4934 0.6639 Tau:0.6547 0.4286 0.0467\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 41 Step: 1066 Index:0.1556 R2:0.7779 0.4405 0.4099 RMSE:0.4647 0.6030 0.6670 Tau:0.6465 0.4476 0.0679\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 42 Step: 1092 Index:0.2415 R2:0.7907 0.4429 0.4447 RMSE:0.4350 0.4942 0.6144 Tau:0.6571 0.4286 0.0545\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 43 Step: 1118 Index:0.1543 R2:0.7844 0.4354 0.4074 RMSE:0.4407 0.6043 0.6687 Tau:0.6477 0.4476 0.0703\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 44 Step: 1144 Index:0.2443 R2:0.8008 0.4440 0.4580 RMSE:0.4893 0.4913 0.6383 Tau:0.6782 0.4286 0.0470\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 45 Step: 1170 Index:0.1904 R2:0.8061 0.4436 0.4163 RMSE:0.4097 0.5453 0.6344 Tau:0.6594 0.4286 0.0630\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 46 Step: 1196 Index:0.1164 R2:0.7949 0.4223 0.3990 RMSE:0.4504 0.6612 0.6976 Tau:0.6559 0.4667 0.0725\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Epoch: 47 Step: 1222 Index:0.2344 R2:0.8172 0.4339 0.4290 RMSE:0.4147 0.5203 0.6329 Tau:0.6759 0.4476 0.0594\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Epoch: 48 Step: 1248 Index:0.2132 R2:0.8233 0.4423 0.4230 RMSE:0.4032 0.5224 0.6335 Tau:0.6770 0.4286 0.0642\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Epoch: 49 Step: 1274 Index:0.1674 R2:0.8249 0.4282 0.4192 RMSE:0.3912 0.5462 0.6438 Tau:0.6676 0.4095 0.0627\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Epoch: 50 Step: 1300 Index:0.2366 R2:0.8282 0.4344 0.4285 RMSE:0.4310 0.5181 0.6487 Tau:0.6864 0.4476 0.0558\n",
      "EarlyStopping counter: 16 out of 100\n",
      "Epoch: 51 Step: 1326 Index:0.1774 R2:0.8359 0.4320 0.4042 RMSE:0.3822 0.5773 0.6512 Tau:0.6864 0.4476 0.0668\n",
      "EarlyStopping counter: 17 out of 100\n",
      "Epoch: 52 Step: 1352 Index:0.1967 R2:0.8233 0.4274 0.4475 RMSE:0.4528 0.5199 0.6536 Tau:0.6887 0.4095 0.0625\n",
      "EarlyStopping counter: 18 out of 100\n",
      "Epoch: 53 Step: 1378 Index:0.2108 R2:0.8339 0.4398 0.4425 RMSE:0.4561 0.5219 0.6614 Tau:0.6958 0.4286 0.0566\n",
      "EarlyStopping counter: 19 out of 100\n",
      "Epoch: 54 Step: 1404 Index:0.1768 R2:0.8360 0.4264 0.4143 RMSE:0.3754 0.5588 0.6435 Tau:0.6829 0.4286 0.0650\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Epoch: 55 Step: 1430 Index:0.1083 R2:0.8361 0.4122 0.3960 RMSE:0.4062 0.6625 0.6947 Tau:0.6735 0.4667 0.0740\n",
      "EarlyStopping counter: 21 out of 100\n",
      "Epoch: 56 Step: 1456 Index:0.1491 R2:0.8483 0.4227 0.4029 RMSE:0.3637 0.5645 0.6577 Tau:0.6876 0.4095 0.0661\n",
      "EarlyStopping counter: 22 out of 100\n",
      "Epoch: 57 Step: 1482 Index:0.0995 R2:0.8445 0.4170 0.3839 RMSE:0.3710 0.6332 0.6919 Tau:0.6911 0.4286 0.0751\n",
      "EarlyStopping counter: 23 out of 100\n",
      "Epoch: 58 Step: 1508 Index:0.1128 R2:0.8526 0.4101 0.3998 RMSE:0.3532 0.6008 0.6678 Tau:0.6934 0.4095 0.0680\n",
      "EarlyStopping counter: 24 out of 100\n",
      "Epoch: 59 Step: 1534 Index:0.1170 R2:0.8551 0.4086 0.3940 RMSE:0.3506 0.6157 0.6768 Tau:0.6981 0.4286 0.0728\n",
      "EarlyStopping counter: 25 out of 100\n",
      "Epoch: 60 Step: 1560 Index:0.1264 R2:0.8596 0.4045 0.4025 RMSE:0.3584 0.5872 0.6767 Tau:0.7028 0.4095 0.0701\n",
      "EarlyStopping counter: 26 out of 100\n",
      "Epoch: 61 Step: 1586 Index:0.0722 R2:0.8590 0.4009 0.3868 RMSE:0.3490 0.6605 0.7025 Tau:0.6981 0.4286 0.0749\n",
      "EarlyStopping counter: 27 out of 100\n",
      "Epoch: 62 Step: 1612 Index:0.0884 R2:0.8655 0.4026 0.3897 RMSE:0.3382 0.6443 0.6935 Tau:0.7016 0.4286 0.0742\n",
      "EarlyStopping counter: 28 out of 100\n",
      "Epoch: 63 Step: 1638 Index:0.1024 R2:0.8644 0.4020 0.3901 RMSE:0.3645 0.6113 0.7155 Tau:0.7028 0.4095 0.0679\n",
      "EarlyStopping counter: 29 out of 100\n",
      "Epoch: 64 Step: 1664 Index:0.0649 R2:0.8688 0.3924 0.3723 RMSE:0.3302 0.6420 0.7085 Tau:0.7087 0.4095 0.0798\n",
      "EarlyStopping counter: 30 out of 100\n",
      "Epoch: 65 Step: 1690 Index:0.0782 R2:0.8723 0.3937 0.3821 RMSE:0.3287 0.6288 0.7018 Tau:0.7181 0.4095 0.0742\n",
      "EarlyStopping counter: 31 out of 100\n",
      "Epoch: 66 Step: 1716 Index:0.0761 R2:0.8750 0.3850 0.3802 RMSE:0.3269 0.6308 0.6969 Tau:0.7122 0.4095 0.0821\n",
      "EarlyStopping counter: 32 out of 100\n",
      "Epoch: 67 Step: 1742 Index:0.0827 R2:0.8803 0.3942 0.3763 RMSE:0.3235 0.6243 0.6991 Tau:0.7298 0.4095 0.0787\n",
      "EarlyStopping counter: 33 out of 100\n",
      "Epoch: 68 Step: 1768 Index:0.0580 R2:0.8810 0.3883 0.3620 RMSE:0.3192 0.6490 0.7159 Tau:0.7192 0.4095 0.0838\n",
      "EarlyStopping counter: 34 out of 100\n",
      "Epoch: 69 Step: 1794 Index:0.0892 R2:0.8827 0.3853 0.3816 RMSE:0.3333 0.6178 0.7069 Tau:0.7286 0.4095 0.0794\n",
      "EarlyStopping counter: 35 out of 100\n",
      "Epoch: 70 Step: 1820 Index:0.0823 R2:0.8843 0.3891 0.3828 RMSE:0.3744 0.6056 0.7237 Tau:0.7403 0.3905 0.0727\n",
      "EarlyStopping counter: 36 out of 100\n",
      "Epoch: 71 Step: 1846 Index:0.0047 R2:0.8861 0.3751 0.3616 RMSE:0.3127 0.6591 0.7286 Tau:0.7239 0.3714 0.0759\n",
      "EarlyStopping counter: 37 out of 100\n",
      "Epoch: 72 Step: 1872 Index:0.0311 R2:0.8830 0.3739 0.3796 RMSE:0.3297 0.6327 0.7155 Tau:0.7181 0.3714 0.0720\n",
      "EarlyStopping counter: 38 out of 100\n",
      "Epoch: 73 Step: 1898 Index:-0.0624 R2:0.8826 0.3802 0.3558 RMSE:0.3659 0.7452 0.7619 Tau:0.7227 0.3905 0.0853\n",
      "EarlyStopping counter: 39 out of 100\n",
      "Epoch: 74 Step: 1924 Index:0.0506 R2:0.8878 0.3683 0.3649 RMSE:0.3633 0.6373 0.7447 Tau:0.7333 0.3905 0.0683\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Epoch: 75 Step: 1950 Index:-0.0168 R2:0.8858 0.3656 0.3567 RMSE:0.3148 0.6996 0.7412 Tau:0.7216 0.3905 0.0910\n",
      "EarlyStopping counter: 41 out of 100\n",
      "Epoch: 76 Step: 1976 Index:-0.0085 R2:0.8964 0.3721 0.3518 RMSE:0.2990 0.6914 0.7415 Tau:0.7415 0.3905 0.0886\n",
      "EarlyStopping counter: 42 out of 100\n",
      "Epoch: 77 Step: 2002 Index:0.0303 R2:0.8969 0.3682 0.3659 RMSE:0.3147 0.6525 0.7382 Tau:0.7415 0.3905 0.0831\n",
      "EarlyStopping counter: 43 out of 100\n",
      "Epoch: 78 Step: 2028 Index:-0.0318 R2:0.8909 0.3708 0.3478 RMSE:0.3227 0.7337 0.7636 Tau:0.7321 0.4095 0.1006\n",
      "EarlyStopping counter: 44 out of 100\n",
      "Epoch: 79 Step: 2054 Index:-0.0260 R2:0.9001 0.3655 0.3456 RMSE:0.2895 0.7089 0.7573 Tau:0.7462 0.3905 0.0904\n",
      "EarlyStopping counter: 45 out of 100\n",
      "Epoch: 80 Step: 2080 Index:-0.0087 R2:0.8984 0.3577 0.3617 RMSE:0.2902 0.6916 0.7474 Tau:0.7403 0.3905 0.0930\n",
      "EarlyStopping counter: 46 out of 100\n",
      "Epoch: 81 Step: 2106 Index:0.0018 R2:0.9038 0.3647 0.3509 RMSE:0.2986 0.6620 0.7464 Tau:0.7509 0.3714 0.0938\n",
      "EarlyStopping counter: 47 out of 100\n",
      "Epoch: 82 Step: 2132 Index:-0.0257 R2:0.9027 0.3622 0.3475 RMSE:0.2837 0.7085 0.7640 Tau:0.7438 0.3905 0.0984\n",
      "EarlyStopping counter: 48 out of 100\n",
      "Epoch: 83 Step: 2158 Index:-0.0147 R2:0.9064 0.3630 0.3429 RMSE:0.2859 0.6785 0.7552 Tau:0.7544 0.3714 0.0972\n",
      "EarlyStopping counter: 49 out of 100\n",
      "Epoch: 84 Step: 2184 Index:-0.0330 R2:0.9064 0.3572 0.3462 RMSE:0.3463 0.6520 0.7703 Tau:0.7614 0.3333 0.0974\n",
      "EarlyStopping counter: 50 out of 100\n",
      "Epoch: 85 Step: 2210 Index:-0.0057 R2:0.9008 0.3684 0.3310 RMSE:0.3468 0.7076 0.8289 Tau:0.7474 0.4095 0.0893\n",
      "EarlyStopping counter: 51 out of 100\n",
      "Epoch: 86 Step: 2236 Index:-0.1354 R2:0.9063 0.3553 0.3340 RMSE:0.3558 0.8116 0.8038 Tau:0.7438 0.3905 0.1049\n",
      "EarlyStopping counter: 52 out of 100\n",
      "Epoch: 87 Step: 2262 Index:-0.0565 R2:0.9064 0.3624 0.3354 RMSE:0.2833 0.7136 0.7683 Tau:0.7474 0.3714 0.1036\n",
      "EarlyStopping counter: 53 out of 100\n",
      "Epoch: 88 Step: 2288 Index:-0.0229 R2:0.9087 0.3544 0.3524 RMSE:0.3479 0.6610 0.7770 Tau:0.7544 0.3524 0.0939\n",
      "EarlyStopping counter: 54 out of 100\n",
      "Epoch: 89 Step: 2314 Index:-0.0183 R2:0.9124 0.3615 0.3403 RMSE:0.2835 0.6821 0.7671 Tau:0.7567 0.3714 0.1002\n",
      "EarlyStopping counter: 55 out of 100\n",
      "Epoch: 90 Step: 2340 Index:-0.0477 R2:0.9121 0.3492 0.3445 RMSE:0.3290 0.6668 0.7729 Tau:0.7579 0.3333 0.0942\n",
      "EarlyStopping counter: 56 out of 100\n",
      "Epoch: 91 Step: 2366 Index:-0.0616 R2:0.9094 0.3463 0.3328 RMSE:0.2896 0.7378 0.7725 Tau:0.7532 0.3905 0.1080\n",
      "EarlyStopping counter: 57 out of 100\n",
      "Epoch: 92 Step: 2392 Index:-0.0647 R2:0.9134 0.3644 0.3404 RMSE:0.3468 0.6838 0.8054 Tau:0.7567 0.3333 0.0921\n",
      "EarlyStopping counter: 58 out of 100\n",
      "Epoch: 93 Step: 2418 Index:-0.0275 R2:0.9138 0.3473 0.3415 RMSE:0.2921 0.6656 0.7512 Tau:0.7673 0.3524 0.1043\n",
      "EarlyStopping counter: 59 out of 100\n",
      "Epoch: 94 Step: 2444 Index:-0.0558 R2:0.9118 0.3418 0.3378 RMSE:0.2725 0.7130 0.7694 Tau:0.7556 0.3714 0.0999\n",
      "EarlyStopping counter: 60 out of 100\n",
      "Epoch: 95 Step: 2470 Index:-0.1371 R2:0.9134 0.3371 0.3339 RMSE:0.2838 0.7752 0.7923 Tau:0.7567 0.3524 0.1058\n",
      "EarlyStopping counter: 61 out of 100\n",
      "Epoch: 96 Step: 2496 Index:-0.1034 R2:0.9189 0.3337 0.3388 RMSE:0.2632 0.7415 0.7752 Tau:0.7661 0.3524 0.1038\n",
      "EarlyStopping counter: 62 out of 100\n",
      "Epoch: 97 Step: 2522 Index:-0.0711 R2:0.9130 0.3294 0.3394 RMSE:0.2677 0.7473 0.7872 Tau:0.7544 0.3905 0.1098\n",
      "EarlyStopping counter: 63 out of 100\n",
      "Epoch: 98 Step: 2548 Index:-0.1046 R2:0.9206 0.3518 0.3382 RMSE:0.2933 0.7046 0.7962 Tau:0.7673 0.3143 0.1001\n",
      "EarlyStopping counter: 64 out of 100\n",
      "Epoch: 99 Step: 2574 Index:-0.1595 R2:0.9182 0.3406 0.3208 RMSE:0.2706 0.7785 0.8035 Tau:0.7579 0.3333 0.1052\n",
      "EarlyStopping counter: 65 out of 100\n",
      "Epoch: 100 Step: 2600 Index:-0.1177 R2:0.9210 0.3454 0.3251 RMSE:0.2634 0.7368 0.7935 Tau:0.7661 0.3333 0.0992\n",
      "EarlyStopping counter: 66 out of 100\n",
      "Epoch: 101 Step: 2626 Index:-0.0228 R2:0.9135 0.3392 0.3441 RMSE:0.2745 0.6990 0.7625 Tau:0.7521 0.3905 0.1038\n",
      "EarlyStopping counter: 67 out of 100\n",
      "Epoch: 102 Step: 2652 Index:-0.1146 R2:0.9218 0.3385 0.3254 RMSE:0.2624 0.7527 0.8187 Tau:0.7673 0.3524 0.1043\n",
      "EarlyStopping counter: 68 out of 100\n",
      "Epoch: 103 Step: 2678 Index:-0.2513 R2:0.9167 0.3280 0.3076 RMSE:0.3233 0.8658 0.8512 Tau:0.7591 0.3333 0.1198\n",
      "EarlyStopping counter: 69 out of 100\n",
      "Epoch: 104 Step: 2704 Index:-0.0525 R2:0.9222 0.3356 0.3311 RMSE:0.3837 0.6905 0.8239 Tau:0.7755 0.3524 0.1004\n",
      "EarlyStopping counter: 70 out of 100\n",
      "Epoch: 105 Step: 2730 Index:-0.0756 R2:0.9236 0.3452 0.3290 RMSE:0.2570 0.7327 0.7976 Tau:0.7626 0.3714 0.1014\n",
      "EarlyStopping counter: 71 out of 100\n",
      "Epoch: 106 Step: 2756 Index:-0.1565 R2:0.9272 0.3417 0.3235 RMSE:0.2467 0.7756 0.8226 Tau:0.7732 0.3333 0.1056\n",
      "EarlyStopping counter: 72 out of 100\n",
      "Epoch: 107 Step: 2782 Index:-0.1327 R2:0.9298 0.3392 0.3214 RMSE:0.2530 0.7518 0.8184 Tau:0.7814 0.3333 0.1062\n",
      "EarlyStopping counter: 73 out of 100\n",
      "Epoch: 108 Step: 2808 Index:-0.0606 R2:0.9222 0.3411 0.3382 RMSE:0.4312 0.6987 0.8585 Tau:0.7732 0.3524 0.0937\n",
      "EarlyStopping counter: 74 out of 100\n",
      "Epoch: 109 Step: 2834 Index:-0.1628 R2:0.9105 0.3266 0.3511 RMSE:0.4630 0.8383 0.7893 Tau:0.7474 0.3905 0.0996\n",
      "EarlyStopping counter: 75 out of 100\n",
      "Epoch: 110 Step: 2860 Index:-0.0701 R2:0.9158 0.3303 0.3510 RMSE:0.2722 0.7272 0.7528 Tau:0.7579 0.3714 0.0948\n",
      "EarlyStopping counter: 76 out of 100\n",
      "Epoch: 111 Step: 2886 Index:-0.0756 R2:0.9252 0.3303 0.3402 RMSE:0.2607 0.7137 0.7600 Tau:0.7825 0.3524 0.0996\n",
      "EarlyStopping counter: 77 out of 100\n",
      "Epoch: 112 Step: 2912 Index:-0.0664 R2:0.9284 0.3236 0.3379 RMSE:0.3204 0.6855 0.7866 Tau:0.7849 0.3333 0.0999\n",
      "EarlyStopping counter: 78 out of 100\n",
      "Epoch: 113 Step: 2938 Index:-0.0788 R2:0.9253 0.3154 0.3412 RMSE:0.2817 0.7169 0.7422 Tau:0.7720 0.3524 0.1053\n",
      "EarlyStopping counter: 79 out of 100\n",
      "Epoch: 114 Step: 2964 Index:-0.1610 R2:0.9192 0.3173 0.3385 RMSE:0.2739 0.7952 0.7993 Tau:0.7743 0.3524 0.1084\n",
      "EarlyStopping counter: 80 out of 100\n",
      "Epoch: 115 Step: 2990 Index:-0.1447 R2:0.9250 0.3362 0.3203 RMSE:0.2476 0.7894 0.8307 Tau:0.7849 0.3524 0.1064\n",
      "EarlyStopping counter: 81 out of 100\n",
      "Epoch: 116 Step: 3016 Index:-0.1040 R2:0.9278 0.3341 0.3157 RMSE:0.2702 0.7224 0.8074 Tau:0.7708 0.3333 0.1044\n",
      "EarlyStopping counter: 82 out of 100\n",
      "Epoch: 117 Step: 3042 Index:-0.0608 R2:0.9301 0.3238 0.3228 RMSE:0.2534 0.7370 0.8000 Tau:0.7814 0.3905 0.1127\n",
      "EarlyStopping counter: 83 out of 100\n",
      "Epoch: 118 Step: 3068 Index:-0.1740 R2:0.9247 0.3242 0.3196 RMSE:0.2488 0.8082 0.8429 Tau:0.7732 0.3524 0.1161\n",
      "EarlyStopping counter: 84 out of 100\n",
      "Epoch: 119 Step: 3094 Index:-0.1496 R2:0.9335 0.3349 0.3160 RMSE:0.2360 0.7877 0.8366 Tau:0.7872 0.3524 0.1123\n",
      "EarlyStopping counter: 85 out of 100\n",
      "Epoch: 120 Step: 3120 Index:-0.1966 R2:0.9338 0.3270 0.3084 RMSE:0.2356 0.8347 0.8621 Tau:0.7872 0.3524 0.1199\n",
      "EarlyStopping counter: 86 out of 100\n",
      "Epoch: 121 Step: 3146 Index:-0.1784 R2:0.9353 0.3315 0.3083 RMSE:0.2345 0.8165 0.8521 Tau:0.7837 0.3524 0.1188\n",
      "EarlyStopping counter: 87 out of 100\n",
      "Epoch: 122 Step: 3172 Index:-0.2385 R2:0.9306 0.3184 0.3068 RMSE:0.2399 0.8529 0.8867 Tau:0.7732 0.3333 0.1165\n",
      "EarlyStopping counter: 88 out of 100\n",
      "Epoch: 123 Step: 3198 Index:-0.1461 R2:0.9374 0.3349 0.2999 RMSE:0.2403 0.7842 0.8635 Tau:0.7931 0.3524 0.1138\n",
      "EarlyStopping counter: 89 out of 100\n",
      "Epoch: 124 Step: 3224 Index:-0.2054 R2:0.9377 0.3333 0.2976 RMSE:0.2276 0.8244 0.8829 Tau:0.7989 0.3333 0.1133\n",
      "EarlyStopping counter: 90 out of 100\n",
      "Epoch: 125 Step: 3250 Index:-0.2147 R2:0.9377 0.3280 0.3002 RMSE:0.2289 0.8299 0.8934 Tau:0.7978 0.3333 0.1112\n",
      "EarlyStopping counter: 91 out of 100\n",
      "Epoch: 126 Step: 3276 Index:-0.1349 R2:0.9390 0.3187 0.3088 RMSE:0.2415 0.7921 0.8630 Tau:0.7872 0.3714 0.1144\n",
      "EarlyStopping counter: 92 out of 100\n",
      "Epoch: 127 Step: 3302 Index:-0.1322 R2:0.9387 0.3273 0.3072 RMSE:0.2986 0.7703 0.8929 Tau:0.7872 0.3524 0.1042\n",
      "EarlyStopping counter: 93 out of 100\n",
      "Epoch: 128 Step: 3328 Index:-0.2051 R2:0.9413 0.3170 0.3021 RMSE:0.2302 0.8203 0.8886 Tau:0.7989 0.3333 0.1140\n",
      "EarlyStopping counter: 94 out of 100\n",
      "Epoch: 129 Step: 3354 Index:-0.2930 R2:0.9397 0.3098 0.2959 RMSE:0.2502 0.9082 0.9104 Tau:0.7978 0.3333 0.1129\n",
      "EarlyStopping counter: 95 out of 100\n",
      "Epoch: 130 Step: 3380 Index:-0.1843 R2:0.9430 0.3170 0.3080 RMSE:0.2834 0.8027 0.9083 Tau:0.7954 0.3333 0.1048\n",
      "EarlyStopping counter: 96 out of 100\n",
      "Epoch: 131 Step: 3406 Index:-0.3245 R2:0.9405 0.3158 0.2963 RMSE:0.2558 0.9199 0.9161 Tau:0.7978 0.3143 0.1128\n",
      "EarlyStopping counter: 97 out of 100\n",
      "Epoch: 132 Step: 3432 Index:-0.2285 R2:0.9424 0.3231 0.2924 RMSE:0.2217 0.8437 0.9170 Tau:0.7978 0.3333 0.1115\n",
      "EarlyStopping counter: 98 out of 100\n",
      "Epoch: 133 Step: 3458 Index:-0.1142 R2:0.9404 0.3345 0.3104 RMSE:0.4036 0.7714 0.9587 Tau:0.8048 0.3714 0.1076\n",
      "EarlyStopping counter: 99 out of 100\n",
      "Epoch: 134 Step: 3484 Index:-0.3493 R2:0.9295 0.3199 0.3034 RMSE:0.3371 0.9257 0.9087 Tau:0.7896 0.2952 0.1133\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Epoch: 135 Step: 3510 Index:-0.1794 R2:0.9378 0.3277 0.3275 RMSE:0.4699 0.8175 1.0005 Tau:0.7884 0.3524 0.0975\n"
     ]
    }
   ],
   "source": [
    "train_f_list=[]\n",
    "train_mse_list=[]\n",
    "train_r2_list=[]\n",
    "test_f_list=[]\n",
    "test_mse_list=[]\n",
    "test_r2_list=[]\n",
    "val_f_list=[]\n",
    "val_mse_list=[]\n",
    "val_r2_list=[]\n",
    "epoch_list=[]\n",
    "train_predict_list=[]\n",
    "test_predict_list=[]\n",
    "val_predict_list=[]\n",
    "train_y_list=[]\n",
    "test_y_list=[]\n",
    "val_y_list=[]\n",
    "train_d_list=[]\n",
    "test_d_list=[]\n",
    "val_d_list=[]\n",
    "\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_one_hot', one_hot_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_interger', interger_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_binary', binary_loss, global_step)\n",
    "    \n",
    "    train_mse_list.append(train_MSE**0.5)\n",
    "    train_r2_list.append(train_r2)\n",
    "    val_mse_list.append(val_MSE**0.5)  \n",
    "    val_r2_list.append(val_r2)\n",
    "    train_f_list.append(train_f)\n",
    "    val_f_list.append(val_f)\n",
    "    test_f_list.append(test_f)\n",
    "    epoch_list.append(epoch)\n",
    "    train_predict_list.append(train_predict.flatten())\n",
    "    test_predict_list.append(test_predict.flatten())\n",
    "    val_predict_list.append(val_predict.flatten())\n",
    "    train_y_list.append(train_df[tasks[0]].values)\n",
    "    val_y_list.append(val_df[tasks[0]].values)\n",
    "    test_y_list.append(test_df[tasks[0]].values)\n",
    "    train_d_list.append(train_d)\n",
    "    val_d_list.append(val_d)\n",
    "    test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = - val_MSE**0.5 + val_tau\n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 135 r2:0.4564 RMSE:0.6112 WTI:0.1829 AP:0.3923 Tau:0.0508 \n",
      " \n",
      " Top-1:0.0000 Top-1-fp:0.0000 \n",
      " Top-5:0.3077 Top-5-fp:0.5385 \n",
      " Top-10:0.2692 Top-10-fp:0.6154 \n",
      " Top-15:0.2250 Top-15-fp:0.7250 \n",
      " Top-20:0.2642 Top-20-fp:0.7358 \n",
      " Top-25:0.2537 Top-25-fp:0.7463 \n",
      " Top-30:0.2375 Top-30-fp:0.7625 \n",
      " Top-40:0.3200 Top-40-fp:0.7009 \n",
      " Top-50:0.4900 Top-50-fp:0.6343 \n",
      " \n",
      " Top50:0.2800 Top50-fp:0.7200 \n",
      " Top100:0.2700 Top100-fp:0.7300 \n",
      " Top150:0.5700 Top150-fp:0.6200 \n",
      " Top200:0.7800 Top200-fp:0.6100 \n",
      " Top250:0.9500 Top250-fp:0.6200 \n",
      " Top300:1.0000 Top300-fp:0.5600 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = loss_function(mol_prediction,y)\n",
    "#             loss.backward(retain_graph=True)\n",
    "#             optimizer_AFSE.zero_grad()\n",
    "#             punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "\n",
    "# loss =  regression_loss + vat_loss + test_vat_loss\n",
    "\n",
    "#         init_lr = 1e-4\n",
    "#         max_lr = 10**-(init_lr-1)\n",
    "#         conv_lr = conv_lr - conv_lr**2 + 0.1 * punish_lr\n",
    "#         if conv_lr < max_lr:\n",
    "#             for param_group in optimizer_AFSE.param_groups:\n",
    "#                 param_group[\"lr\"] = conv_lr.detach()\n",
    "#                 AFSE_lr = conv_lr    \n",
    "#         else:\n",
    "#             for param_group in optimizer_AFSE.param_groups:\n",
    "#                 param_group[\"lr\"] = max_lr\n",
    "#                 AFSE_lr = max_lr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
