{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "torch.manual_seed(8)\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ki_P19327_4_200\n",
      "model_file/1_GAFSE_Ki_P19327_4_200_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/Ki_P19327_4_200_train.csv\"\n",
    "test_filename = \"./data/benchmark/Ki_P19327_4_200_test.csv\"\n",
    "test_active = 200\n",
    "val_rate = 0.2\n",
    "random_seed = 68\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/1_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"1_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/1_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0  CN1C2=C(C(=O)N(C1=O)C)N3C=CN(C3=N2)CCCCN4CCN(C... -0.301030\n",
      "1  CC(=O)C1=C(C(=O)N(N=C1C2=CC=CC=C2)CCN3CCN(CC3)... -2.089905\n",
      "2        C1=CC=C(C=C1)CCCNCCCCN2C(=O)C3=CC=CC=C3C2=O -1.812913\n",
      "3                 CC(=CCN1CCC2C1CCC3=C2C(=CC=C3)Br)C -2.424882\n",
      "4              COC1=CC=CC2=C1CCCC2CCCNCCNC3=CC=CC=C3 -1.147676\n",
      "number of all smiles:  722\n",
      "number of successfully processed smiles:  722\n",
      "                                              smiles     value  \\\n",
      "0  CN1C2=C(C(=O)N(C1=O)C)N3C=CN(C3=N2)CCCCN4CCN(C... -0.301030   \n",
      "1  CC(=O)C1=C(C(=O)N(N=C1C2=CC=CC=C2)CCN3CCN(CC3)... -2.089905   \n",
      "2        C1=CC=C(C=C1)CCCNCCCCN2C(=O)C3=CC=CC=C3C2=O -1.812913   \n",
      "3                 CC(=CCN1CCC2C1CCC3=C2C(=CC=C3)Br)C -2.424882   \n",
      "4              COC1=CC=CC2=C1CCCC2CCCNCCNC3=CC=CC=C3 -1.147676   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  Cn1c(=O)c2c(nc3n(CCCCN4CCN(c5ccccc5O)CC4)ccn23...  \n",
      "1  COc1ccccc1N1CCN(CCn2nc(-c3ccccc3)c(C(C)=O)c(N)...  \n",
      "2                O=C1c2ccccc2C(=O)N1CCCCNCCCc1ccccc1  \n",
      "3                    CC(C)=CCN1CCC2c3c(Br)cccc3CCC21  \n",
      "4                    COc1cccc2c1CCCC2CCCNCCNc1ccccc1  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  618\n",
      "number of successfully processed smiles:  618\n",
      "(618, 3)\n",
      "                                              smiles     value  \\\n",
      "0  C1CN(CCN1CCCOC2=CC3=C(C=C2)N=CN3)C4=CC=CC(=C4)... -2.590005   \n",
      "1                 CCCN(CC=CI)C1CCC2=C(C1)C(=CC=C2)OC -1.107210   \n",
      "2       COC1=C(C(=CC=C1)OC)OCCNCC2COC3=CC=CC=C3O2.Cl -1.610021   \n",
      "3  C1CN(CCN1CCCCNS(=O)(=O)C2=CC3=CC=CC=C3C=C2)C4=... -2.696356   \n",
      "4                            CN(C)CCOC1=NC(=CC=C1)Br -2.955207   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0   FC(F)(F)c1cccc(N2CCN(CCCOc3ccc4nc[nH]c4c3)CC2)c1  \n",
      "1                      CCCN(CC=CI)C1CCc2cccc(OC)c2C1  \n",
      "2               COc1cccc(OC)c1OCCNCC1COc2ccccc2O1.Cl  \n",
      "3  O=S(=O)(NCCCCN1CCN(c2cccc(Cl)c2)CC1)c1ccc2cccc...  \n",
      "4                               CN(C)CCOc1cccc(Br)n1  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/Ki_P19327_4_200_train.pickle\n",
      "./data/benchmark/Ki_P19327_4_200_train\n",
      "1340\n",
      "feature dicts file saved as ./data/benchmark/Ki_P19327_4_200_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(578, 3) (144, 3) (618, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    [d,e,f,g] = bond_neighbor.shape\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    one_hot_loss = 0\n",
    "    interger_loss = 0\n",
    "    binary_loss = 0\n",
    "    counter_i = 0\n",
    "    counter_j = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        one_hot_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-6)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)+\\\n",
    "                         weighted_CE_loss(atom_list[i,:l,16:22], x_atom[i,:l,16:22])+ \\\n",
    "                         weighted_CE_loss(atom_list[i,:l,24:30], x_atom[i,:l,24:30])+ \\\n",
    "                         weighted_CE_loss(atom_list[i,:l,31:36], x_atom[i,:l,31:36])\n",
    "        interger_loss += loss_function(atom_list[i,:l,23], x_atom[i,:l,23])+ \\\n",
    "                        loss_function(atom_list[i,:l,24], x_atom[i,:l,24])\n",
    "        binary_loss += CE(atom_list[i,:l,30], x_atom[i,:l,30])+ \\\n",
    "                        CE(atom_list[i,:l,36], x_atom[i,:l,36])+ \\\n",
    "                        CE(atom_list[i,:l,37], x_atom[i,:l,37])+ \\\n",
    "                        CE(atom_list[i,:l,38], x_atom[i,:l,38])\n",
    "        counter_i += 1\n",
    "        for j in range(l):\n",
    "            n = (bond_neighbor[i,j].sum(-1)!=0).sum(-1)\n",
    "            if n==0:\n",
    "                continue\n",
    "            one_hot_loss += weighted_CE_loss(bond_list[i,j,:n,:4], bond_neighbor[i,j,:n,:4])+ \\\n",
    "                             weighted_CE_loss(bond_list[i,j,:n,6:], bond_neighbor[i,j,:n,6:])\n",
    "            binary_loss += CE(bond_neighbor[i,j,:n,4], bond_list[i,j,:n,4])+ \\\n",
    "                           CE(bond_neighbor[i,j,:n,5], bond_list[i,j,:n,5])\n",
    "            counter_j += 1\n",
    "    one_hot_loss = one_hot_loss/(5*counter_i+2*counter_j)\n",
    "    interger_loss = interger_loss/(2*counter_i)\n",
    "    binary_loss = binary_loss/(4*counter_i+2*counter_j)\n",
    "    total_loss = (one_hot_loss + interger_loss + binary_loss)/3\n",
    "    return total_loss, one_hot_loss, interger_loss, binary_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "#         atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "#                                       torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "#         success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "#                             bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "#                                                      refer_atom_list, refer_bond_list,topn=1)\n",
    "#         reconstruction_loss, one_hot_loss, interger_loss,binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, \n",
    "#                                                                                               bond_neighbor, validity_mask, atom_list, \n",
    "#                                                                                               bond_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "#         atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "#                                                 torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "#                                                 mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "#         refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "#                                                             torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "#                                                             mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "#         success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "#                             bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "#                                                      refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "#         test_reconstruction_loss, test_one_hot_loss, test_interger_loss,test_binary_loss = generate_loss_function(atom_list_test, x_atom_test, bond_list_test, bond_neighbor_test, validity_mask_test, atom_list_test, bond_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        max_lr = 1e-3\n",
    "        conv_lr = conv_lr - conv_lr**2 + 0.06 * punish_lr\n",
    "        if conv_lr < max_lr and conv_lr >= 0:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = conv_lr.detach()\n",
    "                AFSE_lr = conv_lr    \n",
    "        elif conv_lr < 0:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = 0\n",
    "                AFSE_lr = 0\n",
    "        elif conv_lr >= max_lr:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = max_lr\n",
    "                AFSE_lr = max_lr\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_one_hot', one_hot_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_interger', interger_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_binary', binary_loss, global_step)\n",
    "        logger.add_scalar('lr/max_lr', conv_lr, global_step)\n",
    "        logger.add_scalar('lr/punish_lr', punish_lr, global_step)\n",
    "        logger.add_scalar('lr/AFSE_lr', AFSE_lr, global_step)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "#         optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6*(vat_loss + test_vat_loss) # + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "#         optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 10\n",
    "patience = 100\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/1_GAFSE_Ki_P19327_4_200_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 61 Index:-0.9627 R2:0.0187 0.0086 0.0046 RMSE:1.0989 1.0301 1.1605 Tau:0.1040 0.0675 -0.0022\n",
      "Epoch: 2 Step: 122 Index:-0.9504 R2:0.0484 0.0267 0.0224 RMSE:1.0804 1.0208 1.1466 Tau:0.1542 0.0704 0.0141\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 3 Step: 183 Index:-1.1466 R2:0.0400 0.0197 0.0128 RMSE:1.2265 1.2341 1.2915 Tau:0.1404 0.0875 0.0138\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 4 Step: 244 Index:-0.9666 R2:0.0492 0.0262 0.0169 RMSE:1.0945 1.0656 1.1648 Tau:0.1542 0.0990 0.0231\n",
      "Epoch: 5 Step: 305 Index:-0.9283 R2:0.0555 0.0302 0.0214 RMSE:1.0727 1.0212 1.1415 Tau:0.1653 0.0930 0.0278\n",
      "Epoch: 6 Step: 366 Index:-0.9058 R2:0.0662 0.0375 0.0309 RMSE:1.0644 1.0213 1.1376 Tau:0.1852 0.1155 0.0405\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 7 Step: 427 Index:-0.9879 R2:0.0666 0.0370 0.0286 RMSE:1.1217 1.1110 1.1939 Tau:0.1822 0.1231 0.0417\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 8 Step: 488 Index:-0.9231 R2:0.0745 0.0408 0.0343 RMSE:1.0769 1.0510 1.1514 Tau:0.1955 0.1280 0.0483\n",
      "Epoch: 9 Step: 549 Index:-0.8771 R2:0.0837 0.0462 0.0420 RMSE:1.0877 1.0255 1.1582 Tau:0.2097 0.1484 0.0655\n",
      "Epoch: 10 Step: 610 Index:-0.8761 R2:0.0865 0.0479 0.0396 RMSE:1.0590 1.0283 1.1379 Tau:0.2105 0.1523 0.0633\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 11 Step: 671 Index:-0.9516 R2:0.0922 0.0492 0.0423 RMSE:1.1152 1.1126 1.1936 Tau:0.2172 0.1610 0.0685\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 12 Step: 732 Index:-0.8858 R2:0.0963 0.0523 0.0433 RMSE:1.0721 1.0537 1.1517 Tau:0.2242 0.1678 0.0780\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 13 Step: 793 Index:-0.8786 R2:0.1042 0.0561 0.0487 RMSE:1.0664 1.0533 1.1505 Tau:0.2347 0.1746 0.0832\n",
      "Epoch: 14 Step: 854 Index:-0.8464 R2:0.1097 0.0587 0.0492 RMSE:1.0872 1.0300 1.1671 Tau:0.2386 0.1836 0.0968\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 15 Step: 915 Index:-0.8909 R2:0.1147 0.0576 0.0493 RMSE:1.0756 1.0717 1.1636 Tau:0.2445 0.1809 0.0977\n",
      "Epoch: 16 Step: 976 Index:-0.8173 R2:0.1267 0.0592 0.0534 RMSE:1.0407 1.0032 1.1294 Tau:0.2538 0.1859 0.1098\n",
      "Epoch: 17 Step: 1037 Index:-0.8160 R2:0.1299 0.0592 0.0544 RMSE:1.0294 1.0044 1.1255 Tau:0.2555 0.1884 0.1075\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 18 Step: 1098 Index:-0.9654 R2:0.1300 0.0537 0.0527 RMSE:1.1140 1.1360 1.2034 Tau:0.2530 0.1706 0.1069\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 19 Step: 1159 Index:-0.8588 R2:0.1454 0.0592 0.0601 RMSE:1.0400 1.0422 1.1359 Tau:0.2658 0.1834 0.1173\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 20 Step: 1220 Index:-0.9788 R2:0.1509 0.0528 0.0523 RMSE:1.1305 1.1572 1.2217 Tau:0.2655 0.1783 0.1318\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 21 Step: 1281 Index:-0.9836 R2:0.1466 0.0580 0.0629 RMSE:1.1310 1.1631 1.2196 Tau:0.2651 0.1795 0.1175\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 22 Step: 1342 Index:-0.8455 R2:0.1402 0.0599 0.0631 RMSE:1.0277 1.0266 1.1271 Tau:0.2604 0.1811 0.1080\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 23 Step: 1403 Index:-0.9019 R2:0.1636 0.0620 0.0685 RMSE:1.0670 1.0886 1.1614 Tau:0.2767 0.1867 0.1324\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 24 Step: 1464 Index:-0.8364 R2:0.1652 0.0623 0.0696 RMSE:1.0142 1.0227 1.1192 Tau:0.2770 0.1863 0.1297\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 25 Step: 1525 Index:-0.8295 R2:0.1754 0.0703 0.0787 RMSE:1.0141 1.0306 1.1203 Tau:0.2847 0.2011 0.1367\n",
      "Epoch: 26 Step: 1586 Index:-0.7944 R2:0.1767 0.0758 0.0843 RMSE:0.9994 1.0054 1.1110 Tau:0.2876 0.2110 0.1352\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 27 Step: 1647 Index:-0.8378 R2:0.1952 0.0716 0.0794 RMSE:1.0312 1.0499 1.1336 Tau:0.2974 0.2122 0.1633\n",
      "Epoch: 28 Step: 1708 Index:-0.7914 R2:0.1860 0.0782 0.0838 RMSE:0.9985 1.0035 1.1071 Tau:0.2904 0.2122 0.1475\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 29 Step: 1769 Index:-0.8273 R2:0.1839 0.0766 0.0939 RMSE:1.0141 1.0393 1.1178 Tau:0.2913 0.2120 0.1383\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 30 Step: 1830 Index:-0.8155 R2:0.2007 0.0679 0.0825 RMSE:1.0025 1.0144 1.1094 Tau:0.2971 0.1989 0.1770\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 31 Step: 1891 Index:-0.8244 R2:0.2106 0.0792 0.0982 RMSE:1.0113 1.0414 1.1190 Tau:0.3052 0.2170 0.1660\n",
      "Epoch: 32 Step: 1952 Index:-0.7869 R2:0.2074 0.0790 0.1050 RMSE:0.9828 0.9988 1.0959 Tau:0.3051 0.2120 0.1536\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 33 Step: 2013 Index:-0.8705 R2:0.1881 0.0782 0.1048 RMSE:1.0382 1.0829 1.1378 Tau:0.2942 0.2124 0.1380\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 34 Step: 2074 Index:-0.8144 R2:0.1893 0.0836 0.0990 RMSE:1.0051 1.0305 1.1149 Tau:0.2934 0.2161 0.1372\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 35 Step: 2135 Index:-0.7869 R2:0.2191 0.0953 0.1103 RMSE:1.0310 1.0204 1.1411 Tau:0.3129 0.2336 0.1757\n",
      "Epoch: 36 Step: 2196 Index:-0.7588 R2:0.2349 0.0941 0.1223 RMSE:0.9677 0.9900 1.0813 Tau:0.3226 0.2312 0.1786\n",
      "Epoch: 37 Step: 2257 Index:-0.7487 R2:0.2302 0.0985 0.1168 RMSE:0.9765 0.9854 1.0934 Tau:0.3195 0.2367 0.1762\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 38 Step: 2318 Index:-0.7575 R2:0.2367 0.1039 0.1213 RMSE:0.9945 1.0018 1.1174 Tau:0.3249 0.2443 0.1810\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 39 Step: 2379 Index:-0.7953 R2:0.2357 0.0911 0.1208 RMSE:1.0119 1.0197 1.1295 Tau:0.3227 0.2244 0.1796\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 40 Step: 2440 Index:-0.8263 R2:0.2452 0.0936 0.1363 RMSE:1.0139 1.0608 1.1129 Tau:0.3301 0.2345 0.1996\n",
      "Epoch: 41 Step: 2501 Index:-0.7417 R2:0.2476 0.1078 0.1333 RMSE:0.9751 0.9852 1.0921 Tau:0.3317 0.2435 0.1861\n",
      "Epoch: 42 Step: 2562 Index:-0.7270 R2:0.2557 0.1097 0.1487 RMSE:0.9631 0.9773 1.0723 Tau:0.3367 0.2503 0.2060\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 43 Step: 2623 Index:-0.7542 R2:0.2630 0.1025 0.1469 RMSE:0.9637 0.9965 1.0704 Tau:0.3392 0.2423 0.1999\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 44 Step: 2684 Index:-0.7327 R2:0.2585 0.1164 0.1601 RMSE:0.9726 0.9846 1.0817 Tau:0.3403 0.2518 0.1966\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 45 Step: 2745 Index:-0.7827 R2:0.2589 0.1078 0.1564 RMSE:0.9713 1.0240 1.0768 Tau:0.3403 0.2413 0.1860\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 46 Step: 2806 Index:-0.7286 R2:0.2608 0.1153 0.1517 RMSE:0.9502 0.9785 1.0684 Tau:0.3417 0.2499 0.1845\n",
      "Epoch: 47 Step: 2867 Index:-0.7244 R2:0.2668 0.1221 0.1585 RMSE:0.9604 0.9800 1.0781 Tau:0.3452 0.2555 0.2014\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 48 Step: 2928 Index:-0.7471 R2:0.2670 0.1105 0.1596 RMSE:0.9461 0.9865 1.0587 Tau:0.3475 0.2394 0.1965\n",
      "Epoch: 49 Step: 2989 Index:-0.7102 R2:0.2811 0.1280 0.1818 RMSE:0.9379 0.9711 1.0479 Tau:0.3556 0.2609 0.2040\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 50 Step: 3050 Index:-0.7195 R2:0.2819 0.1252 0.1733 RMSE:0.9335 0.9787 1.0512 Tau:0.3567 0.2592 0.2017\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 51 Step: 3111 Index:-0.7443 R2:0.2856 0.1182 0.1754 RMSE:0.9394 0.9959 1.0519 Tau:0.3601 0.2517 0.2039\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 52 Step: 3172 Index:-0.7112 R2:0.2900 0.1267 0.1787 RMSE:0.9382 0.9740 1.0568 Tau:0.3614 0.2627 0.2098\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 53 Step: 3233 Index:-0.7330 R2:0.2882 0.1261 0.1788 RMSE:0.9335 0.9918 1.0503 Tau:0.3626 0.2588 0.2015\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 54 Step: 3294 Index:-0.7413 R2:0.2963 0.1215 0.1881 RMSE:0.9318 0.9998 1.0442 Tau:0.3694 0.2585 0.2070\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 55 Step: 3355 Index:-0.7725 R2:0.3014 0.1224 0.1902 RMSE:0.9489 1.0280 1.0585 Tau:0.3713 0.2555 0.2065\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 56 Step: 3416 Index:-0.7877 R2:0.3070 0.1198 0.2167 RMSE:0.9669 1.0481 1.0567 Tau:0.3793 0.2604 0.2156\n",
      "Epoch: 57 Step: 3477 Index:-0.6796 R2:0.3114 0.1429 0.2029 RMSE:0.9157 0.9673 1.0348 Tau:0.3788 0.2876 0.2160\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 58 Step: 3538 Index:-0.7254 R2:0.3161 0.1242 0.1994 RMSE:0.9286 0.9924 1.0383 Tau:0.3812 0.2670 0.2208\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 59 Step: 3599 Index:-0.7047 R2:0.3043 0.1272 0.1933 RMSE:0.9293 0.9704 1.0428 Tau:0.3660 0.2657 0.2332\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 60 Step: 3660 Index:-0.6989 R2:0.3287 0.1387 0.2107 RMSE:0.9130 0.9729 1.0264 Tau:0.3856 0.2740 0.2279\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 61 Step: 3721 Index:-0.6832 R2:0.3301 0.1450 0.2141 RMSE:0.9057 0.9627 1.0247 Tau:0.3898 0.2795 0.2195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 62 Step: 3782 Index:-0.7622 R2:0.3229 0.1306 0.1982 RMSE:0.9380 1.0240 1.0552 Tau:0.3871 0.2618 0.2097\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 63 Step: 3843 Index:-0.6843 R2:0.3352 0.1458 0.2085 RMSE:0.9041 0.9708 1.0273 Tau:0.3901 0.2865 0.2340\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 64 Step: 3904 Index:-0.7455 R2:0.3135 0.1347 0.2054 RMSE:0.9328 1.0180 1.0441 Tau:0.3831 0.2725 0.2071\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 65 Step: 3965 Index:-0.7444 R2:0.3263 0.1387 0.2249 RMSE:0.9291 1.0197 1.0321 Tau:0.3904 0.2754 0.2199\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 66 Step: 4026 Index:-0.7261 R2:0.3387 0.1441 0.2356 RMSE:0.9264 1.0135 1.0275 Tau:0.3979 0.2874 0.2319\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 67 Step: 4087 Index:-0.6939 R2:0.3439 0.1448 0.2309 RMSE:0.9047 0.9778 1.0148 Tau:0.3974 0.2839 0.2396\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 68 Step: 4148 Index:-0.6796 R2:0.3502 0.1452 0.2354 RMSE:0.8942 0.9679 1.0094 Tau:0.4017 0.2882 0.2343\n",
      "Epoch: 69 Step: 4209 Index:-0.6652 R2:0.3529 0.1530 0.2241 RMSE:0.8908 0.9593 1.0173 Tau:0.4013 0.2940 0.2398\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 70 Step: 4270 Index:-0.6876 R2:0.3492 0.1638 0.2192 RMSE:0.9357 0.9834 1.0717 Tau:0.4030 0.2958 0.2324\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 71 Step: 4331 Index:-0.6799 R2:0.3548 0.1480 0.2265 RMSE:0.8896 0.9693 1.0150 Tau:0.3992 0.2894 0.2457\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 72 Step: 4392 Index:-0.6709 R2:0.3617 0.1522 0.2406 RMSE:0.8847 0.9609 1.0075 Tau:0.4119 0.2900 0.2310\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 73 Step: 4453 Index:-0.7230 R2:0.3701 0.1577 0.2453 RMSE:0.9275 1.0153 1.0331 Tau:0.4127 0.2923 0.2470\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 74 Step: 4514 Index:-0.7195 R2:0.3714 0.1463 0.2463 RMSE:0.9043 1.0013 1.0158 Tau:0.4137 0.2818 0.2430\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 75 Step: 4575 Index:-0.6742 R2:0.3643 0.1607 0.2378 RMSE:0.8802 0.9729 1.0103 Tau:0.4118 0.2987 0.2359\n",
      "Epoch: 76 Step: 4636 Index:-0.6479 R2:0.3695 0.1692 0.2533 RMSE:0.8847 0.9573 1.0131 Tau:0.4140 0.3094 0.2429\n",
      "Epoch: 77 Step: 4697 Index:-0.6371 R2:0.3809 0.1734 0.2604 RMSE:0.8737 0.9477 0.9981 Tau:0.4200 0.3106 0.2507\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 78 Step: 4758 Index:-0.7379 R2:0.3801 0.1648 0.2586 RMSE:0.9327 1.0366 1.0367 Tau:0.4241 0.2987 0.2415\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 79 Step: 4819 Index:-0.6833 R2:0.3789 0.1611 0.2655 RMSE:0.8823 0.9771 0.9927 Tau:0.4254 0.2939 0.2430\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 80 Step: 4880 Index:-0.6879 R2:0.3828 0.1620 0.2550 RMSE:0.8812 0.9858 1.0028 Tau:0.4245 0.2979 0.2407\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 81 Step: 4941 Index:-0.6875 R2:0.3906 0.1689 0.2729 RMSE:0.8888 0.9891 0.9970 Tau:0.4280 0.3016 0.2488\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 82 Step: 5002 Index:-0.6703 R2:0.3918 0.1645 0.2701 RMSE:0.8865 0.9768 0.9952 Tau:0.4293 0.3065 0.2620\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 83 Step: 5063 Index:-0.8199 R2:0.3838 0.1584 0.2717 RMSE:0.9813 1.1072 1.0707 Tau:0.4298 0.2872 0.2412\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 84 Step: 5124 Index:-0.6672 R2:0.3969 0.1640 0.2699 RMSE:0.8619 0.9663 0.9860 Tau:0.4333 0.2991 0.2612\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 85 Step: 5185 Index:-0.6791 R2:0.3903 0.1691 0.2524 RMSE:0.8604 0.9757 1.0043 Tau:0.4312 0.2966 0.2482\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 86 Step: 5246 Index:-0.6650 R2:0.3901 0.1640 0.2482 RMSE:0.8731 0.9709 1.0239 Tau:0.4291 0.3059 0.2374\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 87 Step: 5307 Index:-0.6781 R2:0.3901 0.1790 0.2535 RMSE:0.9230 0.9932 1.0688 Tau:0.4307 0.3151 0.2446\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 88 Step: 5368 Index:-0.7231 R2:0.3989 0.1473 0.2588 RMSE:0.8979 1.0053 1.0121 Tau:0.4313 0.2822 0.2430\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Epoch: 89 Step: 5429 Index:-0.6627 R2:0.4006 0.1626 0.2860 RMSE:0.8588 0.9643 0.9761 Tau:0.4348 0.3016 0.2639\n",
      "Epoch: 90 Step: 5490 Index:-0.6336 R2:0.4122 0.1844 0.2713 RMSE:0.8583 0.9517 1.0038 Tau:0.4424 0.3182 0.2568\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 91 Step: 5551 Index:-0.6454 R2:0.4203 0.1853 0.2757 RMSE:0.8455 0.9576 0.9831 Tau:0.4495 0.3121 0.2574\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 92 Step: 5612 Index:-0.6771 R2:0.4108 0.1735 0.2824 RMSE:0.8578 0.9836 0.9824 Tau:0.4453 0.3065 0.2539\n",
      "Epoch: 93 Step: 5673 Index:-0.6332 R2:0.4043 0.1895 0.2726 RMSE:0.8833 0.9494 1.0156 Tau:0.4362 0.3162 0.2767\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 94 Step: 5734 Index:-0.6900 R2:0.3818 0.1722 0.2306 RMSE:0.9269 1.0014 1.0895 Tau:0.4245 0.3114 0.2365\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 95 Step: 5795 Index:-0.6525 R2:0.4054 0.1808 0.2477 RMSE:0.8886 0.9679 1.0461 Tau:0.4385 0.3154 0.2564\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 96 Step: 5856 Index:-0.6571 R2:0.4064 0.1732 0.2593 RMSE:0.8659 0.9699 1.0224 Tau:0.4395 0.3127 0.2497\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 97 Step: 5917 Index:-0.6521 R2:0.4115 0.1727 0.2920 RMSE:0.8498 0.9596 0.9725 Tau:0.4462 0.3075 0.2715\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 98 Step: 5978 Index:-0.6424 R2:0.4280 0.1862 0.2983 RMSE:0.8398 0.9577 0.9668 Tau:0.4583 0.3152 0.2643\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 99 Step: 6039 Index:-0.6624 R2:0.4085 0.1692 0.2435 RMSE:0.8529 0.9636 1.0164 Tau:0.4405 0.3012 0.2505\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 100 Step: 6100 Index:-0.6643 R2:0.4396 0.1840 0.2934 RMSE:0.8452 0.9794 0.9775 Tau:0.4639 0.3151 0.2678\n",
      "Epoch: 101 Step: 6161 Index:-0.6077 R2:0.4285 0.1975 0.2844 RMSE:0.8366 0.9401 0.9817 Tau:0.4541 0.3324 0.2825\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 102 Step: 6222 Index:-0.6567 R2:0.4410 0.1798 0.2862 RMSE:0.8429 0.9692 0.9787 Tau:0.4635 0.3125 0.2728\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 103 Step: 6283 Index:-0.6482 R2:0.4387 0.1798 0.2825 RMSE:0.8283 0.9582 0.9832 Tau:0.4626 0.3100 0.2690\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 104 Step: 6344 Index:-0.6579 R2:0.4372 0.1759 0.3172 RMSE:0.8332 0.9589 0.9558 Tau:0.4617 0.3011 0.2845\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 105 Step: 6405 Index:-0.8161 R2:0.4135 0.1622 0.2909 RMSE:0.9240 1.0991 1.0258 Tau:0.4530 0.2830 0.2607\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 106 Step: 6466 Index:-0.6874 R2:0.4391 0.1737 0.2831 RMSE:0.8439 0.9970 0.9844 Tau:0.4649 0.3096 0.2639\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 107 Step: 6527 Index:-0.6221 R2:0.4493 0.1958 0.2839 RMSE:0.8242 0.9500 0.9770 Tau:0.4727 0.3279 0.2708\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 108 Step: 6588 Index:-0.7150 R2:0.4146 0.1736 0.2866 RMSE:0.8664 1.0166 0.9882 Tau:0.4535 0.3016 0.2533\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 109 Step: 6649 Index:-0.6154 R2:0.4433 0.2004 0.3000 RMSE:0.8378 0.9372 0.9789 Tau:0.4639 0.3219 0.2926\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 110 Step: 6710 Index:-0.6236 R2:0.4560 0.1951 0.3103 RMSE:0.8200 0.9501 0.9586 Tau:0.4754 0.3265 0.2844\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 111 Step: 6771 Index:-0.6153 R2:0.4664 0.1936 0.3110 RMSE:0.8158 0.9449 0.9678 Tau:0.4812 0.3296 0.2863\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 112 Step: 6832 Index:-0.6496 R2:0.4669 0.1863 0.3000 RMSE:0.8114 0.9681 0.9665 Tau:0.4844 0.3186 0.2809\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Epoch: 113 Step: 6893 Index:-0.6176 R2:0.4565 0.1960 0.2949 RMSE:0.8210 0.9416 0.9776 Tau:0.4723 0.3240 0.2944\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Epoch: 114 Step: 6954 Index:-0.6539 R2:0.4715 0.2010 0.3181 RMSE:0.8346 0.9864 0.9672 Tau:0.4871 0.3326 0.2954\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Epoch: 115 Step: 7015 Index:-0.6419 R2:0.4651 0.1918 0.2985 RMSE:0.8075 0.9692 0.9700 Tau:0.4819 0.3273 0.2756\n",
      "Epoch: 116 Step: 7076 Index:-0.5894 R2:0.4779 0.2075 0.3103 RMSE:0.8042 0.9334 0.9603 Tau:0.4926 0.3440 0.2850\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 117 Step: 7137 Index:-0.6590 R2:0.4555 0.2012 0.2876 RMSE:0.8799 0.9817 1.0465 Tau:0.4724 0.3226 0.2943\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 118 Step: 7198 Index:-0.6384 R2:0.4726 0.1946 0.3272 RMSE:0.8274 0.9607 0.9779 Tau:0.4883 0.3222 0.2843\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 119 Step: 7259 Index:-0.6525 R2:0.4504 0.1944 0.3270 RMSE:0.8271 0.9605 0.9483 Tau:0.4643 0.3081 0.2993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 120 Step: 7320 Index:-0.6400 R2:0.4711 0.2028 0.3092 RMSE:0.8132 0.9708 0.9643 Tau:0.4879 0.3308 0.2777\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 121 Step: 7381 Index:-0.9683 R2:0.3711 0.1358 0.2711 RMSE:1.0943 1.2375 1.1550 Tau:0.4154 0.2692 0.2595\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 122 Step: 7442 Index:-0.6717 R2:0.4635 0.1695 0.3165 RMSE:0.8133 0.9757 0.9545 Tau:0.4801 0.3040 0.2778\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 123 Step: 7503 Index:-0.6618 R2:0.4762 0.1921 0.3255 RMSE:0.8121 0.9840 0.9518 Tau:0.4904 0.3222 0.2822\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 124 Step: 7564 Index:-0.6467 R2:0.4756 0.1892 0.3263 RMSE:0.8147 0.9659 0.9501 Tau:0.4874 0.3191 0.3020\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 125 Step: 7625 Index:-0.6137 R2:0.4791 0.2087 0.3307 RMSE:0.8037 0.9457 0.9583 Tau:0.4937 0.3320 0.3021\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 126 Step: 7686 Index:-0.6134 R2:0.4902 0.2085 0.3331 RMSE:0.7977 0.9494 0.9597 Tau:0.4993 0.3361 0.2971\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 127 Step: 7747 Index:-0.6483 R2:0.4996 0.2013 0.3269 RMSE:0.8109 0.9785 0.9563 Tau:0.5058 0.3302 0.2992\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Epoch: 128 Step: 7808 Index:-0.6743 R2:0.4639 0.1841 0.3201 RMSE:0.8317 0.9871 0.9582 Tau:0.4779 0.3127 0.3048\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Epoch: 129 Step: 7869 Index:-0.6841 R2:0.4494 0.1799 0.2850 RMSE:0.8352 0.9887 0.9848 Tau:0.4684 0.3046 0.2920\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Epoch: 130 Step: 7930 Index:-0.6032 R2:0.4882 0.2094 0.3162 RMSE:0.7905 0.9449 0.9574 Tau:0.4978 0.3417 0.2906\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Epoch: 131 Step: 7991 Index:-0.7373 R2:0.4888 0.1819 0.3243 RMSE:0.8494 1.0453 0.9823 Tau:0.4964 0.3081 0.2866\n",
      "EarlyStopping counter: 16 out of 100\n",
      "Epoch: 132 Step: 8052 Index:-0.6455 R2:0.4886 0.1922 0.3248 RMSE:0.7896 0.9697 0.9520 Tau:0.4979 0.3242 0.2827\n",
      "EarlyStopping counter: 17 out of 100\n",
      "Epoch: 133 Step: 8113 Index:-0.6434 R2:0.4969 0.1983 0.3239 RMSE:0.7918 0.9715 0.9499 Tau:0.5033 0.3281 0.2934\n",
      "EarlyStopping counter: 18 out of 100\n",
      "Epoch: 134 Step: 8174 Index:-0.6015 R2:0.4954 0.2236 0.3333 RMSE:0.8150 0.9480 0.9833 Tau:0.5051 0.3466 0.2944\n",
      "EarlyStopping counter: 19 out of 100\n",
      "Epoch: 135 Step: 8235 Index:-0.6537 R2:0.4962 0.1960 0.3463 RMSE:0.7852 0.9740 0.9336 Tau:0.5040 0.3203 0.2967\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Epoch: 136 Step: 8296 Index:-0.6034 R2:0.5034 0.2199 0.3320 RMSE:0.7841 0.9418 0.9569 Tau:0.5106 0.3384 0.3049\n",
      "Epoch: 137 Step: 8357 Index:-0.5858 R2:0.5062 0.2177 0.3445 RMSE:0.7821 0.9343 0.9381 Tau:0.5105 0.3485 0.3043\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 138 Step: 8418 Index:-0.6738 R2:0.4916 0.1986 0.3053 RMSE:0.8255 1.0102 0.9815 Tau:0.4982 0.3364 0.2884\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 139 Step: 8479 Index:-0.6314 R2:0.5095 0.2046 0.3281 RMSE:0.7820 0.9622 0.9472 Tau:0.5133 0.3308 0.3000\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 140 Step: 8540 Index:-0.7104 R2:0.5169 0.2047 0.3340 RMSE:0.8525 1.0420 0.9868 Tau:0.5158 0.3316 0.2983\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 141 Step: 8601 Index:-0.6334 R2:0.5022 0.2033 0.3436 RMSE:0.7783 0.9695 0.9371 Tau:0.5076 0.3361 0.3017\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 142 Step: 8662 Index:-0.6204 R2:0.5135 0.2094 0.3370 RMSE:0.7740 0.9526 0.9405 Tau:0.5135 0.3322 0.2961\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 143 Step: 8723 Index:-0.6027 R2:0.5048 0.2057 0.3005 RMSE:0.7897 0.9436 0.9832 Tau:0.5139 0.3409 0.2985\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 144 Step: 8784 Index:-0.6230 R2:0.5149 0.2044 0.3377 RMSE:0.7719 0.9554 0.9438 Tau:0.5161 0.3324 0.3079\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 145 Step: 8845 Index:-0.6377 R2:0.5129 0.2045 0.3318 RMSE:0.7707 0.9679 0.9564 Tau:0.5153 0.3302 0.2986\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 146 Step: 8906 Index:-0.6487 R2:0.5086 0.1998 0.3440 RMSE:0.7904 0.9680 0.9594 Tau:0.5124 0.3193 0.3059\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 147 Step: 8967 Index:-0.6523 R2:0.5290 0.2037 0.3313 RMSE:0.7787 0.9835 0.9494 Tau:0.5254 0.3312 0.3057\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 148 Step: 9028 Index:-0.6169 R2:0.5171 0.2121 0.3546 RMSE:0.7802 0.9601 0.9286 Tau:0.5177 0.3433 0.3008\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Epoch: 149 Step: 9089 Index:-0.6169 R2:0.5217 0.2073 0.3521 RMSE:0.7722 0.9568 0.9289 Tau:0.5212 0.3399 0.3147\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Epoch: 150 Step: 9150 Index:-0.6138 R2:0.5287 0.2130 0.3532 RMSE:0.7646 0.9494 0.9283 Tau:0.5269 0.3357 0.3101\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Epoch: 151 Step: 9211 Index:-0.6155 R2:0.5236 0.2187 0.3457 RMSE:0.7670 0.9564 0.9353 Tau:0.5231 0.3409 0.3106\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Epoch: 152 Step: 9272 Index:-0.6255 R2:0.4838 0.2000 0.2993 RMSE:0.7928 0.9614 0.9801 Tau:0.4926 0.3359 0.2797\n",
      "EarlyStopping counter: 16 out of 100\n",
      "Epoch: 153 Step: 9333 Index:-0.6873 R2:0.5227 0.2046 0.3482 RMSE:0.8174 1.0171 0.9594 Tau:0.5194 0.3298 0.3198\n",
      "EarlyStopping counter: 17 out of 100\n",
      "Epoch: 154 Step: 9394 Index:-0.6550 R2:0.5279 0.2081 0.3524 RMSE:0.7919 0.9967 0.9384 Tau:0.5244 0.3417 0.3159\n",
      "EarlyStopping counter: 18 out of 100\n",
      "Epoch: 155 Step: 9455 Index:-0.6283 R2:0.5233 0.2027 0.3285 RMSE:0.7655 0.9586 0.9483 Tau:0.5175 0.3302 0.2957\n",
      "EarlyStopping counter: 19 out of 100\n",
      "Epoch: 156 Step: 9516 Index:-0.6840 R2:0.5232 0.1913 0.3352 RMSE:0.7836 1.0017 0.9463 Tau:0.5212 0.3178 0.3030\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Epoch: 157 Step: 9577 Index:-0.6227 R2:0.5406 0.2120 0.3463 RMSE:0.7530 0.9585 0.9342 Tau:0.5326 0.3359 0.3152\n",
      "EarlyStopping counter: 21 out of 100\n",
      "Epoch: 158 Step: 9638 Index:-0.7098 R2:0.5121 0.1921 0.3377 RMSE:0.7957 1.0338 0.9508 Tau:0.5132 0.3240 0.2969\n",
      "EarlyStopping counter: 22 out of 100\n",
      "Epoch: 159 Step: 9699 Index:-0.6309 R2:0.5279 0.2056 0.3265 RMSE:0.7595 0.9637 0.9505 Tau:0.5256 0.3328 0.3044\n",
      "EarlyStopping counter: 23 out of 100\n",
      "Epoch: 160 Step: 9760 Index:-0.6332 R2:0.5299 0.2051 0.3121 RMSE:0.7877 0.9601 0.9966 Tau:0.5285 0.3269 0.3096\n",
      "EarlyStopping counter: 24 out of 100\n",
      "Epoch: 161 Step: 9821 Index:-0.6206 R2:0.5377 0.2098 0.3393 RMSE:0.7630 0.9576 0.9565 Tau:0.5355 0.3370 0.3127\n",
      "EarlyStopping counter: 25 out of 100\n",
      "Epoch: 162 Step: 9882 Index:-0.6649 R2:0.5284 0.1923 0.3280 RMSE:0.7612 0.9815 0.9484 Tau:0.5271 0.3166 0.3082\n",
      "EarlyStopping counter: 26 out of 100\n",
      "Epoch: 163 Step: 9943 Index:-0.6542 R2:0.5008 0.1948 0.3009 RMSE:0.7925 0.9758 0.9957 Tau:0.5037 0.3217 0.3148\n",
      "EarlyStopping counter: 27 out of 100\n",
      "Epoch: 164 Step: 10004 Index:-0.6277 R2:0.5355 0.2241 0.3625 RMSE:0.7562 0.9729 0.9252 Tau:0.5285 0.3452 0.3123\n",
      "EarlyStopping counter: 28 out of 100\n",
      "Epoch: 165 Step: 10065 Index:-0.6474 R2:0.5434 0.2146 0.3399 RMSE:0.7743 0.9924 0.9485 Tau:0.5376 0.3450 0.3219\n",
      "EarlyStopping counter: 29 out of 100\n",
      "Epoch: 166 Step: 10126 Index:-0.6162 R2:0.5349 0.2243 0.3546 RMSE:0.7710 0.9639 0.9615 Tau:0.5308 0.3477 0.3171\n",
      "EarlyStopping counter: 30 out of 100\n",
      "Epoch: 167 Step: 10187 Index:-0.6674 R2:0.5302 0.2049 0.3247 RMSE:0.8155 0.9924 0.9711 Tau:0.5240 0.3250 0.3166\n",
      "EarlyStopping counter: 31 out of 100\n",
      "Epoch: 168 Step: 10248 Index:-0.6134 R2:0.5455 0.2115 0.3309 RMSE:0.7498 0.9589 0.9462 Tau:0.5377 0.3456 0.3087\n",
      "EarlyStopping counter: 32 out of 100\n",
      "Epoch: 169 Step: 10309 Index:-0.6713 R2:0.5370 0.2117 0.3632 RMSE:0.7685 0.9984 0.9294 Tau:0.5346 0.3271 0.3220\n",
      "EarlyStopping counter: 33 out of 100\n",
      "Epoch: 170 Step: 10370 Index:-0.6793 R2:0.5450 0.1999 0.3401 RMSE:0.7671 1.0044 0.9445 Tau:0.5374 0.3252 0.3200\n",
      "EarlyStopping counter: 34 out of 100\n",
      "Epoch: 171 Step: 10431 Index:-0.6517 R2:0.5410 0.2010 0.3301 RMSE:0.7581 0.9760 0.9464 Tau:0.5349 0.3244 0.3184\n",
      "EarlyStopping counter: 35 out of 100\n",
      "Epoch: 172 Step: 10492 Index:-0.6545 R2:0.5604 0.2010 0.3358 RMSE:0.7499 0.9845 0.9447 Tau:0.5483 0.3300 0.3197\n",
      "EarlyStopping counter: 36 out of 100\n",
      "Epoch: 173 Step: 10553 Index:-0.6425 R2:0.5592 0.2050 0.3424 RMSE:0.7352 0.9753 0.9405 Tau:0.5488 0.3328 0.3192\n",
      "EarlyStopping counter: 37 out of 100\n",
      "Epoch: 174 Step: 10614 Index:-0.6390 R2:0.5590 0.2096 0.3629 RMSE:0.7337 0.9723 0.9259 Tau:0.5476 0.3333 0.3166\n",
      "EarlyStopping counter: 38 out of 100\n",
      "Epoch: 175 Step: 10675 Index:-0.6398 R2:0.5228 0.2027 0.3397 RMSE:0.7619 0.9708 0.9422 Tau:0.5250 0.3310 0.3201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 39 out of 100\n",
      "Epoch: 176 Step: 10736 Index:-0.7353 R2:0.5153 0.1927 0.3592 RMSE:0.7791 1.0558 0.9661 Tau:0.5154 0.3205 0.3058\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Epoch: 177 Step: 10797 Index:-0.6656 R2:0.5423 0.1914 0.3269 RMSE:0.7687 0.9959 0.9515 Tau:0.5309 0.3302 0.3011\n",
      "EarlyStopping counter: 41 out of 100\n",
      "Epoch: 178 Step: 10858 Index:-0.7250 R2:0.5260 0.1837 0.3329 RMSE:0.7834 1.0338 0.9535 Tau:0.5221 0.3088 0.3163\n",
      "EarlyStopping counter: 42 out of 100\n",
      "Epoch: 179 Step: 10919 Index:-0.6254 R2:0.5485 0.2117 0.3406 RMSE:0.7463 0.9694 0.9399 Tau:0.5420 0.3440 0.3226\n",
      "EarlyStopping counter: 43 out of 100\n",
      "Epoch: 180 Step: 10980 Index:-0.6425 R2:0.5467 0.2118 0.3327 RMSE:0.7417 0.9840 0.9596 Tau:0.5368 0.3415 0.3148\n",
      "EarlyStopping counter: 44 out of 100\n",
      "Epoch: 181 Step: 11041 Index:-0.6966 R2:0.5576 0.2006 0.3387 RMSE:0.7720 1.0237 0.9542 Tau:0.5452 0.3271 0.3133\n",
      "EarlyStopping counter: 45 out of 100\n",
      "Epoch: 182 Step: 11102 Index:-0.6474 R2:0.5654 0.2094 0.3530 RMSE:0.7275 0.9882 0.9452 Tau:0.5497 0.3407 0.3174\n",
      "EarlyStopping counter: 46 out of 100\n",
      "Epoch: 183 Step: 11163 Index:-0.6750 R2:0.5576 0.2030 0.3626 RMSE:0.7484 0.9984 0.9265 Tau:0.5485 0.3234 0.3274\n",
      "EarlyStopping counter: 47 out of 100\n",
      "Epoch: 184 Step: 11224 Index:-0.6748 R2:0.5659 0.1913 0.3401 RMSE:0.7310 0.9975 0.9532 Tau:0.5518 0.3226 0.3177\n",
      "EarlyStopping counter: 48 out of 100\n",
      "Epoch: 185 Step: 11285 Index:-0.6686 R2:0.5614 0.2066 0.3442 RMSE:0.7566 0.9948 0.9417 Tau:0.5534 0.3261 0.3226\n",
      "EarlyStopping counter: 49 out of 100\n",
      "Epoch: 186 Step: 11346 Index:-0.6084 R2:0.5559 0.2193 0.3277 RMSE:0.7360 0.9598 0.9565 Tau:0.5464 0.3514 0.3245\n",
      "EarlyStopping counter: 50 out of 100\n",
      "Epoch: 187 Step: 11407 Index:-0.6387 R2:0.5693 0.2127 0.3352 RMSE:0.7411 0.9849 0.9482 Tau:0.5562 0.3462 0.3209\n",
      "EarlyStopping counter: 51 out of 100\n",
      "Epoch: 188 Step: 11468 Index:-0.7117 R2:0.5501 0.1859 0.3218 RMSE:0.7411 1.0274 0.9653 Tau:0.5387 0.3156 0.2979\n",
      "EarlyStopping counter: 52 out of 100\n",
      "Epoch: 189 Step: 11529 Index:-0.6862 R2:0.5615 0.1887 0.3087 RMSE:0.7383 1.0015 0.9673 Tau:0.5485 0.3152 0.3237\n",
      "EarlyStopping counter: 53 out of 100\n",
      "Epoch: 190 Step: 11590 Index:-0.6590 R2:0.5707 0.2143 0.3497 RMSE:0.7715 1.0083 1.0035 Tau:0.5539 0.3493 0.3225\n",
      "EarlyStopping counter: 54 out of 100\n",
      "Epoch: 191 Step: 11651 Index:-0.6340 R2:0.5757 0.2179 0.3641 RMSE:0.7350 0.9754 0.9228 Tau:0.5595 0.3413 0.3227\n",
      "EarlyStopping counter: 55 out of 100\n",
      "Epoch: 192 Step: 11712 Index:-0.6665 R2:0.5731 0.2020 0.3497 RMSE:0.7216 0.9919 0.9394 Tau:0.5551 0.3254 0.3202\n",
      "EarlyStopping counter: 56 out of 100\n",
      "Epoch: 193 Step: 11773 Index:-0.7269 R2:0.4735 0.1557 0.2787 RMSE:0.8260 0.9891 1.0048 Tau:0.4927 0.2622 0.2966\n",
      "EarlyStopping counter: 57 out of 100\n",
      "Epoch: 194 Step: 11834 Index:-0.7204 R2:0.5655 0.2106 0.3413 RMSE:0.8064 1.0578 0.9801 Tau:0.5537 0.3374 0.3198\n",
      "EarlyStopping counter: 58 out of 100\n",
      "Epoch: 195 Step: 11895 Index:-0.7357 R2:0.5131 0.1822 0.2937 RMSE:0.8007 1.0550 0.9950 Tau:0.5129 0.3193 0.2947\n",
      "EarlyStopping counter: 59 out of 100\n",
      "Epoch: 196 Step: 11956 Index:-0.7200 R2:0.5797 0.1960 0.3301 RMSE:0.7826 1.0452 0.9745 Tau:0.5669 0.3252 0.3276\n",
      "EarlyStopping counter: 60 out of 100\n",
      "Epoch: 197 Step: 12017 Index:-0.6957 R2:0.5830 0.2079 0.3385 RMSE:0.7629 1.0308 0.9643 Tau:0.5666 0.3351 0.3215\n",
      "EarlyStopping counter: 61 out of 100\n",
      "Epoch: 198 Step: 12078 Index:-0.6387 R2:0.5837 0.2032 0.3354 RMSE:0.7188 0.9746 0.9540 Tau:0.5628 0.3359 0.3149\n",
      "EarlyStopping counter: 62 out of 100\n",
      "Epoch: 199 Step: 12139 Index:-0.7044 R2:0.5594 0.1977 0.3187 RMSE:0.7457 1.0350 0.9739 Tau:0.5483 0.3306 0.3238\n",
      "EarlyStopping counter: 63 out of 100\n",
      "Epoch: 200 Step: 12200 Index:-0.6755 R2:0.5834 0.2005 0.3346 RMSE:0.7280 0.9964 0.9486 Tau:0.5650 0.3209 0.3268\n",
      "EarlyStopping counter: 64 out of 100\n",
      "Epoch: 201 Step: 12261 Index:-0.6607 R2:0.5668 0.1991 0.3297 RMSE:0.7283 0.9876 0.9529 Tau:0.5531 0.3269 0.3236\n",
      "EarlyStopping counter: 65 out of 100\n",
      "Epoch: 202 Step: 12322 Index:-0.7057 R2:0.5816 0.1929 0.3069 RMSE:0.7796 1.0372 0.9889 Tau:0.5665 0.3316 0.3213\n",
      "EarlyStopping counter: 66 out of 100\n",
      "Epoch: 203 Step: 12383 Index:-0.6565 R2:0.5705 0.2035 0.3600 RMSE:0.7300 0.9823 0.9255 Tau:0.5533 0.3257 0.3308\n",
      "EarlyStopping counter: 67 out of 100\n",
      "Epoch: 204 Step: 12444 Index:-0.6589 R2:0.5867 0.1975 0.3358 RMSE:0.7156 0.9827 0.9525 Tau:0.5664 0.3238 0.3363\n",
      "EarlyStopping counter: 68 out of 100\n",
      "Epoch: 205 Step: 12505 Index:-0.6500 R2:0.5871 0.2005 0.3166 RMSE:0.7273 0.9878 0.9928 Tau:0.5661 0.3378 0.3254\n",
      "EarlyStopping counter: 69 out of 100\n",
      "Epoch: 206 Step: 12566 Index:-0.6959 R2:0.5600 0.1860 0.2995 RMSE:0.7462 1.0326 0.9844 Tau:0.5468 0.3366 0.3042\n",
      "EarlyStopping counter: 70 out of 100\n",
      "Epoch: 207 Step: 12627 Index:-0.7039 R2:0.5839 0.1969 0.3345 RMSE:0.7444 1.0338 0.9580 Tau:0.5618 0.3298 0.3144\n",
      "EarlyStopping counter: 71 out of 100\n",
      "Epoch: 208 Step: 12688 Index:-0.6500 R2:0.5911 0.1979 0.3293 RMSE:0.7179 0.9761 0.9589 Tau:0.5703 0.3261 0.3242\n",
      "EarlyStopping counter: 72 out of 100\n",
      "Epoch: 209 Step: 12749 Index:-0.6601 R2:0.5984 0.2160 0.3566 RMSE:0.7197 1.0090 0.9377 Tau:0.5748 0.3489 0.3278\n",
      "EarlyStopping counter: 73 out of 100\n",
      "Epoch: 210 Step: 12810 Index:-0.6568 R2:0.5933 0.2049 0.3452 RMSE:0.7106 0.9839 0.9382 Tau:0.5717 0.3271 0.3289\n",
      "EarlyStopping counter: 74 out of 100\n",
      "Epoch: 211 Step: 12871 Index:-0.6979 R2:0.5768 0.2012 0.3466 RMSE:0.7300 1.0254 0.9429 Tau:0.5585 0.3275 0.3138\n",
      "EarlyStopping counter: 75 out of 100\n",
      "Epoch: 212 Step: 12932 Index:-0.6297 R2:0.5970 0.2137 0.3430 RMSE:0.7071 0.9677 0.9400 Tau:0.5761 0.3380 0.3354\n",
      "EarlyStopping counter: 76 out of 100\n",
      "Epoch: 213 Step: 12993 Index:-0.6675 R2:0.5935 0.1953 0.3259 RMSE:0.7090 0.9884 0.9568 Tau:0.5698 0.3209 0.3259\n",
      "EarlyStopping counter: 77 out of 100\n",
      "Epoch: 214 Step: 13054 Index:-0.6649 R2:0.6013 0.2066 0.3586 RMSE:0.7109 0.9967 0.9287 Tau:0.5810 0.3318 0.3367\n",
      "EarlyStopping counter: 78 out of 100\n",
      "Epoch: 215 Step: 13115 Index:-0.6212 R2:0.5831 0.2255 0.3438 RMSE:0.7197 0.9668 0.9401 Tau:0.5698 0.3456 0.3448\n",
      "EarlyStopping counter: 79 out of 100\n",
      "Epoch: 216 Step: 13176 Index:-0.6727 R2:0.5894 0.2003 0.3486 RMSE:0.7230 1.0018 0.9392 Tau:0.5692 0.3291 0.3322\n",
      "EarlyStopping counter: 80 out of 100\n",
      "Epoch: 217 Step: 13237 Index:-0.6499 R2:0.5872 0.2174 0.3536 RMSE:0.7211 0.9854 0.9351 Tau:0.5732 0.3355 0.3408\n",
      "EarlyStopping counter: 81 out of 100\n",
      "Epoch: 218 Step: 13298 Index:-0.6665 R2:0.6012 0.2052 0.3536 RMSE:0.6997 0.9998 0.9525 Tau:0.5790 0.3333 0.3287\n",
      "EarlyStopping counter: 82 out of 100\n",
      "Epoch: 219 Step: 13359 Index:-0.6782 R2:0.5913 0.2015 0.3313 RMSE:0.7142 1.0257 0.9623 Tau:0.5649 0.3475 0.3198\n",
      "EarlyStopping counter: 83 out of 100\n",
      "Epoch: 220 Step: 13420 Index:-0.7339 R2:0.6149 0.2076 0.3590 RMSE:0.7617 1.0671 0.9654 Tau:0.5870 0.3331 0.3308\n",
      "EarlyStopping counter: 84 out of 100\n",
      "Epoch: 221 Step: 13481 Index:-0.6569 R2:0.5981 0.1963 0.3093 RMSE:0.7206 0.9914 1.0017 Tau:0.5726 0.3345 0.3240\n",
      "EarlyStopping counter: 85 out of 100\n",
      "Epoch: 222 Step: 13542 Index:-0.6652 R2:0.5941 0.2036 0.3512 RMSE:0.7101 0.9896 0.9514 Tau:0.5721 0.3244 0.3296\n",
      "EarlyStopping counter: 86 out of 100\n",
      "Epoch: 223 Step: 13603 Index:-0.6525 R2:0.6057 0.2108 0.3519 RMSE:0.7135 1.0026 0.9386 Tau:0.5801 0.3501 0.3171\n",
      "EarlyStopping counter: 87 out of 100\n",
      "Epoch: 224 Step: 13664 Index:-0.6575 R2:0.5798 0.2055 0.3236 RMSE:0.7183 0.9887 0.9705 Tau:0.5637 0.3312 0.3345\n",
      "EarlyStopping counter: 88 out of 100\n",
      "Epoch: 225 Step: 13725 Index:-0.6740 R2:0.6173 0.2066 0.3458 RMSE:0.6966 1.0056 0.9416 Tau:0.5896 0.3316 0.3308\n",
      "EarlyStopping counter: 89 out of 100\n",
      "Epoch: 226 Step: 13786 Index:-0.6742 R2:0.6184 0.1935 0.3441 RMSE:0.6870 1.0029 0.9450 Tau:0.5909 0.3287 0.3202\n",
      "EarlyStopping counter: 90 out of 100\n",
      "Epoch: 227 Step: 13847 Index:-0.6502 R2:0.6155 0.2008 0.3209 RMSE:0.6932 0.9808 0.9636 Tau:0.5877 0.3306 0.3296\n",
      "EarlyStopping counter: 91 out of 100\n",
      "Epoch: 228 Step: 13908 Index:-0.6489 R2:0.6160 0.2116 0.3457 RMSE:0.7093 0.9930 0.9791 Tau:0.5871 0.3440 0.3335\n",
      "EarlyStopping counter: 92 out of 100\n",
      "Epoch: 229 Step: 13969 Index:-0.7047 R2:0.5852 0.1950 0.3404 RMSE:0.7141 1.0304 0.9538 Tau:0.5621 0.3257 0.3112\n",
      "EarlyStopping counter: 93 out of 100\n",
      "Epoch: 230 Step: 14030 Index:-0.6559 R2:0.6181 0.2044 0.3468 RMSE:0.6892 0.9935 0.9536 Tau:0.5913 0.3376 0.3321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 94 out of 100\n",
      "Epoch: 231 Step: 14091 Index:-0.6434 R2:0.6195 0.2194 0.3493 RMSE:0.6867 0.9974 0.9438 Tau:0.5877 0.3539 0.3342\n",
      "EarlyStopping counter: 95 out of 100\n",
      "Epoch: 232 Step: 14152 Index:-0.7383 R2:0.6236 0.1994 0.3380 RMSE:0.7412 1.0720 0.9779 Tau:0.5919 0.3337 0.3317\n",
      "EarlyStopping counter: 96 out of 100\n",
      "Epoch: 233 Step: 14213 Index:-0.6779 R2:0.6251 0.2089 0.3533 RMSE:0.7014 1.0180 0.9824 Tau:0.5929 0.3401 0.3294\n",
      "EarlyStopping counter: 97 out of 100\n",
      "Epoch: 234 Step: 14274 Index:-0.6944 R2:0.6221 0.2004 0.3476 RMSE:0.7494 1.0250 1.0122 Tau:0.5933 0.3306 0.3297\n",
      "EarlyStopping counter: 98 out of 100\n",
      "Epoch: 235 Step: 14335 Index:-0.6517 R2:0.6257 0.2102 0.3400 RMSE:0.6840 0.9951 0.9476 Tau:0.5950 0.3434 0.3335\n",
      "EarlyStopping counter: 99 out of 100\n",
      "Epoch: 236 Step: 14396 Index:-0.6471 R2:0.6279 0.2060 0.3531 RMSE:0.6814 0.9840 0.9385 Tau:0.5950 0.3368 0.3254\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Epoch: 237 Step: 14457 Index:-0.6684 R2:0.6234 0.2008 0.3438 RMSE:0.6818 1.0009 0.9495 Tau:0.5909 0.3326 0.3398\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_one_hot', one_hot_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_interger', interger_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_binary', binary_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = - val_MSE**0.5 + val_tau \n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 237 r2:0.3445 RMSE:0.9381 WTI:0.2913 AP:0.4853 Tau:0.3043 \n",
      " \n",
      " Top-1:0.5000 Top-1-fp:0.1667 \n",
      " Top-5:0.3667 Top-5-fp:0.2333 \n",
      " Top-10:0.4754 Top-10-fp:0.2951 \n",
      " Top-15:0.5326 Top-15-fp:0.3152 \n",
      " Top-20:0.5285 Top-20-fp:0.3821 \n",
      " Top-25:0.5130 Top-25-fp:0.4416 \n",
      " Top-30:0.5081 Top-30-fp:0.4865 \n",
      " Top-40:0.6100 Top-40-fp:0.5061 \n",
      " Top-50:0.6750 Top-50-fp:0.5631 \n",
      " \n",
      " Top50:0.4400 Top50-fp:0.3400 \n",
      " Top100:0.5100 Top100-fp:0.3400 \n",
      " Top150:0.5200 Top150-fp:0.4267 \n",
      " Top200:0.5100 Top200-fp:0.4900 \n",
      " Top250:0.6100 Top250-fp:0.5120 \n",
      " Top300:0.6700 Top300-fp:0.5533 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = loss_function(mol_prediction,y)\n",
    "#             loss.backward(retain_graph=True)\n",
    "#             optimizer_AFSE.zero_grad()\n",
    "#             punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "\n",
    "# loss =  regression_loss + vat_loss + test_vat_loss\n",
    "\n",
    "#         init_lr = 1e-4\n",
    "#         max_lr = 10**-(init_lr-1)\n",
    "#         conv_lr = conv_lr - conv_lr**2 + 0.1 * punish_lr\n",
    "#         if conv_lr < max_lr:\n",
    "#             for param_group in optimizer_AFSE.param_groups:\n",
    "#                 param_group[\"lr\"] = conv_lr.detach()\n",
    "#                 AFSE_lr = conv_lr    \n",
    "#         else:\n",
    "#             for param_group in optimizer_AFSE.param_groups:\n",
    "#                 param_group[\"lr\"] = max_lr\n",
    "#                 AFSE_lr = max_lr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
