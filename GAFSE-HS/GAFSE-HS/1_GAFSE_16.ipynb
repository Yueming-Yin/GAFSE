{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "torch.manual_seed(8)\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EC50_P35372_1_180\n",
      "model_file/1_GAFSE_EC50_P35372_1_180_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/EC50_P35372_1_180_train.csv\"\n",
    "test_filename = \"./data/benchmark/EC50_P35372_1_180_test.csv\"\n",
    "test_active = 180\n",
    "val_rate = 0.2\n",
    "random_seed = 68\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/1_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"1_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/1_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0  CC1CN(CCC1(C)C2=CC(=CC=C2)O)CC(C(C)C)NC(=O)C3C... -2.079181\n",
      "1  CC1=CC=C(C=C1)C=CC(=O)N2CC3C(C2)C4(C=CC35C6CC7... -0.931966\n",
      "2   CC1(CCCCCCC1)N2CCC(CC2)N3C4=CC=CC=C4N=C3C5CCCNC5 -1.531479\n",
      "3         CC1C2CC3=C(C1(CCN2CC4CC4)C)C=C(C=C3)C(=O)N -0.431364\n",
      "4  C1CC1CN2CCC34C5C(CCC3(C2CC6=C4C(=C(C=C6)O)O5)O... -0.453318\n",
      "number of all smiles:  703\n",
      "number of successfully processed smiles:  703\n",
      "                                              smiles     value  \\\n",
      "0  CC1CN(CCC1(C)C2=CC(=CC=C2)O)CC(C(C)C)NC(=O)C3C... -2.079181   \n",
      "1  CC1=CC=C(C=C1)C=CC(=O)N2CC3C(C2)C4(C=CC35C6CC7... -0.931966   \n",
      "2   CC1(CCCCCCC1)N2CCC(CC2)N3C4=CC=CC=C4N=C3C5CCCNC5 -1.531479   \n",
      "3         CC1C2CC3=C(C1(CCN2CC4CC4)C)C=C(C=C3)C(=O)N -0.431364   \n",
      "4  C1CC1CN2CCC34C5C(CCC3(C2CC6=C4C(=C(C=C6)O)O5)O... -0.453318   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  CC(C)C(CN1CCC(C)(c2cccc(O)c2)C(C)C1)NC(=O)C1Cc...  \n",
      "1  COC12C=CC3(C4CN(C(=O)C=Cc5ccc(C)cc5)CC41)C1Cc4...  \n",
      "2     CC1(N2CCC(n3c(C4CCCNC4)nc4ccccc43)CC2)CCCCCCC1  \n",
      "3              CC1C2Cc3ccc(C(N)=O)cc3C1(C)CCN2CC1CC1  \n",
      "4  Cl.O=C(CNC(=O)c1ccncc1)NC1CCC2(O)C3Cc4ccc(O)c5...  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  982\n",
      "number of successfully processed smiles:  982\n",
      "(982, 3)\n",
      "                                              smiles     value  \\\n",
      "0  C1CCN(C1)CCN2C3=CC=CC=C3N=C2NC(=O)C4=CC(=C(C=C... -3.255273   \n",
      "1  CC(C(=O)NC(CC1=CC=CC=C1)C(=O)NCC(=O)NC(CC2=CC=... -1.010088   \n",
      "2  COC12CCC3(CC1COCC4=CC=CC=C4)C5CC6=C7C3(C2OC7=C... -1.055760   \n",
      "3  C=CCN1CCC23C4C(=O)CCC2(C1CC5=C3C(=C(C=C5)OC(=O... -0.301030   \n",
      "4  C=CCN1CCC23C4C(=O)CCC2(C1CC5=C3C(=C(C=C5)OC(=O... -0.301030   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0       O=C(Nc1nc2ccccc2n1CCN1CCCC1)c1ccc(Cl)c(Cl)c1  \n",
      "1  CC(NC(=O)C(N)Cc1ccc(O)cc1)C(=O)NC(Cc1ccccc1)C(...  \n",
      "2  COC12CCC3(CC1COCc1ccccc1)C1Cc4ccc(F)c5c4C3(CCN...  \n",
      "3  C=CCN1CCC23c4c5ccc(OC(=O)CCCCCCCCC(=O)Oc6ccc7c...  \n",
      "4  C=CCN1CCC23c4c5ccc(OC(=O)CCCCCCCCC(=O)Oc6ccc7c...  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/EC50_P35372_1_180_train.pickle\n",
      "./data/benchmark/EC50_P35372_1_180_train\n",
      "1685\n",
      "feature dicts file saved as ./data/benchmark/EC50_P35372_1_180_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(562, 3) (141, 3) (982, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    [d,e,f,g] = bond_neighbor.shape\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    one_hot_loss = 0\n",
    "    interger_loss = 0\n",
    "    binary_loss = 0\n",
    "    counter_i = 0\n",
    "    counter_j = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        one_hot_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-6)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)+\\\n",
    "                         weighted_CE_loss(atom_list[i,:l,16:22], x_atom[i,:l,16:22])+ \\\n",
    "                         weighted_CE_loss(atom_list[i,:l,24:30], x_atom[i,:l,24:30])+ \\\n",
    "                         weighted_CE_loss(atom_list[i,:l,31:36], x_atom[i,:l,31:36])\n",
    "        interger_loss += loss_function(atom_list[i,:l,23], x_atom[i,:l,23])+ \\\n",
    "                        loss_function(atom_list[i,:l,24], x_atom[i,:l,24])\n",
    "        binary_loss += CE(atom_list[i,:l,30], x_atom[i,:l,30])+ \\\n",
    "                        CE(atom_list[i,:l,36], x_atom[i,:l,36])+ \\\n",
    "                        CE(atom_list[i,:l,37], x_atom[i,:l,37])+ \\\n",
    "                        CE(atom_list[i,:l,38], x_atom[i,:l,38])\n",
    "        counter_i += 1\n",
    "        for j in range(l):\n",
    "            n = (bond_neighbor[i,j].sum(-1)!=0).sum(-1)\n",
    "            if n==0:\n",
    "                continue\n",
    "            one_hot_loss += weighted_CE_loss(bond_list[i,j,:n,:4], bond_neighbor[i,j,:n,:4])+ \\\n",
    "                             weighted_CE_loss(bond_list[i,j,:n,6:], bond_neighbor[i,j,:n,6:])\n",
    "            binary_loss += CE(bond_neighbor[i,j,:n,4], bond_list[i,j,:n,4])+ \\\n",
    "                           CE(bond_neighbor[i,j,:n,5], bond_list[i,j,:n,5])\n",
    "            counter_j += 1\n",
    "    one_hot_loss = one_hot_loss/(5*counter_i+2*counter_j)\n",
    "    interger_loss = interger_loss/(2*counter_i)\n",
    "    binary_loss = binary_loss/(4*counter_i+2*counter_j)\n",
    "    total_loss = (one_hot_loss + interger_loss + binary_loss)/3\n",
    "    return total_loss, one_hot_loss, interger_loss, binary_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "#         atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "#                                       torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "#         success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "#                             bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "#                                                      refer_atom_list, refer_bond_list,topn=1)\n",
    "#         reconstruction_loss, one_hot_loss, interger_loss,binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, \n",
    "#                                                                                               bond_neighbor, validity_mask, atom_list, \n",
    "#                                                                                               bond_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "#         atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "#                                                 torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "#                                                 mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "#         refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "#                                                             torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "#                                                             mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "#         success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "#                             bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "#                                                      refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "#         test_reconstruction_loss, test_one_hot_loss, test_interger_loss,test_binary_loss = generate_loss_function(atom_list_test, x_atom_test, bond_list_test, bond_neighbor_test, validity_mask_test, atom_list_test, bond_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        max_lr = 1e-3\n",
    "        conv_lr = conv_lr - conv_lr**2 + 0.06 * punish_lr\n",
    "        if conv_lr < max_lr and conv_lr >= 0:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = conv_lr.detach()\n",
    "                AFSE_lr = conv_lr    \n",
    "        elif conv_lr < 0:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = 0\n",
    "                AFSE_lr = 0\n",
    "        elif conv_lr >= max_lr:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = max_lr\n",
    "                AFSE_lr = max_lr\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_one_hot', one_hot_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_interger', interger_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_binary', binary_loss, global_step)\n",
    "        logger.add_scalar('lr/max_lr', conv_lr, global_step)\n",
    "        logger.add_scalar('lr/punish_lr', punish_lr, global_step)\n",
    "        logger.add_scalar('lr/AFSE_lr', AFSE_lr, global_step)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "#         optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6*(vat_loss + test_vat_loss) # + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "#         optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 10\n",
    "patience = 100\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/1_GAFSE_EC50_P35372_1_180_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 98 Index:-0.9753 R2:0.0761 0.1267 0.1411 RMSE:1.3051 1.2595 1.1489 Tau:0.2715 0.2843 0.2659\n",
      "Epoch: 2 Step: 196 Index:-0.9400 R2:0.2366 0.2465 0.2427 RMSE:1.2958 1.2951 1.0922 Tau:0.3378 0.3551 0.3241\n",
      "Epoch: 3 Step: 294 Index:-0.7990 R2:0.2877 0.2554 0.3082 RMSE:1.1720 1.1521 1.0412 Tau:0.3635 0.3531 0.3743\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 4 Step: 392 Index:-0.8327 R2:0.3291 0.2861 0.3715 RMSE:1.1862 1.2124 0.9646 Tau:0.4024 0.3797 0.3787\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 5 Step: 490 Index:-0.9880 R2:0.3092 0.3036 0.3830 RMSE:1.3595 1.3979 1.0917 Tau:0.4181 0.4099 0.3746\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 6 Step: 588 Index:-0.8150 R2:0.3554 0.2727 0.3891 RMSE:1.1753 1.1795 1.1015 Tau:0.4190 0.3644 0.4015\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 7 Step: 686 Index:-0.8687 R2:0.3686 0.2876 0.3954 RMSE:1.1812 1.2514 0.9664 Tau:0.4275 0.3827 0.3940\n",
      "Epoch: 8 Step: 784 Index:-0.7583 R2:0.3593 0.2646 0.3951 RMSE:1.0776 1.1291 0.9310 Tau:0.4313 0.3707 0.4033\n",
      "Epoch: 9 Step: 882 Index:-0.6926 R2:0.4030 0.3126 0.4210 RMSE:1.0735 1.0938 0.9692 Tau:0.4520 0.4012 0.4013\n",
      "Epoch: 10 Step: 980 Index:-0.6481 R2:0.4130 0.3379 0.4268 RMSE:1.0343 1.0710 0.9088 Tau:0.4598 0.4229 0.3972\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 11 Step: 1078 Index:-0.6629 R2:0.4274 0.3289 0.4372 RMSE:1.0234 1.0797 0.8972 Tau:0.4719 0.4168 0.4026\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 12 Step: 1176 Index:-0.7380 R2:0.4102 0.3058 0.4171 RMSE:1.0485 1.1303 0.9027 Tau:0.4568 0.3922 0.4073\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 13 Step: 1274 Index:-0.7300 R2:0.4313 0.3362 0.4312 RMSE:1.1291 1.1450 1.1133 Tau:0.4604 0.4150 0.4065\n",
      "Epoch: 14 Step: 1372 Index:-0.6238 R2:0.4522 0.3532 0.4460 RMSE:1.0055 1.0660 0.8802 Tau:0.4922 0.4422 0.4024\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 15 Step: 1470 Index:-0.7031 R2:0.4513 0.3332 0.4436 RMSE:1.0112 1.1185 0.8946 Tau:0.4783 0.4154 0.4082\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 16 Step: 1568 Index:-0.7083 R2:0.4688 0.3432 0.4465 RMSE:1.0778 1.1263 1.0686 Tau:0.4835 0.4180 0.4125\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 17 Step: 1666 Index:-0.6494 R2:0.4843 0.3795 0.4580 RMSE:1.0145 1.1011 0.8743 Tau:0.5055 0.4517 0.3995\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 18 Step: 1764 Index:-0.6250 R2:0.4969 0.3598 0.4667 RMSE:0.9709 1.0718 0.8620 Tau:0.5158 0.4468 0.4103\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 19 Step: 1862 Index:-0.6657 R2:0.5086 0.3645 0.4710 RMSE:0.9937 1.1095 0.8693 Tau:0.5196 0.4438 0.4108\n",
      "Epoch: 20 Step: 1960 Index:-0.5744 R2:0.5192 0.3847 0.4753 RMSE:0.9414 1.0316 0.8994 Tau:0.5231 0.4572 0.4105\n",
      "Epoch: 21 Step: 2058 Index:-0.5742 R2:0.5261 0.3826 0.4770 RMSE:0.9405 1.0308 0.8884 Tau:0.5322 0.4566 0.4143\n",
      "Epoch: 22 Step: 2156 Index:-0.5651 R2:0.5273 0.3985 0.4746 RMSE:0.9479 1.0407 0.8549 Tau:0.5380 0.4757 0.4070\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 23 Step: 2254 Index:-0.5897 R2:0.5336 0.3978 0.4817 RMSE:0.9678 1.0481 0.9699 Tau:0.5336 0.4584 0.4167\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 24 Step: 2352 Index:-0.6262 R2:0.5095 0.3950 0.4698 RMSE:0.9881 1.0671 1.0155 Tau:0.5130 0.4410 0.4141\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 25 Step: 2450 Index:-0.5740 R2:0.5541 0.4297 0.4973 RMSE:0.9362 1.0558 0.8641 Tau:0.5477 0.4818 0.4086\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 26 Step: 2548 Index:-0.6320 R2:0.5452 0.4053 0.4895 RMSE:1.0138 1.0856 1.0384 Tau:0.5379 0.4535 0.4140\n",
      "Epoch: 27 Step: 2646 Index:-0.5041 R2:0.5611 0.4513 0.4933 RMSE:0.8996 0.9910 0.8516 Tau:0.5419 0.4868 0.4018\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 28 Step: 2744 Index:-0.5837 R2:0.5765 0.4581 0.5024 RMSE:0.9705 1.0779 0.8754 Tau:0.5560 0.4941 0.4006\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 29 Step: 2842 Index:-0.5791 R2:0.5320 0.4005 0.4967 RMSE:0.9270 1.0288 0.8436 Tau:0.5341 0.4497 0.4109\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 30 Step: 2940 Index:-0.5591 R2:0.5522 0.4405 0.4953 RMSE:0.9529 1.0277 1.0018 Tau:0.5378 0.4686 0.4072\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 31 Step: 3038 Index:-0.5470 R2:0.5875 0.4681 0.5153 RMSE:0.9393 1.0499 0.8560 Tau:0.5637 0.5029 0.3981\n",
      "Epoch: 32 Step: 3136 Index:-0.4830 R2:0.5885 0.4658 0.5124 RMSE:0.8816 0.9781 0.8294 Tau:0.5632 0.4952 0.4016\n",
      "Epoch: 33 Step: 3234 Index:-0.4649 R2:0.5896 0.4594 0.5156 RMSE:0.8657 0.9670 0.8524 Tau:0.5645 0.5021 0.4011\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 34 Step: 3332 Index:-0.4998 R2:0.5947 0.4612 0.5196 RMSE:0.8816 0.9972 0.8342 Tau:0.5696 0.4974 0.4035\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 35 Step: 3430 Index:-0.5339 R2:0.5893 0.4351 0.5198 RMSE:0.8955 1.0187 0.8326 Tau:0.5707 0.4848 0.4075\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 36 Step: 3528 Index:-0.6149 R2:0.5838 0.4672 0.5113 RMSE:1.0300 1.1202 0.9180 Tau:0.5680 0.5053 0.3978\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 37 Step: 3626 Index:-0.4732 R2:0.6075 0.4786 0.5275 RMSE:0.8529 0.9688 0.8355 Tau:0.5770 0.4956 0.4044\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 38 Step: 3724 Index:-1.3708 R2:0.5744 0.4517 0.5045 RMSE:1.8110 1.8446 1.9437 Tau:0.5513 0.4738 0.4055\n",
      "Epoch: 39 Step: 3822 Index:-0.4545 R2:0.5998 0.4896 0.5352 RMSE:0.8799 0.9742 0.8217 Tau:0.5723 0.5197 0.3876\n",
      "Epoch: 40 Step: 3920 Index:-0.4313 R2:0.6076 0.4893 0.5336 RMSE:0.8493 0.9488 0.8249 Tau:0.5754 0.5175 0.3937\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 41 Step: 4018 Index:-0.4471 R2:0.6101 0.4787 0.5371 RMSE:0.8410 0.9502 0.8419 Tau:0.5772 0.5031 0.3993\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 42 Step: 4116 Index:-0.4418 R2:0.6188 0.4875 0.5433 RMSE:0.8336 0.9426 0.8434 Tau:0.5819 0.5008 0.3999\n",
      "Epoch: 43 Step: 4214 Index:-0.4251 R2:0.6176 0.5068 0.5317 RMSE:0.8362 0.9331 0.8279 Tau:0.5755 0.5079 0.3901\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 44 Step: 4312 Index:-0.4556 R2:0.6153 0.4929 0.5407 RMSE:0.8543 0.9554 0.8952 Tau:0.5799 0.4998 0.4006\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 45 Step: 4410 Index:-0.4724 R2:0.6154 0.4665 0.5413 RMSE:0.8514 0.9732 0.8110 Tau:0.5856 0.5008 0.4039\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 46 Step: 4508 Index:-0.4622 R2:0.6127 0.4903 0.5334 RMSE:0.8464 0.9594 0.8989 Tau:0.5751 0.4972 0.4022\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 47 Step: 4606 Index:-0.4267 R2:0.6275 0.5071 0.5361 RMSE:0.8286 0.9339 0.8811 Tau:0.5861 0.5071 0.4004\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 48 Step: 4704 Index:-0.4534 R2:0.6357 0.5091 0.5389 RMSE:0.8611 0.9669 0.8209 Tau:0.5891 0.5134 0.3913\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 49 Step: 4802 Index:-0.4335 R2:0.6309 0.5303 0.5351 RMSE:0.8562 0.9613 0.8655 Tau:0.5918 0.5278 0.3925\n",
      "Epoch: 50 Step: 4900 Index:-0.3887 R2:0.6405 0.5237 0.5453 RMSE:0.8147 0.9123 0.8587 Tau:0.5921 0.5236 0.3900\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 51 Step: 4998 Index:-0.4390 R2:0.6427 0.5124 0.5461 RMSE:0.8506 0.9536 0.9192 Tau:0.5953 0.5146 0.3993\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 52 Step: 5096 Index:-0.4998 R2:0.6264 0.4993 0.5376 RMSE:0.8819 1.0122 0.8803 Tau:0.5857 0.5124 0.3986\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 53 Step: 5194 Index:-0.3939 R2:0.6384 0.5264 0.5450 RMSE:0.8241 0.9205 0.8851 Tau:0.5948 0.5266 0.3975\n",
      "Epoch: 54 Step: 5292 Index:-0.3801 R2:0.6407 0.5349 0.5382 RMSE:0.8071 0.9051 0.8329 Tau:0.5933 0.5250 0.3902\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 55 Step: 5390 Index:-0.4102 R2:0.6401 0.5223 0.5422 RMSE:0.8173 0.9332 0.8341 Tau:0.5927 0.5230 0.3953\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 56 Step: 5488 Index:-0.4304 R2:0.6545 0.5276 0.5512 RMSE:0.8368 0.9578 0.8267 Tau:0.6019 0.5274 0.3956\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 57 Step: 5586 Index:-0.4193 R2:0.6510 0.5106 0.5594 RMSE:0.8164 0.9402 0.8687 Tau:0.6024 0.5209 0.4015\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 58 Step: 5684 Index:-0.4357 R2:0.6564 0.5215 0.5555 RMSE:0.8556 0.9702 0.8195 Tau:0.6051 0.5345 0.3955\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 59 Step: 5782 Index:-0.4503 R2:0.6551 0.5274 0.5498 RMSE:0.8479 0.9769 0.8478 Tau:0.5994 0.5266 0.3966\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 60 Step: 5880 Index:-0.4172 R2:0.6570 0.5309 0.5549 RMSE:0.8189 0.9519 0.8367 Tau:0.6024 0.5347 0.3961\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 61 Step: 5978 Index:-0.4336 R2:0.6483 0.5510 0.5479 RMSE:0.8730 0.9803 0.8535 Tau:0.5945 0.5467 0.3809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 Step: 6076 Index:-0.3505 R2:0.6599 0.5542 0.5532 RMSE:0.7869 0.8923 0.8228 Tau:0.6044 0.5418 0.3875\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 63 Step: 6174 Index:-0.4143 R2:0.6690 0.5556 0.5499 RMSE:0.8461 0.9559 0.8339 Tau:0.6076 0.5416 0.3870\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 64 Step: 6272 Index:-0.3885 R2:0.6735 0.5339 0.5622 RMSE:0.7876 0.9275 0.8161 Tau:0.6135 0.5390 0.3963\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 65 Step: 6370 Index:-0.4632 R2:0.6641 0.5379 0.5499 RMSE:0.8655 0.9981 0.8707 Tau:0.6076 0.5349 0.3988\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 66 Step: 6468 Index:-0.3616 R2:0.6732 0.5479 0.5517 RMSE:0.7729 0.8985 0.8262 Tau:0.6093 0.5370 0.3880\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 67 Step: 6566 Index:-0.3990 R2:0.6757 0.5633 0.5486 RMSE:0.8203 0.9445 0.8475 Tau:0.6113 0.5455 0.3837\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 68 Step: 6664 Index:-0.3777 R2:0.6726 0.5542 0.5507 RMSE:0.7798 0.9161 0.9060 Tau:0.6124 0.5384 0.3939\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 69 Step: 6762 Index:-0.3534 R2:0.6860 0.5503 0.5656 RMSE:0.7689 0.9001 0.8559 Tau:0.6220 0.5467 0.3974\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 70 Step: 6860 Index:-0.3695 R2:0.6916 0.5637 0.5623 RMSE:0.7860 0.9219 0.8255 Tau:0.6211 0.5524 0.3941\n",
      "Epoch: 71 Step: 6958 Index:-0.3330 R2:0.6853 0.5558 0.5599 RMSE:0.7633 0.8846 0.8435 Tau:0.6203 0.5516 0.3898\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 72 Step: 7056 Index:-0.3473 R2:0.6919 0.5722 0.5536 RMSE:0.7683 0.9011 0.8350 Tau:0.6198 0.5538 0.3898\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 73 Step: 7154 Index:-0.3621 R2:0.6902 0.5401 0.5709 RMSE:0.7530 0.9031 0.8026 Tau:0.6257 0.5410 0.4018\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 74 Step: 7252 Index:-0.4141 R2:0.6961 0.5651 0.5673 RMSE:0.8216 0.9716 0.8460 Tau:0.6222 0.5575 0.3941\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 75 Step: 7350 Index:-0.3724 R2:0.6997 0.5458 0.5691 RMSE:0.7632 0.9231 0.8159 Tau:0.6278 0.5508 0.4021\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 76 Step: 7448 Index:-0.3342 R2:0.7020 0.5614 0.5712 RMSE:0.7339 0.8872 0.8391 Tau:0.6276 0.5530 0.3993\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 77 Step: 7546 Index:-0.3696 R2:0.7013 0.5376 0.5639 RMSE:0.7411 0.9135 0.8186 Tau:0.6292 0.5439 0.4036\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 78 Step: 7644 Index:-0.3482 R2:0.6853 0.5758 0.5454 RMSE:0.8085 0.8991 0.9574 Tau:0.6159 0.5510 0.3939\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 79 Step: 7742 Index:-0.3584 R2:0.7013 0.5710 0.5669 RMSE:0.7593 0.9124 0.8225 Tau:0.6246 0.5540 0.3990\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 80 Step: 7840 Index:-0.3400 R2:0.7020 0.5532 0.5695 RMSE:0.7353 0.8993 0.8209 Tau:0.6322 0.5593 0.4038\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 81 Step: 7938 Index:-0.3769 R2:0.7002 0.5775 0.5544 RMSE:0.7876 0.9449 0.8959 Tau:0.6322 0.5680 0.3914\n",
      "Epoch: 82 Step: 8036 Index:-0.3144 R2:0.7080 0.5670 0.5623 RMSE:0.7389 0.8765 0.8070 Tau:0.6319 0.5621 0.3907\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 83 Step: 8134 Index:-0.3523 R2:0.7155 0.5522 0.5819 RMSE:0.7529 0.9100 0.7852 Tau:0.6418 0.5577 0.4053\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 84 Step: 8232 Index:-0.3426 R2:0.7166 0.5887 0.5735 RMSE:0.7440 0.9108 0.8500 Tau:0.6381 0.5682 0.3955\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 85 Step: 8330 Index:-0.4186 R2:0.6772 0.4925 0.5666 RMSE:0.7659 0.9527 0.8085 Tau:0.6192 0.5341 0.4118\n",
      "Epoch: 86 Step: 8428 Index:-0.3017 R2:0.7224 0.5964 0.5702 RMSE:0.7306 0.8797 0.8200 Tau:0.6408 0.5780 0.3885\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 87 Step: 8526 Index:-0.3269 R2:0.7200 0.5546 0.5784 RMSE:0.7150 0.8907 0.8034 Tau:0.6450 0.5638 0.3981\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 88 Step: 8624 Index:-0.3235 R2:0.7114 0.5755 0.5637 RMSE:0.7347 0.8968 0.8353 Tau:0.6354 0.5733 0.3995\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 89 Step: 8722 Index:-0.3398 R2:0.7241 0.5587 0.5723 RMSE:0.7289 0.8987 0.8555 Tau:0.6422 0.5589 0.4069\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 90 Step: 8820 Index:-0.3299 R2:0.7105 0.5924 0.5635 RMSE:0.7514 0.9042 0.8429 Tau:0.6321 0.5743 0.3830\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 91 Step: 8918 Index:-0.3453 R2:0.7296 0.5588 0.5813 RMSE:0.7233 0.9117 0.8071 Tau:0.6514 0.5664 0.4056\n",
      "Epoch: 92 Step: 9016 Index:-0.2844 R2:0.7234 0.5738 0.5657 RMSE:0.7199 0.8636 0.8109 Tau:0.6445 0.5792 0.3874\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 93 Step: 9114 Index:-0.3954 R2:0.7357 0.5977 0.5704 RMSE:0.8033 0.9790 0.8778 Tau:0.6481 0.5837 0.3957\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 94 Step: 9212 Index:-0.2859 R2:0.7361 0.5846 0.5846 RMSE:0.7264 0.8622 0.8391 Tau:0.6511 0.5763 0.4005\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 95 Step: 9310 Index:-0.3033 R2:0.7393 0.5957 0.5811 RMSE:0.7006 0.8835 0.8238 Tau:0.6481 0.5802 0.3946\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 96 Step: 9408 Index:-0.3401 R2:0.7197 0.5822 0.5701 RMSE:0.7322 0.9069 0.9125 Tau:0.6393 0.5668 0.4038\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 97 Step: 9506 Index:-0.2874 R2:0.7405 0.5840 0.5793 RMSE:0.6881 0.8682 0.8364 Tau:0.6521 0.5808 0.4050\n",
      "Epoch: 98 Step: 9604 Index:-0.2828 R2:0.7312 0.6056 0.5660 RMSE:0.7184 0.8717 0.8184 Tau:0.6394 0.5889 0.3807\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 99 Step: 9702 Index:-0.4425 R2:0.7030 0.5923 0.5430 RMSE:0.8360 1.0188 0.9671 Tau:0.6236 0.5763 0.3908\n",
      "Epoch: 100 Step: 9800 Index:-0.2748 R2:0.7479 0.5803 0.5932 RMSE:0.6825 0.8674 0.7832 Tau:0.6608 0.5926 0.4054\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 101 Step: 9898 Index:-0.2877 R2:0.7355 0.5791 0.5688 RMSE:0.7723 0.8709 0.8208 Tau:0.6459 0.5832 0.3935\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 102 Step: 9996 Index:-0.2793 R2:0.7470 0.5818 0.5894 RMSE:0.6869 0.8587 0.7889 Tau:0.6561 0.5794 0.3998\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 103 Step: 10094 Index:-0.2928 R2:0.7550 0.5944 0.5862 RMSE:0.6866 0.8815 0.8092 Tau:0.6638 0.5887 0.4026\n",
      "Epoch: 104 Step: 10192 Index:-0.2512 R2:0.7481 0.6124 0.5716 RMSE:0.6801 0.8456 0.8601 Tau:0.6532 0.5944 0.3914\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 105 Step: 10290 Index:-0.2767 R2:0.7556 0.5873 0.5865 RMSE:0.6710 0.8750 0.8071 Tau:0.6633 0.5983 0.4048\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 106 Step: 10388 Index:-0.2572 R2:0.7602 0.5913 0.5879 RMSE:0.6653 0.8533 0.7919 Tau:0.6657 0.5960 0.4041\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 107 Step: 10486 Index:-0.2961 R2:0.7618 0.5867 0.5949 RMSE:0.6627 0.8854 0.8412 Tau:0.6673 0.5893 0.4088\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 108 Step: 10584 Index:-0.2963 R2:0.7471 0.6054 0.5852 RMSE:0.7572 0.8819 0.8949 Tau:0.6563 0.5857 0.3984\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 109 Step: 10682 Index:-0.2779 R2:0.7614 0.6039 0.5850 RMSE:0.6635 0.8737 0.8227 Tau:0.6669 0.5958 0.3988\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 110 Step: 10780 Index:-0.2993 R2:0.7694 0.5895 0.5895 RMSE:0.6638 0.8915 0.8156 Tau:0.6697 0.5922 0.4065\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 111 Step: 10878 Index:-0.2610 R2:0.7715 0.5951 0.5986 RMSE:0.6493 0.8639 0.7884 Tau:0.6729 0.6029 0.4078\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 112 Step: 10976 Index:-0.4226 R2:0.7714 0.5842 0.5973 RMSE:0.7990 1.0215 0.9046 Tau:0.6758 0.5989 0.4131\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 113 Step: 11074 Index:-0.2570 R2:0.7719 0.6082 0.5994 RMSE:0.6436 0.8610 0.8050 Tau:0.6743 0.6039 0.4042\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 114 Step: 11172 Index:-0.2749 R2:0.7649 0.5866 0.5932 RMSE:0.6597 0.8705 0.8145 Tau:0.6717 0.5956 0.4130\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 115 Step: 11270 Index:-0.3042 R2:0.7632 0.6087 0.5732 RMSE:0.6907 0.9010 0.8521 Tau:0.6709 0.5968 0.3991\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Epoch: 116 Step: 11368 Index:-0.2634 R2:0.7666 0.6078 0.5885 RMSE:0.6738 0.8624 0.8615 Tau:0.6712 0.5991 0.4060\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Epoch: 117 Step: 11466 Index:-0.3267 R2:0.7726 0.5744 0.5947 RMSE:0.6898 0.9245 0.8147 Tau:0.6796 0.5979 0.4114\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Epoch: 118 Step: 11564 Index:-0.2517 R2:0.7687 0.6217 0.5893 RMSE:0.6521 0.8589 0.8158 Tau:0.6681 0.6072 0.3967\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Epoch: 119 Step: 11662 Index:-0.4496 R2:0.7217 0.5933 0.5870 RMSE:0.8696 1.0560 0.9426 Tau:0.6646 0.6064 0.4021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 16 out of 100\n",
      "Epoch: 120 Step: 11760 Index:-0.3467 R2:0.7528 0.5581 0.5887 RMSE:0.6729 0.9307 0.8386 Tau:0.6699 0.5841 0.4152\n",
      "EarlyStopping counter: 17 out of 100\n",
      "Epoch: 121 Step: 11858 Index:-0.2791 R2:0.7771 0.6144 0.5945 RMSE:0.6484 0.8893 0.8359 Tau:0.6753 0.6102 0.4036\n",
      "EarlyStopping counter: 18 out of 100\n",
      "Epoch: 122 Step: 11956 Index:-0.3431 R2:0.7847 0.5938 0.5985 RMSE:0.6777 0.9464 0.8570 Tau:0.6831 0.6033 0.4128\n",
      "Epoch: 123 Step: 12054 Index:-0.2448 R2:0.7853 0.6106 0.6001 RMSE:0.6364 0.8532 0.8240 Tau:0.6802 0.6084 0.4114\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 124 Step: 12152 Index:-0.3396 R2:0.7637 0.5726 0.5916 RMSE:0.6587 0.9285 0.8650 Tau:0.6711 0.5889 0.4183\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 125 Step: 12250 Index:-0.2945 R2:0.7875 0.6049 0.6082 RMSE:0.6504 0.9023 0.8143 Tau:0.6857 0.6078 0.4068\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 126 Step: 12348 Index:-0.2903 R2:0.7625 0.6230 0.5737 RMSE:0.7562 0.9033 0.9620 Tau:0.6670 0.6131 0.4005\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 127 Step: 12446 Index:-0.2521 R2:0.7966 0.6110 0.5998 RMSE:0.6124 0.8680 0.8059 Tau:0.6910 0.6159 0.4084\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 128 Step: 12544 Index:-0.3117 R2:0.7793 0.5674 0.5910 RMSE:0.6352 0.9037 0.8064 Tau:0.6833 0.5920 0.4209\n",
      "Epoch: 129 Step: 12642 Index:-0.2359 R2:0.7922 0.6196 0.5926 RMSE:0.6185 0.8494 0.8366 Tau:0.6864 0.6135 0.4051\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 130 Step: 12740 Index:-0.4135 R2:0.7923 0.5885 0.6023 RMSE:0.7745 1.0182 0.9100 Tau:0.6911 0.6048 0.4135\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 131 Step: 12838 Index:-0.3257 R2:0.7864 0.5958 0.6031 RMSE:0.6355 0.9369 0.8611 Tau:0.6873 0.6113 0.4058\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 132 Step: 12936 Index:-0.2786 R2:0.7958 0.6166 0.5978 RMSE:0.6474 0.8947 0.8252 Tau:0.6954 0.6161 0.4042\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 133 Step: 13034 Index:-0.3114 R2:0.8005 0.5858 0.6015 RMSE:0.6499 0.9239 0.8233 Tau:0.6982 0.6125 0.4153\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 134 Step: 13132 Index:-0.3119 R2:0.7965 0.6047 0.5860 RMSE:0.6310 0.9211 0.8725 Tau:0.6935 0.6092 0.4075\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 135 Step: 13230 Index:-0.2846 R2:0.7764 0.6178 0.5698 RMSE:0.6496 0.8938 0.8831 Tau:0.6745 0.6092 0.4060\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 136 Step: 13328 Index:-0.3168 R2:0.7784 0.5542 0.6006 RMSE:0.6409 0.9163 0.7902 Tau:0.6940 0.5995 0.4192\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 137 Step: 13426 Index:-0.3314 R2:0.7821 0.5467 0.5808 RMSE:0.6378 0.9103 0.8045 Tau:0.6912 0.5790 0.4178\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 138 Step: 13524 Index:-0.5741 R2:0.7905 0.5732 0.6035 RMSE:0.8951 1.1762 1.0568 Tau:0.6945 0.6021 0.4194\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 139 Step: 13622 Index:-0.3941 R2:0.8049 0.5930 0.6024 RMSE:0.7026 1.0086 0.9015 Tau:0.7003 0.6145 0.4134\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 140 Step: 13720 Index:-0.2844 R2:0.8055 0.5969 0.5979 RMSE:0.6065 0.8970 0.8183 Tau:0.6989 0.6127 0.4130\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Epoch: 141 Step: 13818 Index:-0.2625 R2:0.8025 0.6000 0.6031 RMSE:0.6082 0.8778 0.8192 Tau:0.7007 0.6153 0.4082\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Epoch: 142 Step: 13916 Index:-0.3546 R2:0.8139 0.5981 0.6073 RMSE:0.7052 0.9754 0.8678 Tau:0.7106 0.6208 0.4101\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Epoch: 143 Step: 14014 Index:-0.2994 R2:0.8002 0.5701 0.5953 RMSE:0.6308 0.8997 0.7889 Tau:0.7031 0.6003 0.4183\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Epoch: 144 Step: 14112 Index:-0.2774 R2:0.8086 0.6036 0.5935 RMSE:0.6256 0.8966 0.8158 Tau:0.7001 0.6192 0.4082\n",
      "EarlyStopping counter: 16 out of 100\n",
      "Epoch: 145 Step: 14210 Index:-0.2854 R2:0.8038 0.6167 0.6109 RMSE:0.6303 0.9033 0.8144 Tau:0.6969 0.6180 0.4087\n",
      "EarlyStopping counter: 17 out of 100\n",
      "Epoch: 146 Step: 14308 Index:-0.3127 R2:0.8099 0.5836 0.5934 RMSE:0.6243 0.9213 0.8274 Tau:0.7089 0.6086 0.4158\n",
      "EarlyStopping counter: 18 out of 100\n",
      "Epoch: 147 Step: 14406 Index:-0.2413 R2:0.8071 0.6200 0.6080 RMSE:0.5911 0.8613 0.8034 Tau:0.7003 0.6200 0.4085\n",
      "EarlyStopping counter: 19 out of 100\n",
      "Epoch: 148 Step: 14504 Index:-0.3097 R2:0.8057 0.5854 0.6061 RMSE:0.5916 0.9203 0.8288 Tau:0.7075 0.6106 0.4208\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Epoch: 149 Step: 14602 Index:-0.2985 R2:0.8184 0.6018 0.6079 RMSE:0.5853 0.9193 0.8173 Tau:0.7094 0.6208 0.4154\n",
      "EarlyStopping counter: 21 out of 100\n",
      "Epoch: 150 Step: 14700 Index:-0.2813 R2:0.8026 0.5906 0.5984 RMSE:0.6274 0.8911 0.8237 Tau:0.7011 0.6098 0.4189\n",
      "EarlyStopping counter: 22 out of 100\n",
      "Epoch: 151 Step: 14798 Index:-0.3712 R2:0.8098 0.5812 0.5999 RMSE:0.6660 0.9861 0.8723 Tau:0.7051 0.6149 0.4179\n",
      "EarlyStopping counter: 23 out of 100\n",
      "Epoch: 152 Step: 14896 Index:-0.2873 R2:0.8089 0.5910 0.5953 RMSE:0.5877 0.9020 0.8239 Tau:0.7002 0.6147 0.4199\n",
      "EarlyStopping counter: 24 out of 100\n",
      "Epoch: 153 Step: 14994 Index:-0.3344 R2:0.7991 0.5838 0.6018 RMSE:0.6486 0.9441 0.8754 Tau:0.6966 0.6096 0.4241\n",
      "EarlyStopping counter: 25 out of 100\n",
      "Epoch: 154 Step: 15092 Index:-0.2683 R2:0.8226 0.5925 0.6151 RMSE:0.5856 0.8861 0.7959 Tau:0.7156 0.6178 0.4190\n",
      "EarlyStopping counter: 26 out of 100\n",
      "Epoch: 155 Step: 15190 Index:-0.3424 R2:0.8203 0.5939 0.5987 RMSE:0.6572 0.9671 0.8573 Tau:0.7120 0.6247 0.4114\n",
      "EarlyStopping counter: 27 out of 100\n",
      "Epoch: 156 Step: 15288 Index:-0.3094 R2:0.8162 0.6027 0.5997 RMSE:0.5864 0.9314 0.8529 Tau:0.7082 0.6220 0.4172\n",
      "EarlyStopping counter: 28 out of 100\n",
      "Epoch: 157 Step: 15386 Index:-0.3018 R2:0.8083 0.5843 0.6165 RMSE:0.5881 0.9137 0.7959 Tau:0.7097 0.6119 0.4210\n",
      "EarlyStopping counter: 29 out of 100\n",
      "Epoch: 158 Step: 15484 Index:-0.2933 R2:0.8244 0.6071 0.6060 RMSE:0.5772 0.9178 0.8220 Tau:0.7124 0.6244 0.4116\n",
      "EarlyStopping counter: 30 out of 100\n",
      "Epoch: 159 Step: 15582 Index:-0.2571 R2:0.8198 0.5962 0.5942 RMSE:0.5844 0.8791 0.8077 Tau:0.7148 0.6220 0.4137\n",
      "EarlyStopping counter: 31 out of 100\n",
      "Epoch: 160 Step: 15680 Index:-0.2881 R2:0.8129 0.6125 0.5917 RMSE:0.6013 0.9062 0.8785 Tau:0.7070 0.6182 0.4180\n",
      "EarlyStopping counter: 32 out of 100\n",
      "Epoch: 161 Step: 15778 Index:-0.3192 R2:0.8282 0.5956 0.6096 RMSE:0.5869 0.9390 0.8185 Tau:0.7168 0.6198 0.4156\n",
      "EarlyStopping counter: 33 out of 100\n",
      "Epoch: 162 Step: 15876 Index:-0.2947 R2:0.8125 0.6024 0.5973 RMSE:0.5851 0.9193 0.8255 Tau:0.7082 0.6247 0.4165\n",
      "EarlyStopping counter: 34 out of 100\n",
      "Epoch: 163 Step: 15974 Index:-0.3440 R2:0.8150 0.5878 0.5942 RMSE:0.5811 0.9555 0.8740 Tau:0.7074 0.6115 0.4210\n",
      "EarlyStopping counter: 35 out of 100\n",
      "Epoch: 164 Step: 16072 Index:-0.2713 R2:0.8262 0.5992 0.6009 RMSE:0.5625 0.8931 0.8061 Tau:0.7160 0.6218 0.4194\n",
      "EarlyStopping counter: 36 out of 100\n",
      "Epoch: 165 Step: 16170 Index:-0.2689 R2:0.8220 0.6008 0.6232 RMSE:0.5693 0.8986 0.7817 Tau:0.7144 0.6297 0.4219\n",
      "EarlyStopping counter: 37 out of 100\n",
      "Epoch: 166 Step: 16268 Index:-0.2720 R2:0.8286 0.5964 0.6026 RMSE:0.5592 0.8989 0.7999 Tau:0.7194 0.6269 0.4221\n",
      "EarlyStopping counter: 38 out of 100\n",
      "Epoch: 167 Step: 16366 Index:-0.3387 R2:0.8256 0.5987 0.6085 RMSE:0.5742 0.9642 0.8554 Tau:0.7149 0.6255 0.4245\n",
      "EarlyStopping counter: 39 out of 100\n",
      "Epoch: 168 Step: 16464 Index:-0.3244 R2:0.8328 0.5921 0.6120 RMSE:0.5511 0.9439 0.8303 Tau:0.7249 0.6196 0.4254\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Epoch: 169 Step: 16562 Index:-0.3094 R2:0.8325 0.6030 0.6180 RMSE:0.5537 0.9385 0.8341 Tau:0.7241 0.6291 0.4228\n",
      "EarlyStopping counter: 41 out of 100\n",
      "Epoch: 170 Step: 16660 Index:-0.2591 R2:0.8086 0.5876 0.5822 RMSE:0.5996 0.8795 0.8012 Tau:0.7015 0.6204 0.4174\n",
      "EarlyStopping counter: 42 out of 100\n",
      "Epoch: 171 Step: 16758 Index:-0.3492 R2:0.8145 0.5657 0.5857 RMSE:0.6068 0.9569 0.8379 Tau:0.7075 0.6076 0.4194\n",
      "EarlyStopping counter: 43 out of 100\n",
      "Epoch: 172 Step: 16856 Index:-0.3255 R2:0.8273 0.5797 0.6067 RMSE:0.5596 0.9412 0.8287 Tau:0.7209 0.6157 0.4292\n",
      "EarlyStopping counter: 44 out of 100\n",
      "Epoch: 173 Step: 16954 Index:-0.3358 R2:0.7631 0.5226 0.5815 RMSE:0.7353 0.9449 0.7861 Tau:0.6835 0.6090 0.4087\n",
      "EarlyStopping counter: 45 out of 100\n",
      "Epoch: 174 Step: 17052 Index:-0.2755 R2:0.8288 0.5875 0.6218 RMSE:0.5674 0.9034 0.7620 Tau:0.7211 0.6279 0.4219\n",
      "EarlyStopping counter: 46 out of 100\n",
      "Epoch: 175 Step: 17150 Index:-0.2758 R2:0.8352 0.5923 0.6067 RMSE:0.5497 0.9043 0.8019 Tau:0.7259 0.6285 0.4240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 47 out of 100\n",
      "Epoch: 176 Step: 17248 Index:-0.2867 R2:0.8291 0.6069 0.6030 RMSE:0.5590 0.9152 0.8173 Tau:0.7202 0.6285 0.4217\n",
      "EarlyStopping counter: 48 out of 100\n",
      "Epoch: 177 Step: 17346 Index:-0.3489 R2:0.8283 0.5994 0.6051 RMSE:0.5622 0.9695 0.8651 Tau:0.7163 0.6206 0.4224\n",
      "EarlyStopping counter: 49 out of 100\n",
      "Epoch: 178 Step: 17444 Index:-0.3520 R2:0.8333 0.5825 0.6214 RMSE:0.5508 0.9706 0.8195 Tau:0.7256 0.6186 0.4222\n",
      "EarlyStopping counter: 50 out of 100\n",
      "Epoch: 179 Step: 17542 Index:-0.3250 R2:0.8102 0.5870 0.6068 RMSE:0.6191 0.9399 0.7935 Tau:0.7027 0.6149 0.4273\n",
      "EarlyStopping counter: 51 out of 100\n",
      "Epoch: 180 Step: 17640 Index:-0.2910 R2:0.8246 0.6008 0.6100 RMSE:0.5793 0.9183 0.7977 Tau:0.7145 0.6273 0.4245\n",
      "EarlyStopping counter: 52 out of 100\n",
      "Epoch: 181 Step: 17738 Index:-0.3464 R2:0.8411 0.5875 0.6224 RMSE:0.5747 0.9743 0.8241 Tau:0.7331 0.6279 0.4276\n",
      "EarlyStopping counter: 53 out of 100\n",
      "Epoch: 182 Step: 17836 Index:-0.2798 R2:0.8266 0.6044 0.6216 RMSE:0.5590 0.9093 0.7834 Tau:0.7181 0.6295 0.4212\n",
      "EarlyStopping counter: 54 out of 100\n",
      "Epoch: 183 Step: 17934 Index:-0.3592 R2:0.8367 0.5632 0.6156 RMSE:0.5946 0.9769 0.8125 Tau:0.7274 0.6178 0.4264\n",
      "EarlyStopping counter: 55 out of 100\n",
      "Epoch: 184 Step: 18032 Index:-0.3255 R2:0.8408 0.6014 0.6138 RMSE:0.5394 0.9502 0.8231 Tau:0.7286 0.6247 0.4253\n",
      "EarlyStopping counter: 56 out of 100\n",
      "Epoch: 185 Step: 18130 Index:-0.3562 R2:0.8447 0.5721 0.6027 RMSE:0.5728 0.9796 0.8298 Tau:0.7359 0.6234 0.4221\n",
      "EarlyStopping counter: 57 out of 100\n",
      "Epoch: 186 Step: 18228 Index:-0.3956 R2:0.8377 0.5900 0.6080 RMSE:0.5728 1.0256 0.8678 Tau:0.7254 0.6299 0.4258\n",
      "EarlyStopping counter: 58 out of 100\n",
      "Epoch: 187 Step: 18326 Index:-0.2773 R2:0.8380 0.5929 0.5954 RMSE:0.5508 0.9103 0.8032 Tau:0.7304 0.6330 0.4184\n",
      "EarlyStopping counter: 59 out of 100\n",
      "Epoch: 188 Step: 18424 Index:-0.3641 R2:0.8396 0.5526 0.6214 RMSE:0.5467 0.9790 0.7781 Tau:0.7373 0.6149 0.4368\n",
      "EarlyStopping counter: 60 out of 100\n",
      "Epoch: 189 Step: 18522 Index:-0.4478 R2:0.8296 0.5502 0.5974 RMSE:0.6210 1.0611 0.8928 Tau:0.7266 0.6133 0.4326\n",
      "EarlyStopping counter: 61 out of 100\n",
      "Epoch: 190 Step: 18620 Index:-0.3636 R2:0.8347 0.5981 0.5999 RMSE:0.5957 0.9960 0.8609 Tau:0.7273 0.6324 0.4179\n",
      "EarlyStopping counter: 62 out of 100\n",
      "Epoch: 191 Step: 18718 Index:-0.3407 R2:0.8429 0.5857 0.5994 RMSE:0.5433 0.9666 0.8482 Tau:0.7323 0.6259 0.4262\n",
      "EarlyStopping counter: 63 out of 100\n",
      "Epoch: 192 Step: 18816 Index:-0.3522 R2:0.8508 0.5682 0.6169 RMSE:0.5342 0.9845 0.8082 Tau:0.7442 0.6324 0.4308\n",
      "EarlyStopping counter: 64 out of 100\n",
      "Epoch: 193 Step: 18914 Index:-0.3035 R2:0.8406 0.5648 0.5995 RMSE:0.5639 0.9418 0.7919 Tau:0.7329 0.6383 0.4236\n",
      "EarlyStopping counter: 65 out of 100\n",
      "Epoch: 194 Step: 19012 Index:-0.3456 R2:0.8460 0.5853 0.6182 RMSE:0.5285 0.9729 0.8177 Tau:0.7360 0.6273 0.4270\n",
      "EarlyStopping counter: 66 out of 100\n",
      "Epoch: 195 Step: 19110 Index:-0.3833 R2:0.8138 0.5467 0.6167 RMSE:0.5920 0.9986 0.7776 Tau:0.7122 0.6153 0.4291\n",
      "EarlyStopping counter: 67 out of 100\n",
      "Epoch: 196 Step: 19208 Index:-0.5058 R2:0.8534 0.5695 0.6066 RMSE:0.6821 1.1347 0.9447 Tau:0.7438 0.6289 0.4294\n",
      "EarlyStopping counter: 68 out of 100\n",
      "Epoch: 197 Step: 19306 Index:-0.3872 R2:0.8330 0.5712 0.6167 RMSE:0.5498 1.0023 0.8157 Tau:0.7212 0.6151 0.4281\n",
      "EarlyStopping counter: 69 out of 100\n",
      "Epoch: 198 Step: 19404 Index:-0.4897 R2:0.8460 0.5416 0.6138 RMSE:0.6124 1.1076 0.8856 Tau:0.7425 0.6180 0.4364\n",
      "EarlyStopping counter: 70 out of 100\n",
      "Epoch: 199 Step: 19502 Index:-0.3863 R2:0.8419 0.5301 0.6193 RMSE:0.5528 1.0071 0.7682 Tau:0.7412 0.6208 0.4318\n",
      "EarlyStopping counter: 71 out of 100\n",
      "Epoch: 200 Step: 19600 Index:-0.3699 R2:0.8465 0.5535 0.6038 RMSE:0.6086 0.9949 0.8379 Tau:0.7367 0.6251 0.4322\n",
      "EarlyStopping counter: 72 out of 100\n",
      "Epoch: 201 Step: 19698 Index:-0.4951 R2:0.8495 0.5449 0.6152 RMSE:0.6042 1.1147 0.8911 Tau:0.7406 0.6196 0.4356\n",
      "EarlyStopping counter: 73 out of 100\n",
      "Epoch: 202 Step: 19796 Index:-0.4175 R2:0.8490 0.5373 0.6175 RMSE:0.5469 1.0439 0.7956 Tau:0.7459 0.6265 0.4335\n",
      "EarlyStopping counter: 74 out of 100\n",
      "Epoch: 203 Step: 19894 Index:-0.3497 R2:0.8437 0.5716 0.6141 RMSE:0.5391 0.9756 0.7854 Tau:0.7325 0.6259 0.4284\n",
      "EarlyStopping counter: 75 out of 100\n",
      "Epoch: 204 Step: 19992 Index:-0.3812 R2:0.8523 0.5390 0.6116 RMSE:0.5269 1.0000 0.7857 Tau:0.7445 0.6188 0.4411\n",
      "EarlyStopping counter: 76 out of 100\n",
      "Epoch: 205 Step: 20090 Index:-0.4631 R2:0.8551 0.5608 0.6174 RMSE:0.5215 1.0894 0.8688 Tau:0.7470 0.6263 0.4332\n",
      "EarlyStopping counter: 77 out of 100\n",
      "Epoch: 206 Step: 20188 Index:-0.3303 R2:0.8534 0.5407 0.5951 RMSE:0.5488 0.9562 0.7764 Tau:0.7444 0.6259 0.4342\n",
      "EarlyStopping counter: 78 out of 100\n",
      "Epoch: 207 Step: 20286 Index:-0.3628 R2:0.8544 0.5417 0.6177 RMSE:0.5351 0.9925 0.7726 Tau:0.7491 0.6297 0.4343\n",
      "EarlyStopping counter: 79 out of 100\n",
      "Epoch: 208 Step: 20384 Index:-0.3772 R2:0.8488 0.5719 0.5960 RMSE:0.5249 1.0092 0.8252 Tau:0.7421 0.6320 0.4361\n",
      "EarlyStopping counter: 80 out of 100\n",
      "Epoch: 209 Step: 20482 Index:-0.3683 R2:0.8601 0.5733 0.6135 RMSE:0.5036 1.0080 0.8210 Tau:0.7501 0.6397 0.4324\n",
      "EarlyStopping counter: 81 out of 100\n",
      "Epoch: 210 Step: 20580 Index:-0.3572 R2:0.8623 0.5655 0.6241 RMSE:0.5068 0.9932 0.7873 Tau:0.7515 0.6360 0.4355\n",
      "EarlyStopping counter: 82 out of 100\n",
      "Epoch: 211 Step: 20678 Index:-0.3982 R2:0.8605 0.5472 0.6174 RMSE:0.5059 1.0265 0.8001 Tau:0.7515 0.6283 0.4354\n",
      "EarlyStopping counter: 83 out of 100\n",
      "Epoch: 212 Step: 20776 Index:-0.3910 R2:0.8601 0.5557 0.6194 RMSE:0.5230 1.0256 0.7894 Tau:0.7499 0.6346 0.4340\n",
      "EarlyStopping counter: 84 out of 100\n",
      "Epoch: 213 Step: 20874 Index:-0.3611 R2:0.8519 0.5730 0.6235 RMSE:0.5218 0.9926 0.7973 Tau:0.7413 0.6316 0.4369\n",
      "EarlyStopping counter: 85 out of 100\n",
      "Epoch: 214 Step: 20972 Index:-0.4944 R2:0.8361 0.5659 0.6352 RMSE:0.5884 1.1107 0.8390 Tau:0.7274 0.6163 0.4328\n",
      "EarlyStopping counter: 86 out of 100\n",
      "Epoch: 215 Step: 21070 Index:-0.3904 R2:0.8340 0.5812 0.5689 RMSE:0.5640 1.0094 0.8926 Tau:0.7270 0.6190 0.4241\n",
      "EarlyStopping counter: 87 out of 100\n",
      "Epoch: 216 Step: 21168 Index:-0.4072 R2:0.8501 0.5488 0.5891 RMSE:0.5303 1.0227 0.8443 Tau:0.7398 0.6155 0.4335\n",
      "EarlyStopping counter: 88 out of 100\n",
      "Epoch: 217 Step: 21266 Index:-0.3895 R2:0.8684 0.5658 0.6277 RMSE:0.4993 1.0290 0.8001 Tau:0.7577 0.6395 0.4378\n",
      "EarlyStopping counter: 89 out of 100\n",
      "Epoch: 218 Step: 21364 Index:-0.6360 R2:0.8569 0.5329 0.6200 RMSE:0.8044 1.2651 1.0250 Tau:0.7516 0.6291 0.4423\n",
      "EarlyStopping counter: 90 out of 100\n",
      "Epoch: 219 Step: 21462 Index:-0.3818 R2:0.8629 0.5586 0.6181 RMSE:0.4979 1.0121 0.7982 Tau:0.7511 0.6303 0.4350\n",
      "EarlyStopping counter: 91 out of 100\n",
      "Epoch: 220 Step: 21560 Index:-0.5544 R2:0.8635 0.5555 0.6257 RMSE:0.6194 1.1878 0.9283 Tau:0.7533 0.6334 0.4372\n",
      "EarlyStopping counter: 92 out of 100\n",
      "Epoch: 221 Step: 21658 Index:-0.4501 R2:0.8601 0.5187 0.6160 RMSE:0.5096 1.0612 0.7950 Tau:0.7538 0.6111 0.4380\n",
      "EarlyStopping counter: 93 out of 100\n",
      "Epoch: 222 Step: 21756 Index:-0.4849 R2:0.8596 0.5365 0.6053 RMSE:0.5349 1.1071 0.8634 Tau:0.7517 0.6222 0.4410\n",
      "EarlyStopping counter: 94 out of 100\n",
      "Epoch: 223 Step: 21854 Index:-0.5053 R2:0.8587 0.5178 0.6302 RMSE:0.5266 1.1318 0.8135 Tau:0.7559 0.6265 0.4479\n",
      "EarlyStopping counter: 95 out of 100\n",
      "Epoch: 224 Step: 21952 Index:-0.5059 R2:0.8588 0.5298 0.6365 RMSE:0.5138 1.1168 0.8177 Tau:0.7505 0.6108 0.4441\n",
      "EarlyStopping counter: 96 out of 100\n",
      "Epoch: 225 Step: 22050 Index:-0.5385 R2:0.8623 0.5440 0.6186 RMSE:0.5203 1.1638 0.8821 Tau:0.7549 0.6253 0.4434\n",
      "EarlyStopping counter: 97 out of 100\n",
      "Epoch: 226 Step: 22148 Index:-0.4231 R2:0.8703 0.5527 0.6232 RMSE:0.4889 1.0652 0.8088 Tau:0.7605 0.6421 0.4413\n",
      "EarlyStopping counter: 98 out of 100\n",
      "Epoch: 227 Step: 22246 Index:-0.5006 R2:0.8605 0.5506 0.6024 RMSE:0.5766 1.1386 0.8658 Tau:0.7526 0.6380 0.4371\n",
      "EarlyStopping counter: 99 out of 100\n",
      "Epoch: 228 Step: 22344 Index:-0.4797 R2:0.8480 0.5691 0.5868 RMSE:0.5590 1.1149 0.8870 Tau:0.7356 0.6352 0.4384\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Epoch: 229 Step: 22442 Index:-0.4470 R2:0.8675 0.5437 0.5905 RMSE:0.4888 1.0763 0.8383 Tau:0.7590 0.6293 0.4397\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_one_hot', one_hot_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_interger', interger_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_binary', binary_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = - val_MSE**0.5 + val_tau \n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 229 r2:0.5926 RMSE:0.8366 WTI:0.3677 AP:0.5464 Tau:0.4051 \n",
      " \n",
      " Top-1:0.1111 Top-1-fp:0.0000 \n",
      " Top-5:0.5918 Top-5-fp:0.1633 \n",
      " Top-10:0.6633 Top-10-fp:0.2857 \n",
      " Top-15:0.6054 Top-15-fp:0.3946 \n",
      " Top-20:0.5833 Top-20-fp:0.4643 \n",
      " Top-25:0.6500 Top-25-fp:0.5224 \n",
      " Top-30:0.7056 Top-30-fp:0.5680 \n",
      " Top-40:0.8500 Top-40-fp:0.6097 \n",
      " Top-50:0.9222 Top-50-fp:0.6619 \n",
      " \n",
      " Top50:0.5800 Top50-fp:0.1800 \n",
      " Top100:0.6700 Top100-fp:0.2800 \n",
      " Top150:0.6000 Top150-fp:0.4000 \n",
      " Top200:0.5833 Top200-fp:0.4750 \n",
      " Top250:0.6556 Top250-fp:0.5280 \n",
      " Top300:0.7389 Top300-fp:0.5567 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = loss_function(mol_prediction,y)\n",
    "#             loss.backward(retain_graph=True)\n",
    "#             optimizer_AFSE.zero_grad()\n",
    "#             punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "\n",
    "# loss =  regression_loss + vat_loss + test_vat_loss\n",
    "\n",
    "#         init_lr = 1e-4\n",
    "#         max_lr = 10**-(init_lr-1)\n",
    "#         conv_lr = conv_lr - conv_lr**2 + 0.1 * punish_lr\n",
    "#         if conv_lr < max_lr:\n",
    "#             for param_group in optimizer_AFSE.param_groups:\n",
    "#                 param_group[\"lr\"] = conv_lr.detach()\n",
    "#                 AFSE_lr = conv_lr    \n",
    "#         else:\n",
    "#             for param_group in optimizer_AFSE.param_groups:\n",
    "#                 param_group[\"lr\"] = max_lr\n",
    "#                 AFSE_lr = max_lr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
