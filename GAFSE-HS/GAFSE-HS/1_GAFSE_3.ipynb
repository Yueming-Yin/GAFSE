{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "import math\n",
    "torch.manual_seed(8)\n",
    "import time\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "import pickle\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "# from tensorboardX import SummaryWriter\n",
    "torch.nn.Module.dump_patches = True\n",
    "import copy\n",
    "import pandas as pd\n",
    "#then import my own modules\n",
    "from AttentiveFP.AttentiveLayers_Sim_copy import Fingerprint, GRN, AFSE\n",
    "from AttentiveFP import Fingerprint_viz, save_smiles_dicts, get_smiles_dicts, get_smiles_array, moltosvg_highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "# from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit.Chem import rdMolDescriptors, MolSurf\n",
    "from rdkit.Chem.Draw import SimilarityMaps\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "%matplotlib inline\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "import seaborn as sns; sns.set()\n",
    "from IPython.display import SVG, display\n",
    "import sascorer\n",
    "from AttentiveFP.utils import EarlyStopping\n",
    "from AttentiveFP.utils import Meter\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import AttentiveFP.Featurizer\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC50_P28564_1_80\n",
      "model_file/1_GAFSE_IC50_P28564_1_80_run_0\n"
     ]
    }
   ],
   "source": [
    "train_filename = \"./data/benchmark/IC50_P28564_1_80_train.csv\"\n",
    "test_filename = \"./data/benchmark/IC50_P28564_1_80_test.csv\"\n",
    "test_active = 80\n",
    "val_rate = 0.2\n",
    "random_seed = 68\n",
    "file_list1 = train_filename.split('/')\n",
    "file1 = file_list1[-1]\n",
    "file1 = file1[:-10]\n",
    "number = '_run_0'\n",
    "model_file = \"model_file/1_GAFSE_\"+file1+number\n",
    "log_dir = f'log/{\"1_GAFSE_\"+file1}'+number\n",
    "result_dir = './result/1_GAFSE_'+file1+number\n",
    "print(file1)\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              smiles     value\n",
      "0         CN1CCN(CC1)C2CC3=CC=CC=C3SC4=C2C=C(C=C4)SC -2.260036\n",
      "1  CC1=C(C=CC(=C1)C2=NOC(=N2)C)C3=CC=C(C=C3)C(=O)... -0.322219\n",
      "2  CN1CCN(CC1)C2=C(C=CC(=C2)NC(=O)N3CCC(CC3)C4=CN... -1.698970\n",
      "3    CN1C2=C(C=C(C=C2)CCN3CCN(CC3)C4=CC=CC=C4O)SC1=O -1.491362\n",
      "4  C1CN(CCN1CC2=CC3=C(C=C2)OCO3)C4=NC5=C(C=CS5)N6... -3.850000\n",
      "number of all smiles:  99\n",
      "number of successfully processed smiles:  99\n",
      "                                              smiles     value  \\\n",
      "0         CN1CCN(CC1)C2CC3=CC=CC=C3SC4=C2C=C(C=C4)SC -2.260036   \n",
      "1  CC1=C(C=CC(=C1)C2=NOC(=N2)C)C3=CC=C(C=C3)C(=O)... -0.322219   \n",
      "2  CN1CCN(CC1)C2=C(C=CC(=C2)NC(=O)N3CCC(CC3)C4=CN... -1.698970   \n",
      "3    CN1C2=C(C=C(C=C2)CCN3CCN(CC3)C4=CC=CC=C4O)SC1=O -1.491362   \n",
      "4  C1CN(CCN1CC2=CC3=C(C=C2)OCO3)C4=NC5=C(C=CS5)N6... -3.850000   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0             CSc1ccc2c(c1)C(N1CCN(C)CC1)Cc1ccccc1S2  \n",
      "1  Cc1nc(-c2ccc(-c3ccc(C(=O)Nc4ccc5c(c4)C4(CCN(C)...  \n",
      "2  COc1ccc(NC(=O)N2CCC(c3c[nH]c4ccccc34)CC2)cc1N1...  \n",
      "3          Cn1c(=O)sc2cc(CCN3CCN(c4ccccc4O)CC3)ccc21  \n",
      "4     c1cc2c(N3CCN(Cc4ccc5c(c4)OCO5)CC3)nc3sccc3n2c1  \n"
     ]
    }
   ],
   "source": [
    "# task_name = 'Malaria Bioactivity'\n",
    "tasks = ['value']\n",
    "\n",
    "# train_filename = \"../data/active_inactive/median_active/EC50/Q99500.csv\"\n",
    "feature_filename = train_filename.replace('.csv','.pickle')\n",
    "filename = train_filename.replace('.csv','')\n",
    "prefix_filename = train_filename.split('/')[-1].replace('.csv','')\n",
    "train_df = pd.read_csv(train_filename, header=0, names = [\"smiles\",\"value\"],usecols=[0,1])\n",
    "# train_df = train_df[1:]\n",
    "# train_df = train_df.drop(0,axis=1,inplace=False) \n",
    "print(train_df[:5])\n",
    "# print(train_df.iloc(1))\n",
    "def add_canonical_smiles(train_df):\n",
    "    smilesList = train_df.smiles.values\n",
    "    print(\"number of all smiles: \",len(smilesList))\n",
    "    atom_num_dist = []\n",
    "    remained_smiles = []\n",
    "    canonical_smiles_list = []\n",
    "    for smiles in smilesList:\n",
    "        try:        \n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            atom_num_dist.append(len(mol.GetAtoms()))\n",
    "            remained_smiles.append(smiles)\n",
    "            canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "        except:\n",
    "            print(smiles)\n",
    "            pass\n",
    "    print(\"number of successfully processed smiles: \", len(remained_smiles))\n",
    "    train_df = train_df[train_df[\"smiles\"].isin(remained_smiles)]\n",
    "    train_df['cano_smiles'] =canonical_smiles_list\n",
    "    return train_df\n",
    "# print(train_df)\n",
    "train_df = add_canonical_smiles(train_df)\n",
    "\n",
    "print(train_df.head())\n",
    "# plt.figure(figsize=(5, 3))\n",
    "# sns.set(font_scale=1.5)\n",
    "# ax = sns.distplot(atom_num_dist, bins=28, kde=False)\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig(\"atom_num_dist_\"+prefix_filename+\".png\",dpi=200)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = str(time.ctime()).replace(':','-').replace(' ','_')\n",
    "\n",
    "p_dropout= 0.03\n",
    "fingerprint_dim = 100\n",
    "\n",
    "weight_decay = 4.3 # also known as l2_regularization_lambda\n",
    "learning_rate = 4\n",
    "radius = 2 # default: 2\n",
    "T = 1\n",
    "per_task_output_units_num = 1 # for regression model\n",
    "output_units_num = len(tasks) * per_task_output_units_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all smiles:  195\n",
      "number of successfully processed smiles:  195\n",
      "(195, 3)\n",
      "                                              smiles     value  \\\n",
      "0  CN1CCN(CC1)C2=C(C=CC(=C2)NC(=O)N3CCC(CC3)C4=CN... -1.903090   \n",
      "1  CCC(CO)NC(=O)C1CN(C2CC3=CN(C4=CC=CC(=C34)C2=C1... -2.247973   \n",
      "2  CC(C)(C)C1=CC=C(C=C1)C(=O)CCCN2CCC(CC2)OC(C3=C... -3.364363   \n",
      "3  CN1C2=C(C=C(C=C2)CCN3CCN(CC3)C4=CC=CC(=C4)C(F)... -1.845098   \n",
      "4       CCOC(=O)CN1CCN(CC1)C2=NC3=C(N4C2=CC=C4)SC=C3 -2.829998   \n",
      "\n",
      "                                         cano_smiles  \n",
      "0  COc1ccc(NC(=O)N2CCC(c3c[nH]c4cc(F)ccc34)CC2)cc...  \n",
      "1       CCC(CO)NC(=O)C1C=C2c3cccc4c3c(cn4C)CC2N(C)C1  \n",
      "2  CC(C)(C)c1ccc(C(=O)CCCN2CCC(OC(c3ccccc3)c3cccc...  \n",
      "3  Cn1c(=O)sc2cc(CCN3CCN(c4cccc(C(F)(F)F)c4)CC3)c...  \n",
      "4              CCOC(=O)CN1CCN(c2nc3ccsc3n3cccc23)CC1  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_filename,header=0,names=[\"smiles\",\"value\"],usecols=[0,1])\n",
    "test_df = add_canonical_smiles(test_df)\n",
    "for l in test_df[\"cano_smiles\"]:\n",
    "    if l in train_df[\"cano_smiles\"]:\n",
    "        print(\"same smiles:\",l)\n",
    "        \n",
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/benchmark/IC50_P28564_1_80_train.pickle\n",
      "./data/benchmark/IC50_P28564_1_80_train\n",
      "294\n",
      "feature dicts file saved as ./data/benchmark/IC50_P28564_1_80_train.pickle\n"
     ]
    }
   ],
   "source": [
    "print(feature_filename)\n",
    "print(filename)\n",
    "total_df = pd.concat([train_df,test_df],axis=0)\n",
    "total_smilesList = total_df['smiles'].values\n",
    "print(len(total_smilesList))\n",
    "# if os.path.isfile(feature_filename):\n",
    "#     feature_dicts = pickle.load(open(feature_filename, \"rb\" ))\n",
    "# else:\n",
    "#     feature_dicts = save_smiles_dicts(smilesList,filename)\n",
    "feature_dicts = save_smiles_dicts(total_smilesList,filename)\n",
    "remained_df = total_df[total_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "uncovered_df = total_df.drop(remained_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 3) (20, 3) (195, 3)\n"
     ]
    }
   ],
   "source": [
    "val_df = train_df.sample(frac=val_rate,random_state=random_seed)\n",
    "train_df = train_df.drop(val_df.index)\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "train_df = train_df[train_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df[val_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df[test_df[\"cano_smiles\"].isin(feature_dicts['smiles_to_atom_mask'].keys())]\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(train_df.shape,val_df.shape,test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 101, 6)\n",
      "[[[  2. 100. 100. 100. 100. 100.]\n",
      "  [ 21. 100. 100. 100. 100. 100.]\n",
      "  [  0.  16. 100. 100. 100. 100.]\n",
      "  [ 16.   4. 100. 100. 100. 100.]\n",
      "  [  3.  17. 100. 100. 100. 100.]\n",
      "  [ 18.  16. 100. 100. 100. 100.]\n",
      "  [ 20.   7. 100. 100. 100. 100.]\n",
      "  [  6.  21. 100. 100. 100. 100.]\n",
      "  [ 21.   9. 100. 100. 100. 100.]\n",
      "  [  8.  20. 100. 100. 100. 100.]\n",
      "  [ 19.  22. 100. 100. 100. 100.]\n",
      "  [ 22.  12. 100. 100. 100. 100.]\n",
      "  [ 11.  13. 100. 100. 100. 100.]\n",
      "  [ 12.  14. 100. 100. 100. 100.]\n",
      "  [ 13.  23. 100. 100. 100. 100.]\n",
      "  [ 23.  17. 100. 100. 100. 100.]\n",
      "  [  2.   3.   5. 100. 100. 100.]\n",
      "  [  4.  18.  15. 100. 100. 100.]\n",
      "  [ 17.   5.  19. 100. 100. 100.]\n",
      "  [ 18.  20.  10. 100. 100. 100.]\n",
      "  [ 19.   6.   9. 100. 100. 100.]\n",
      "  [  7.   1.   8. 100. 100. 100.]\n",
      "  [ 10.  11.  23. 100. 100. 100.]\n",
      "  [ 14.  15.  22. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]\n",
      "  [100. 100. 100. 100. 100. 100.]]]\n"
     ]
    }
   ],
   "source": [
    "x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array([total_df[\"cano_smiles\"].values[0]],feature_dicts)\n",
    "print(x_atom_index.shape)\n",
    "print(x_atom_index)\n",
    "\n",
    "num_atom_features = x_atom.shape[-1]\n",
    "num_bond_features = x_bonds.shape[-1]\n",
    "loss_function = nn.MSELoss()\n",
    "model = Fingerprint(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, output_units_num, p_dropout)\n",
    "amodel = AFSE(fingerprint_dim, output_units_num, p_dropout)\n",
    "gmodel = GRN(radius, T, num_atom_features, num_bond_features,\n",
    "            fingerprint_dim, p_dropout)\n",
    "model.cuda()\n",
    "amodel.cuda()\n",
    "gmodel.cuda()\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "# {'params': model.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# {'params': gmodel.parameters(), 'lr': 10**(-learning_rate), 'weight_decay ': 10**-weight_decay}, \n",
    "# ])\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "optimizer_AFSE = optim.Adam(params=amodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# optimizer_AFSE = optim.SGD(params=amodel.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "optimizer_GRN = optim.Adam(params=gmodel.parameters(), lr=10**(-learning_rate), weight_decay=10**-weight_decay)\n",
    "\n",
    "# tensorboard = SummaryWriter(log_dir=\"runs/\"+start_time+\"_\"+prefix_filename+\"_\"+str(fingerprint_dim)+\"_\"+str(p_dropout))\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# print(params)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         print(name, param.data.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def sorted_show_pik(dataset, p, k, k_predict, i, acc):\n",
    "    p_value = dataset[tasks[0]].astype(float).tolist()\n",
    "    x = np.arange(0,len(dataset),1)\n",
    "#     print('plt',dataset.head(),p[:10],k_predict,k)\n",
    "#     plt.figure()\n",
    "#     fig, ax1 = plt.subplots()\n",
    "#     ax1.grid(False)\n",
    "#     ax2 = ax1.twinx()\n",
    "#     plt.grid(False)\n",
    "    plt.scatter(x,p,marker='.',s=6,color='r',label='predict')\n",
    "#     plt.ylabel('predict')\n",
    "    plt.scatter(x,p_value,s=6,marker=',',color='blue',label='p_value')\n",
    "    plt.axvline(x=k-1,ls=\"-\",c=\"black\")#添加垂直直线\n",
    "    k_value = np.ones(len(dataset))\n",
    "# #     print(EC50[k-1])\n",
    "    k_value = k_value*k_predict\n",
    "    plt.plot(x,k_value,'-',color='black')\n",
    "    plt.ylabel('p_value')\n",
    "    plt.title(\"epoch: {},  top-k recall: {}\".format(i,acc))\n",
    "    plt.legend(loc=3,fontsize=5)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def topk_acc2(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "    \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    k_true = true_sort[tasks[0]].values[k-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('k_true: ',type(k_true),k_true)\n",
    "#     print('k_true: ',k_true,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=k_true,'acc_num: ',len(df3[df3['predict'].values>=k_true]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k)\n",
    "    acc = len(df3[df3[tasks[0]].values>=k_true])/k #预测值前k个中真实排在前k个的个数/k\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/k\n",
    "    if k>active_num:\n",
    "        min_active = true_sort[tasks[0]].values[active_num-1]\n",
    "        acc = len(df3[df3[tasks[0]].values>=min_active])/k\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "def topk_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    df['predict'] = predict\n",
    "    df2 = df.sort_values(by='predict',ascending=False) # 拼接预测值后对预测值进行排序\n",
    "#     print('df2:\\n',df2)\n",
    "        \n",
    "    df3 = df2[:k]  #取按预测值排完序后的前k个，因为后面的全是-4.1\n",
    "    \n",
    "    true_sort = df.sort_values(by=tasks[0],ascending=False) #返回一个新的按真实值排序列表\n",
    "    min_active = true_sort[tasks[0]].values[active_num-1]  # 真实排第k个的活性值\n",
    "#     print('df3:\\n',df3['predict'])\n",
    "#     print('min_active: ',type(min_active),min_active)\n",
    "#     print('min_active: ',min_active,'min_predict: ',df3['predict'].values[-1],'index: ',df3['predict'].values>=min_active,'acc_num: ',len(df3[df3['predict'].values>=min_active]),\n",
    "#           'fp_num: ',len(df3[df3['predict'].values>=-4.1]),'k: ',k,'active_num: ',active_num)\n",
    "    acc = len(df3[df3[tasks[0]].values>-4.1])/active_num #预测值前k个中真实排在前active_num个的个数/active_num\n",
    "    fp = len(df3[df3[tasks[0]].values==-4.1])/k  #预测值前k个中为-4.1的个数/active_num\n",
    "    \n",
    "    if(show_flag):\n",
    "        #进来的是按实际活性值排好序的\n",
    "        sorted_show_pik(true_sort,true_sort['predict'],k,k_predict,i,acc)\n",
    "    return acc,fp\n",
    "\n",
    "    \n",
    "def topk_acc_recall(df, predict, k, active_num, show_flag=False, i=0):\n",
    "    if k>active_num:\n",
    "        return topk_recall(df, predict, k, active_num, show_flag, i)\n",
    "    return topk_acc2(df,predict,k, active_num,show_flag,i)\n",
    "\n",
    "def weighted_top_index(df, predict, active_num):\n",
    "    weighted_acc_list=[]\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        acc, fp = topk_acc_recall(df, predict, k, active_num)\n",
    "        weight = (len(df)-k)/len(df)\n",
    "#         print('weight=',weight,'acc=',acc)\n",
    "        weighted_acc_list.append(acc*weight)#\n",
    "    weighted_acc_list = np.array(weighted_acc_list)\n",
    "#     print('weighted_acc_list=',weighted_acc_list)\n",
    "    return np.sum(weighted_acc_list)/weighted_acc_list.shape[0]\n",
    "\n",
    "def AP(df, predict, active_num):\n",
    "    prec = []\n",
    "    rec = []\n",
    "    for k in np.arange(1,len(df)+1,1):\n",
    "        prec_k, fp1 = topk_acc2(df,predict,k, active_num)\n",
    "        rec_k, fp2 = topk_recall(df, predict, k, active_num)\n",
    "        prec.append(prec_k)\n",
    "        rec.append(rec_k)\n",
    "    # 取所有不同的recall对应的点处的精度值做平均\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # 计算包络线，从后往前取最大保证precise非减\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # 找出所有检测结果中recall不同的点\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "#     print(prec)\n",
    "#     print('prec='+str(prec)+'\\n\\n'+'rec='+str(rec))\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    # 用recall的间隔对精度作加权平均\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_r2(y,predict):\n",
    "#     print(y)\n",
    "#     print(predict)\n",
    "    y = torch.FloatTensor(y).reshape(-1,1)\n",
    "    predict = torch.FloatTensor(predict).reshape(-1,1)\n",
    "    y_mean = torch.mean(y)\n",
    "    predict_mean = torch.mean(predict)\n",
    "    \n",
    "    y1 = torch.pow(torch.mm((y-y_mean).t(),(predict-predict_mean)),2)\n",
    "    y2 = torch.mm((y-y_mean).t(),(y-y_mean))*torch.mm((predict-predict_mean).t(),(predict-predict_mean))\n",
    "#     print(y1,y2)\n",
    "    return y1/y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def l2_norm(input, dim):\n",
    "    norm = torch.norm(input, dim=dim, keepdim=True)\n",
    "    output = torch.div(input, norm+1e-6)\n",
    "    return output\n",
    "\n",
    "def normalize_perturbation(d,dim=-1):\n",
    "    output = l2_norm(d, dim)\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    return (torch.exp(x)-torch.exp(-x))/(torch.exp(x)+torch.exp(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "def perturb_feature(f, model, alpha=1, lamda=10**-learning_rate, output_lr=False, output_plr=False, y=None):\n",
    "    mol_prediction = model(feature=f, d=0)\n",
    "    pred = mol_prediction.detach()\n",
    "#     f = torch.div(f, torch.norm(f, dim=-1, keepdim=True)+1e-9)\n",
    "    eps = 1e-6 * normalize_perturbation(torch.randn(f.shape))\n",
    "    eps = Variable(eps, requires_grad=True)\n",
    "    # Predict on randomly perturbed image\n",
    "    eps_p = model(feature=f, d=eps.cuda())\n",
    "    eps_p_ = model(feature=f, d=-eps.cuda())\n",
    "    p_aux = nn.Sigmoid()(eps_p/(pred+1e-6))\n",
    "    p_aux_ = nn.Sigmoid()(eps_p_/(pred+1e-6))\n",
    "#     loss = nn.BCELoss()(abs(p_aux),torch.ones_like(p_aux))+nn.BCELoss()(abs(p_aux_),torch.ones_like(p_aux_))\n",
    "    loss = loss_function(p_aux,torch.ones_like(p_aux))+loss_function(p_aux_,torch.ones_like(p_aux_))\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # Based on perturbed image, get direction of greatest error\n",
    "    eps_adv = eps.grad#/10**-learning_rate\n",
    "    optimizer_AFSE.zero_grad()\n",
    "    # Use that direction as adversarial perturbation\n",
    "    eps_adv_normed = normalize_perturbation(eps_adv)\n",
    "    d_adv = lamda * eps_adv_normed.cuda()\n",
    "    if output_lr:\n",
    "        f_p, max_lr = model(feature=f, d=d_adv, output_lr=output_lr)\n",
    "    f_p = model(feature=f, d=d_adv)\n",
    "    f_p_ = model(feature=f, d=-d_adv)\n",
    "    p = nn.Sigmoid()(f_p/(pred+1e-6))\n",
    "    p_ = nn.Sigmoid()(f_p_/(pred+1e-6))\n",
    "    vat_loss = loss_function(p,torch.ones_like(p))+loss_function(p_,torch.ones_like(p_))\n",
    "    if output_lr:\n",
    "        if output_plr:\n",
    "            loss = loss_function(mol_prediction,y)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer_AFSE.zero_grad()\n",
    "            punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "            return eps_adv, d_adv, vat_loss, mol_prediction, max_lr, punish_lr\n",
    "        return eps_adv, d_adv, vat_loss, mol_prediction, max_lr\n",
    "    return eps_adv, d_adv, vat_loss, mol_prediction\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "\n",
    "def d_loss(f, pred, model, y_val):\n",
    "    diff_loss = 0\n",
    "    length = len(pred)\n",
    "    for i in range(length):\n",
    "        for j in range(length):\n",
    "            if j == i:\n",
    "                continue\n",
    "            pred_diff = model(feature_only=True, feature1=f[i], feature2=f[j])\n",
    "            true_diff = y_val[i] - y_val[j]\n",
    "            diff_loss += loss_function(pred_diff, torch.Tensor([true_diff]).view(-1,1))\n",
    "    diff_loss = diff_loss/(length*(length-1))\n",
    "    return diff_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CE(x,y):\n",
    "    c = 0\n",
    "    l = len(y)\n",
    "    for i in range(l):\n",
    "        if y[i]==1:\n",
    "            c += 1\n",
    "    w1 = (l-c)/l\n",
    "    w0 = c/l\n",
    "    loss = -w1*y*torch.log(x+1e-6)-w0*(1-y)*torch.log(1-x+1e-6)\n",
    "    loss = loss.mean(-1)\n",
    "    return loss\n",
    "\n",
    "def weighted_CE_loss(x,y):\n",
    "    weight = 1/(y.detach().float().mean(0)+1e-9)\n",
    "    weighted_CE = nn.CrossEntropyLoss(weight=weight)\n",
    "#     atom_weights = (atom_weights-min(atom_weights))/(max(atom_weights)-min(atom_weights))\n",
    "    return weighted_CE(x, torch.argmax(y,-1))\n",
    "\n",
    "def generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list):\n",
    "    [a,b,c] = x_atom.shape\n",
    "    [d,e,f,g] = bond_neighbor.shape\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    one_hot_loss = 0\n",
    "    interger_loss = 0\n",
    "    binary_loss = 0\n",
    "    counter_i = 0\n",
    "    counter_j = 0\n",
    "    validity_mask = torch.from_numpy(validity_mask).cuda()\n",
    "    for i in range(a):\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        one_hot_loss += weighted_CE_loss(refer_atom_list[i,:l,:16], x_atom[i,:l,:16]) - \\\n",
    "                        ((validity_mask[i,:l]*torch.log(1-atom_list[i,:l,:16]+1e-6)).sum(-1)/(validity_mask[i,:l].sum(-1)+1e-9)).mean(-1).mean(-1)+\\\n",
    "                         weighted_CE_loss(atom_list[i,:l,16:22], x_atom[i,:l,16:22])+ \\\n",
    "                         weighted_CE_loss(atom_list[i,:l,24:30], x_atom[i,:l,24:30])+ \\\n",
    "                         weighted_CE_loss(atom_list[i,:l,31:36], x_atom[i,:l,31:36])\n",
    "        interger_loss += loss_function(atom_list[i,:l,23], x_atom[i,:l,23])+ \\\n",
    "                        loss_function(atom_list[i,:l,24], x_atom[i,:l,24])\n",
    "        binary_loss += CE(atom_list[i,:l,30], x_atom[i,:l,30])+ \\\n",
    "                        CE(atom_list[i,:l,36], x_atom[i,:l,36])+ \\\n",
    "                        CE(atom_list[i,:l,37], x_atom[i,:l,37])+ \\\n",
    "                        CE(atom_list[i,:l,38], x_atom[i,:l,38])\n",
    "        counter_i += 1\n",
    "        for j in range(l):\n",
    "            n = (bond_neighbor[i,j].sum(-1)!=0).sum(-1)\n",
    "            if n==0:\n",
    "                continue\n",
    "            one_hot_loss += weighted_CE_loss(bond_list[i,j,:n,:4], bond_neighbor[i,j,:n,:4])+ \\\n",
    "                             weighted_CE_loss(bond_list[i,j,:n,6:], bond_neighbor[i,j,:n,6:])\n",
    "            binary_loss += CE(bond_neighbor[i,j,:n,4], bond_list[i,j,:n,4])+ \\\n",
    "                           CE(bond_neighbor[i,j,:n,5], bond_list[i,j,:n,5])\n",
    "            counter_j += 1\n",
    "    one_hot_loss = one_hot_loss/(5*counter_i+2*counter_j)\n",
    "    interger_loss = interger_loss/(2*counter_i)\n",
    "    binary_loss = binary_loss/(4*counter_i+2*counter_j)\n",
    "    total_loss = (one_hot_loss + interger_loss + binary_loss)/3\n",
    "    return total_loss, one_hot_loss, interger_loss, binary_loss\n",
    "\n",
    "\n",
    "def train(model, amodel, gmodel, dataset, test_df, optimizer_list, loss_function, epoch):\n",
    "    model.train()\n",
    "    amodel.train()\n",
    "    gmodel.train()\n",
    "    optimizer, optimizer_AFSE, optimizer_GRN = optimizer_list\n",
    "    np.random.seed(epoch)\n",
    "    max_len = np.max([len(dataset),len(test_df)])\n",
    "    valList = np.arange(0,max_len)\n",
    "    #shuffle them\n",
    "    np.random.shuffle(valList)\n",
    "    batch_list = []\n",
    "    for i in range(0, max_len, batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "        batch_df = dataset.loc[batch%len(dataset),:]\n",
    "        batch_test = test_df.loc[batch%len(test_df),:]\n",
    "        global_step = epoch * len(batch_list) + counter\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        smiles_list_test = batch_test.cano_smiles.values\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "        \n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(smiles_list,feature_dicts)\n",
    "        \n",
    "        x_atom_test, x_bonds_test, x_atom_index_test, x_bond_index_test, x_mask_test, smiles_to_rdkit_list_test = get_smiles_array(smiles_list_test,feature_dicts)\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),\n",
    "                                                  torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),\n",
    "                                                  mol_feature=mol_feature,activated_features=activated_features.detach())\n",
    "        \n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        \n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        eps_adv, d_adv, vat_loss, mol_prediction, conv_lr, punish_lr = perturb_feature(mol_feature, amodel, alpha=1, \n",
    "                                                                                       lamda=10**-learning_rate, output_lr=True, \n",
    "                                                                                       output_plr=True, y=torch.Tensor(y_val).view(-1,1)) # 10**-learning_rate     \n",
    "        regression_loss = loss_function(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "#         atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "#                                       torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/1e-6,activated_features=activated_features.detach())\n",
    "#         success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(smiles_list, x_atom, \n",
    "#                             bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "#                                                      refer_atom_list, refer_bond_list,topn=1)\n",
    "#         reconstruction_loss, one_hot_loss, interger_loss,binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, \n",
    "#                                                                                               bond_neighbor, validity_mask, atom_list, \n",
    "#                                                                                               bond_list)\n",
    "        x_atom_test = torch.Tensor(x_atom_test)\n",
    "        x_bonds_test = torch.Tensor(x_bonds_test)\n",
    "        x_bond_index_test = torch.cuda.LongTensor(x_bond_index_test)\n",
    "        \n",
    "        bond_neighbor_test = [x_bonds_test[i][x_bond_index_test[i]] for i in range(len(batch_test))]\n",
    "        bond_neighbor_test = torch.stack(bond_neighbor_test, dim=0)\n",
    "        activated_features_test, mol_feature_test = model(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "                                                          torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),\n",
    "                                                          torch.Tensor(x_mask_test),output_activated_features=True)\n",
    "#         mol_feature_test = torch.div(mol_feature_test, torch.norm(mol_feature_test, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features_test = torch.div(activated_features_test, torch.norm(activated_features_test, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_test, d_test, test_vat_loss, mol_prediction_test = perturb_feature(mol_feature_test, amodel, \n",
    "                                                                                    alpha=1, lamda=10**-learning_rate)\n",
    "#         atom_list_test, bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),torch.cuda.LongTensor(x_atom_index_test),\n",
    "#                                                 torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "#                                                 mol_feature=mol_feature_test+d_test/1e-6,activated_features=activated_features_test.detach())\n",
    "#         refer_atom_list_test, refer_bond_list_test = gmodel(torch.Tensor(x_atom_test),torch.Tensor(x_bonds_test),\n",
    "#                                                             torch.cuda.LongTensor(x_atom_index_test),torch.cuda.LongTensor(x_bond_index_test),torch.Tensor(x_mask_test),\n",
    "#                                                             mol_feature=mol_feature_test,activated_features=activated_features_test.detach())\n",
    "#         success_smiles_batch_test, modified_smiles_test, success_batch_test, total_batch_test, reconstruction_test, validity_test, validity_mask_test = modify_atoms(smiles_list_test, x_atom_test, \n",
    "#                             bond_neighbor_test, atom_list_test, bond_list_test,smiles_list_test,smiles_to_rdkit_list_test,\n",
    "#                                                      refer_atom_list_test, refer_bond_list_test,topn=1)\n",
    "#         test_reconstruction_loss, test_one_hot_loss, test_interger_loss,test_binary_loss = generate_loss_function(atom_list_test, x_atom_test, bond_list_test, bond_neighbor_test, validity_mask_test, atom_list_test, bond_list_test)\n",
    "        \n",
    "        if vat_loss>1 or test_vat_loss>1:\n",
    "            vat_loss = 1*(vat_loss/(vat_loss+1e-6).item())\n",
    "            test_vat_loss = 1*(test_vat_loss/(test_vat_loss+1e-6).item())\n",
    "        \n",
    "        max_lr = 1e-3\n",
    "        conv_lr = conv_lr - conv_lr**2 + 0.06 * punish_lr\n",
    "        if conv_lr < max_lr and conv_lr >= 0:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = conv_lr.detach()\n",
    "                AFSE_lr = conv_lr    \n",
    "        elif conv_lr < 0:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = 0\n",
    "                AFSE_lr = 0\n",
    "        elif conv_lr >= max_lr:\n",
    "            for param_group in optimizer_AFSE.param_groups:\n",
    "                param_group[\"lr\"] = max_lr\n",
    "                AFSE_lr = max_lr\n",
    "        \n",
    "        logger.add_scalar('loss/regression', regression_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE', vat_loss, global_step)\n",
    "        logger.add_scalar('loss/AFSE_test', test_vat_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN', reconstruction_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_test', test_reconstruction_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_one_hot', one_hot_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_interger', interger_loss, global_step)\n",
    "#         logger.add_scalar('loss/GRN_binary', binary_loss, global_step)\n",
    "        logger.add_scalar('lr/max_lr', conv_lr, global_step)\n",
    "        logger.add_scalar('lr/punish_lr', punish_lr, global_step)\n",
    "        logger.add_scalar('lr/AFSE_lr', AFSE_lr, global_step)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        optimizer_AFSE.zero_grad()\n",
    "#         optimizer_GRN.zero_grad()\n",
    "        loss =  regression_loss + 0.6*(vat_loss + test_vat_loss) # + reconstruction_loss + test_reconstruction_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_AFSE.step()\n",
    "#         optimizer_GRN.step()\n",
    "\n",
    "        \n",
    "def clear_atom_map(mol):\n",
    "    [a.ClearProp('molAtomMapNumber') for a  in mol.GetAtoms()]\n",
    "    return mol\n",
    "\n",
    "def mol_with_atom_index( mol ):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for idx in range( atoms ):\n",
    "        mol.GetAtomWithIdx( idx ).SetProp( 'molAtomMapNumber', str( mol.GetAtomWithIdx( idx ).GetIdx() ) )\n",
    "    return mol\n",
    "        \n",
    "def modify_atoms(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list,refer_atom_list, refer_bond_list,topn=1,viz=False):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    refer_atom_list = refer_atom_list.cpu().detach().numpy()\n",
    "    refer_bond_list = refer_bond_list.cpu().detach().numpy()\n",
    "    atom_symbol_sorted = np.argsort(x_atom[:,:,:16], axis=-1)\n",
    "    atom_symbol_generated_sorted = np.argsort(atom_list[:,:,:16], axis=-1)\n",
    "    generate_confidence_sorted = np.sort(atom_list[:,:,:16], axis=-1)\n",
    "    modified_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    confidence_threshold = 0.001\n",
    "    validity_mask = np.zeros_like(atom_list[:,:,:16])\n",
    "    symbol_list = ['B','C','N','O','F','Si','P','S','Cl','As','Se','Br','Te','I','At','other']\n",
    "    symbol_to_rdkit = [4,6,7,8,9,14,15,16,17,33,34,35,52,53,85,0]\n",
    "    for i in range(len(atom_list)):\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        first_run_flag = True\n",
    "        l = (x_atom[i].sum(-1)!=0).sum(-1)\n",
    "        cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "        mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "        counter = 0\n",
    "        for j in range(l): \n",
    "            if mol.GetAtomWithIdx(int(smiles_to_rdkit_list[cano_smiles][j])).GetAtomicNum() == \\\n",
    "                symbol_to_rdkit[refer_atom_list[i,j,:16].argmax(-1)]:\n",
    "                counter += 1\n",
    "#             print(f'atom#{smiles_to_rdkit_list[cano_smiles][j]}(f):',{symbol_list[k]: np.around(refer_atom_list[i,j,k],3) for k in range(16)},\n",
    "#                   f'\\natom#{smiles_to_rdkit_list[cano_smiles][j]}(f+d):',{symbol_list[k]: np.around(atom_list[i,j,k],3) for k in range(16)},\n",
    "#                  '\\n------------------------------------------------------------------------------------------------------------')\n",
    "#         print('预测为每个原子的平均概率：\\n',np.around(atom_list[i,:l,:16].mean(1),2))\n",
    "#         print('预测为每个原子的最大概率：\\n',np.around(atom_list[i,:l,:16].max(1),2))\n",
    "        if counter == l:\n",
    "            success_reconstruction += 1\n",
    "        while not flag==topn:\n",
    "            if rank == 16:\n",
    "                rank = 0\n",
    "                top_idx += 1\n",
    "            if top_idx == l:\n",
    "#                 print('没有满足条件的分子生成。')\n",
    "                flag += 1\n",
    "                continue\n",
    "#             if np.sum((atom_symbol_sorted[i,:l,-1]!=atom_symbol_generated_sorted[i,:l,-1-rank]).astype(int))==0:\n",
    "#                 print(f'根据预测的第{rank}大概率的原子构成的分子与原分子一致，原子位重置为0，生成下一个元素……')\n",
    "#                 rank += 1\n",
    "#                 top_idx = 0\n",
    "#                 generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "#                                              x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "#             print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            if rank == 0:\n",
    "                generate_index = np.argsort((atom_list[i,:l,:16]-refer_atom_list[i,:l,:16] -\\\n",
    "                                             x_atom[i,:l,:16]).max(-1))[-1-top_idx]\n",
    "            atom_symbol_generated = np.argsort(atom_list[i,generate_index,:16]-\\\n",
    "                                                    refer_atom_list[i,generate_index,:16] -\\\n",
    "                                                    x_atom[i,generate_index,:16])[-1-rank]\n",
    "            if atom_symbol_generated==x_atom[i,generate_index,:16].argmax(-1):\n",
    "#                 print('生成了相同元素，生成下一个元素……')\n",
    "                rank += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            if np.sort(atom_list[i,generate_index,:16]-\\\n",
    "                refer_atom_list[i,generate_index,:16] -\\\n",
    "                x_atom[i,generate_index,:16])[-1-rank]<confidence_threshold:\n",
    "#                 print(f'原子位{generate_rdkit_index}生成{symbol_list[atom_symbol_generated]}元素的置信度小于{confidence_threshold}，寻找下一个原子位……')\n",
    "                top_idx += 1\n",
    "                rank = 0\n",
    "                continue\n",
    "#             if symbol_to_rdkit[atom_symbol_generated]==6:\n",
    "#                 print('生成了不推荐的C元素')\n",
    "#                 rank += 1\n",
    "#                 continue\n",
    "            mol.GetAtomWithIdx(int(generate_rdkit_index)).SetAtomicNum(symbol_to_rdkit[atom_symbol_generated])\n",
    "            print_mol = mol\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                if first_run_flag == True:\n",
    "                    success_validity += 1\n",
    "                total[flag] += 1\n",
    "                if Chem.MolToSmiles(clear_atom_map(print_mol))==y_smiles[i]:\n",
    "                    success[flag] +=1\n",
    "#                     print('Congratulations!', success, total)\n",
    "                    success_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "#                 print(\"修改前的分子：\", smiles[i])\n",
    "#                 display(mol_init)\n",
    "                modified_smiles.append(Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 print(f\"将第{generate_rdkit_index}个原子修改为{symbol_list[atom_symbol_generated]}的分子：\", Chem.MolToSmiles(clear_atom_map(print_mol)))\n",
    "#                 display(mol_with_atom_index(mol))\n",
    "                mol_y = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "#                 print(\"高活性分子：\", y_smiles[i])\n",
    "#                 display(mol_y)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "#                 print(f\"第{generate_rdkit_index}个原子符号修改为{symbol_list[atom_symbol_generated]}不符合规范，生成下一个元素……\")\n",
    "                validity_mask[i,generate_index,atom_symbol_generated] = 1\n",
    "                rank += 1\n",
    "                first_run_flag = False\n",
    "    return success_smiles, modified_smiles, success, total, success_reconstruction, success_validity, validity_mask\n",
    "\n",
    "def modify_bonds(smiles, x_atom, bond_neighbor, atom_list, bond_list, y_smiles, smiles_to_rdkit_list):\n",
    "    x_atom = x_atom.cpu().detach().numpy()\n",
    "    bond_neighbor = bond_neighbor.cpu().detach().numpy()\n",
    "    atom_list = atom_list.cpu().detach().numpy()\n",
    "    bond_list = bond_list.cpu().detach().numpy()\n",
    "    modified_smiles = []\n",
    "    for i in range(len(bond_neighbor)):\n",
    "        l = (bond_neighbor[i].sum(-1).sum(-1)!=0).sum(-1)\n",
    "        bond_type_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        bond_type_generated_sorted = np.argsort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        generate_confidence_sorted = np.sort(bond_list[i,:l,:,:4], axis=-1)\n",
    "        rank = 0\n",
    "        top_idx = 0\n",
    "        flag = 0\n",
    "        while not flag==3:\n",
    "            cano_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(smiles[i]))\n",
    "            if np.sum((bond_type_sorted[i,:,-1]!=bond_type_generated_sorted[:,:,-1-rank]).astype(int))==0:\n",
    "                rank += 1\n",
    "                top_idx = 0\n",
    "            print('i:',i,'top_idx:', top_idx, 'rank:',rank)\n",
    "            bond_type = bond_type_sorted[i,:,-1]\n",
    "            bond_type_generated = bond_type_generated_sorted[:,:,-1-rank]\n",
    "            generate_confidence = generate_confidence_sorted[:,:,-1-rank]\n",
    "#             print(np.sort(generate_confidence + \\\n",
    "#                                     (atom_symbol!=atom_symbol_generated).astype(int), axis=-1))\n",
    "            generate_index = np.argsort(generate_confidence + \n",
    "                                (bond_type!=bond_type_generated).astype(int), axis=-1)[-1-top_idx]\n",
    "            bond_type_generated_one = bond_type_generated[generate_index]\n",
    "            mol = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "            if generate_index >= len(smiles_to_rdkit_list[cano_smiles]):\n",
    "                top_idx += 1\n",
    "                continue\n",
    "            generate_rdkit_index = smiles_to_rdkit_list[cano_smiles][generate_index]\n",
    "            mol.GetBondWithIdx(int(generate_rdkit_index)).SetBondType(bond_type_generated_one)\n",
    "            try:\n",
    "                Chem.SanitizeMol(mol)\n",
    "                mol_init = mol_with_atom_index(Chem.MolFromSmiles(smiles[i]))\n",
    "                print(\"修改前的分子：\")\n",
    "                display(mol_init)\n",
    "                modified_smiles.append(mol)\n",
    "                print(f\"将第{generate_rdkit_index}个键修改为{atom_symbol_generated}的分子：\")\n",
    "                display(mol)\n",
    "                mol = mol_with_atom_index(Chem.MolFromSmiles(y_smiles[i]))\n",
    "                print(\"高活性分子：\")\n",
    "                display(mol)\n",
    "                rank += 1\n",
    "                flag += 1\n",
    "            except:\n",
    "                print(f\"第{generate_rdkit_index}个原子符号修改为{atom_symbol_generated}不符合规范\")\n",
    "                top_idx += 1\n",
    "    return modified_smiles\n",
    "        \n",
    "def eval(model, amodel, gmodel, dataset, topn=1, output_feature=False, generate=False, modify_atom=True,return_GRN_loss=False, viz=False):\n",
    "    model.eval()\n",
    "    amodel.eval()\n",
    "    gmodel.eval()\n",
    "    predict_list = []\n",
    "    test_MSE_list = []\n",
    "    r2_list = []\n",
    "    valList = np.arange(0,dataset.shape[0])\n",
    "    batch_list = []\n",
    "    feature_list = []\n",
    "    d_list = []\n",
    "    success = [0 for i in range(topn)]\n",
    "    total = [0 for i in range(topn)]\n",
    "    generated_smiles = []\n",
    "    success_smiles = []\n",
    "    success_reconstruction = 0\n",
    "    success_validity = 0\n",
    "    reconstruction_loss, one_hot_loss, interger_loss, binary_loss = [0,0,0,0]\n",
    "    \n",
    "# #     取dataset中排序后的第k个\n",
    "#     sorted_dataset = dataset.sort_values(by=tasks[0],ascending=False)\n",
    "#     k_df = sorted_dataset.iloc[[k-1]]\n",
    "#     k_smiles = k_df['cano_smiles'].values\n",
    "#     k_value = k_df[tasks[0]].values.astype(float)    \n",
    "    \n",
    "    for i in range(0, dataset.shape[0], batch_size):\n",
    "        batch = valList[i:i+batch_size]\n",
    "        batch_list.append(batch) \n",
    "#     print(batch_list)\n",
    "    for counter, batch in enumerate(batch_list):\n",
    "#         print(type(batch))\n",
    "        batch_df = dataset.loc[batch,:]\n",
    "        smiles_list = batch_df.cano_smiles.values\n",
    "        matched_smiles_list = smiles_list\n",
    "#         print(batch_df)\n",
    "        y_val = batch_df[tasks[0]].values.astype(float)\n",
    "#         print(type(y_val))\n",
    "        \n",
    "        x_atom, x_bonds, x_atom_index, x_bond_index, x_mask, smiles_to_rdkit_list = get_smiles_array(matched_smiles_list,feature_dicts)\n",
    "        x_atom = torch.Tensor(x_atom)\n",
    "        x_bonds = torch.Tensor(x_bonds)\n",
    "        x_bond_index = torch.cuda.LongTensor(x_bond_index)\n",
    "        bond_neighbor = [x_bonds[i][x_bond_index[i]] for i in range(len(batch_df))]\n",
    "        bond_neighbor = torch.stack(bond_neighbor, dim=0)\n",
    "        \n",
    "        lamda=10**-learning_rate\n",
    "        activated_features, mol_feature = model(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),output_activated_features=True)\n",
    "#         mol_feature = torch.div(mol_feature, torch.norm(mol_feature, dim=-1, keepdim=True)+1e-9)\n",
    "#         activated_features = torch.div(activated_features, torch.norm(activated_features, dim=-1, keepdim=True)+1e-9)\n",
    "        eps_adv, d_adv, vat_loss, mol_prediction = perturb_feature(mol_feature, amodel, alpha=1, lamda=lamda)\n",
    "#         print(mol_feature,d_adv)\n",
    "        atom_list, bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),\n",
    "                                      torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),\n",
    "                                      torch.Tensor(x_mask),mol_feature=mol_feature+d_adv/(1e-6),activated_features=activated_features)\n",
    "        refer_atom_list, refer_bond_list = gmodel(torch.Tensor(x_atom),torch.Tensor(x_bonds),torch.cuda.LongTensor(x_atom_index),torch.cuda.LongTensor(x_bond_index),torch.Tensor(x_mask),mol_feature=mol_feature,activated_features=activated_features)\n",
    "        if generate:\n",
    "            if modify_atom:\n",
    "                success_smiles_batch, modified_smiles, success_batch, total_batch, reconstruction, validity, validity_mask = modify_atoms(matched_smiles_list, x_atom, \n",
    "                            bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list,\n",
    "                                                     refer_atom_list, refer_bond_list,topn=topn,viz=viz)\n",
    "            else:\n",
    "                modified_smiles = modify_bonds(matched_smiles_list, x_atom, bond_neighbor, atom_list, bond_list,smiles_list,smiles_to_rdkit_list)\n",
    "            generated_smiles.extend(modified_smiles)\n",
    "            success_smiles.extend(success_smiles_batch)\n",
    "#             for n in range(topn):\n",
    "#                 success[n] += success_batch[n]\n",
    "#                 total[n] += total_batch[n]\n",
    "#                 print('congratulations:',success,total)\n",
    "            success_reconstruction += reconstruction\n",
    "            success_validity += validity\n",
    "            reconstruction_loss, one_hot_loss, interger_loss, binary_loss = generate_loss_function(refer_atom_list, x_atom, refer_bond_list, bond_neighbor, validity_mask, atom_list, bond_list)\n",
    "        d = d_adv.cpu().detach().numpy().tolist()\n",
    "        d_list.extend(d)\n",
    "        mol_feature_output = mol_feature.cpu().detach().numpy().tolist()\n",
    "        feature_list.extend(mol_feature_output)\n",
    "#         MAE = F.l1_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')   \n",
    "#         print(type(mol_prediction))\n",
    "        \n",
    "        MSE = F.mse_loss(mol_prediction, torch.Tensor(y_val).view(-1,1), reduction='none')\n",
    "#         r2 = caculate_r2(mol_prediction, torch.Tensor(y_val).view(-1,1))\n",
    "# #         r2_list.extend(r2.cpu().detach().numpy())\n",
    "#         if r2!=r2:\n",
    "#             r2 = torch.tensor(0)\n",
    "#         r2_list.append(r2.item())\n",
    "#         predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         print(x_mask[:2],atoms_prediction.shape, mol_prediction,MSE)\n",
    "        predict_list.extend(mol_prediction.cpu().detach().numpy())\n",
    "#         test_MAE_list.extend(MAE.data.squeeze().cpu().numpy())\n",
    "        test_MSE_list.extend(MSE.data.view(-1,1).cpu().numpy())\n",
    "#     print(r2_list)\n",
    "    if generate:\n",
    "        generated_num = len(generated_smiles)\n",
    "        eval_num = len(dataset)\n",
    "        unique = generated_num\n",
    "        novelty = generated_num\n",
    "        for i in range(generated_num):\n",
    "            for j in range(generated_num-i-1):\n",
    "                if generated_smiles[i]==generated_smiles[i+j+1]:\n",
    "                    unique -= 1\n",
    "            for k in range(eval_num):\n",
    "                if generated_smiles[i]==dataset['smiles'].values[k]:\n",
    "                    novelty -= 1\n",
    "        unique_rate = unique/(generated_num+1e-9)\n",
    "        novelty_rate = novelty/(generated_num+1e-9)\n",
    "#         print(f'successfully/total generated molecules =', {f'Top-{i+1}': f'{success[i]}/{total[i]}' for i in range(topn)})\n",
    "        return success_reconstruction/len(dataset), success_validity/len(dataset), unique_rate, novelty_rate, success_smiles, generated_smiles, caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    if return_GRN_loss:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list,reconstruction_loss, one_hot_loss, interger_loss,binary_loss\n",
    "    if output_feature:\n",
    "        return d_list, feature_list,caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "    return caculate_r2(predict_list,dataset[tasks[0]].values.astype(float).tolist()),np.array(test_MSE_list).mean(),predict_list\n",
    "\n",
    "epoch = 0\n",
    "max_epoch = 1000\n",
    "batch_size = 10\n",
    "patience = 100\n",
    "stopper = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_model.pth')\n",
    "stopper_afse = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_amodel.pth')\n",
    "stopper_generate = EarlyStopping(mode='higher', patience=patience, filename=model_file + '_gmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/1_GAFSE_IC50_P28564_1_80_run_0\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "now = datetime.datetime.now().strftime('%b%d_%H-%M-%S')\n",
    "if os.path.isdir(log_dir):\n",
    "    for files in os.listdir(log_dir):\n",
    "        os.remove(log_dir+\"/\"+files)\n",
    "    os.rmdir(log_dir)\n",
    "logger = SummaryWriter(log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Step: 19 Index:-0.6303 R2:0.0084 0.1492 0.0061 RMSE:1.1683 0.8045 1.1456 Tau:0.0677 0.1741 0.0909\n",
      "Epoch: 2 Step: 38 Index:-0.5003 R2:0.0583 0.2268 0.0465 RMSE:1.1127 0.7800 1.0781 Tau:0.1764 0.2797 0.1682\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 3 Step: 57 Index:-0.5921 R2:0.1202 0.2576 0.0811 RMSE:1.1438 0.9246 1.1107 Tau:0.2734 0.3325 0.1876\n",
      "Epoch: 4 Step: 76 Index:-0.4100 R2:0.2326 0.2832 0.1814 RMSE:1.0304 0.7636 1.0094 Tau:0.3261 0.3536 0.2275\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 5 Step: 95 Index:-0.8030 R2:0.2501 0.2714 0.1739 RMSE:1.2383 1.1565 1.2181 Tau:0.3515 0.3536 0.2026\n",
      "Epoch: 6 Step: 114 Index:-0.4013 R2:0.3401 0.2411 0.2920 RMSE:0.9758 0.7654 0.9661 Tau:0.3964 0.3641 0.2088\n",
      "Epoch: 7 Step: 133 Index:-0.4008 R2:0.3475 0.2084 0.3381 RMSE:0.9668 0.7755 0.9477 Tau:0.3847 0.3747 0.2034\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 8 Step: 152 Index:-0.5376 R2:0.3628 0.2340 0.3063 RMSE:0.9777 0.9123 0.9897 Tau:0.4127 0.3747 0.2280\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 9 Step: 171 Index:-0.5331 R2:0.3733 0.2262 0.3184 RMSE:0.9407 0.8655 0.9544 Tau:0.4225 0.3325 0.2376\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 10 Step: 190 Index:-0.4523 R2:0.3840 0.2039 0.3456 RMSE:0.9105 0.8375 0.9213 Tau:0.4173 0.3852 0.2354\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 11 Step: 209 Index:-0.6558 R2:0.3799 0.2329 0.3182 RMSE:1.0061 0.9988 1.0171 Tau:0.4225 0.3430 0.2466\n",
      "Epoch: 12 Step: 228 Index:-0.3960 R2:0.3982 0.1972 0.3594 RMSE:0.8985 0.8023 0.9001 Tau:0.4140 0.4063 0.2357\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 13 Step: 247 Index:-0.4029 R2:0.3943 0.2006 0.3549 RMSE:0.9177 0.7881 0.9175 Tau:0.4212 0.3852 0.2389\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 14 Step: 266 Index:-0.4950 R2:0.4146 0.1947 0.3697 RMSE:0.8866 0.8591 0.9045 Tau:0.4290 0.3641 0.2776\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 15 Step: 285 Index:-0.6004 R2:0.4061 0.2098 0.3567 RMSE:0.9622 0.9962 0.9719 Tau:0.4322 0.3958 0.2734\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 16 Step: 304 Index:-0.5598 R2:0.4196 0.2267 0.3526 RMSE:0.9544 0.9662 0.9697 Tau:0.4452 0.4063 0.2910\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 17 Step: 323 Index:-0.5439 R2:0.3793 0.1783 0.3566 RMSE:0.9044 0.8764 0.9046 Tau:0.4264 0.3325 0.3251\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 18 Step: 342 Index:-0.4024 R2:0.3722 0.2074 0.3501 RMSE:0.9140 0.7876 0.9123 Tau:0.4478 0.3852 0.2877\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 19 Step: 361 Index:-0.4605 R2:0.4325 0.1894 0.3747 RMSE:0.8647 0.8246 0.8840 Tau:0.4498 0.3641 0.3318\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 20 Step: 380 Index:-0.5094 R2:0.4534 0.2050 0.3843 RMSE:0.8768 0.8946 0.9053 Tau:0.4570 0.3852 0.3294\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 21 Step: 399 Index:-0.5571 R2:0.4618 0.2043 0.3955 RMSE:0.8757 0.9107 0.9034 Tau:0.4563 0.3536 0.3232\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 22 Step: 418 Index:-0.6427 R2:0.4557 0.2202 0.3903 RMSE:0.9733 1.0384 0.9874 Tau:0.4583 0.3958 0.3260\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 23 Step: 437 Index:-0.4216 R2:0.4730 0.1965 0.4023 RMSE:0.8512 0.7963 0.8742 Tau:0.4570 0.3747 0.3492\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Epoch: 24 Step: 456 Index:-0.5718 R2:0.4391 0.1458 0.3775 RMSE:1.0169 0.8726 0.9944 Tau:0.4355 0.3008 0.3489\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Epoch: 25 Step: 475 Index:-0.4161 R2:0.4795 0.1905 0.3999 RMSE:0.8696 0.7908 0.8942 Tau:0.4661 0.3747 0.3629\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Epoch: 26 Step: 494 Index:-0.4906 R2:0.4573 0.1858 0.3943 RMSE:0.8468 0.8230 0.8675 Tau:0.4544 0.3325 0.3765\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Epoch: 27 Step: 513 Index:-0.4360 R2:0.4905 0.1923 0.4163 RMSE:0.8445 0.8001 0.8674 Tau:0.4609 0.3641 0.3550\n",
      "EarlyStopping counter: 16 out of 100\n",
      "Epoch: 28 Step: 532 Index:-0.5228 R2:0.4838 0.1787 0.4147 RMSE:0.8331 0.8236 0.8515 Tau:0.4518 0.3008 0.3557\n",
      "EarlyStopping counter: 17 out of 100\n",
      "Epoch: 29 Step: 551 Index:-0.5791 R2:0.4984 0.2269 0.4183 RMSE:0.9029 0.9854 0.9299 Tau:0.4719 0.4063 0.3572\n",
      "EarlyStopping counter: 18 out of 100\n",
      "Epoch: 30 Step: 570 Index:-0.4971 R2:0.5083 0.2121 0.4237 RMSE:0.8221 0.8718 0.8591 Tau:0.4765 0.3747 0.3703\n",
      "EarlyStopping counter: 19 out of 100\n",
      "Epoch: 31 Step: 589 Index:-0.5452 R2:0.5125 0.2066 0.4264 RMSE:0.8329 0.9198 0.8708 Tau:0.4713 0.3747 0.3764\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Epoch: 32 Step: 608 Index:-0.4725 R2:0.5204 0.2019 0.4362 RMSE:0.8015 0.8155 0.8346 Tau:0.4622 0.3430 0.3741\n",
      "EarlyStopping counter: 21 out of 100\n",
      "Epoch: 33 Step: 627 Index:-0.4752 R2:0.5007 0.1740 0.4187 RMSE:0.8439 0.8287 0.8590 Tau:0.4524 0.3536 0.3785\n",
      "EarlyStopping counter: 22 out of 100\n",
      "Epoch: 34 Step: 646 Index:-0.4432 R2:0.5305 0.2173 0.4394 RMSE:0.7952 0.8390 0.8351 Tau:0.4765 0.3958 0.3915\n",
      "EarlyStopping counter: 23 out of 100\n",
      "Epoch: 35 Step: 665 Index:-0.4911 R2:0.5325 0.2219 0.4408 RMSE:0.8106 0.8869 0.8495 Tau:0.4810 0.3958 0.3925\n",
      "EarlyStopping counter: 24 out of 100\n",
      "Epoch: 36 Step: 684 Index:-0.4865 R2:0.5205 0.1779 0.4197 RMSE:0.8576 0.8295 0.8819 Tau:0.4622 0.3430 0.3997\n",
      "EarlyStopping counter: 25 out of 100\n",
      "Epoch: 37 Step: 703 Index:-0.4210 R2:0.5463 0.2154 0.4380 RMSE:0.7867 0.8695 0.8388 Tau:0.4810 0.4486 0.4080\n",
      "Epoch: 38 Step: 722 Index:-0.3938 R2:0.5510 0.2178 0.4405 RMSE:0.7707 0.8213 0.8239 Tau:0.4752 0.4274 0.4100\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 39 Step: 741 Index:-0.4583 R2:0.5462 0.1900 0.4358 RMSE:0.7889 0.8329 0.8349 Tau:0.4726 0.3747 0.4005\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 40 Step: 760 Index:-0.5017 R2:0.5224 0.2521 0.4343 RMSE:0.8897 0.9503 0.9033 Tau:0.4895 0.4486 0.4097\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 41 Step: 779 Index:-0.4809 R2:0.5655 0.2492 0.4379 RMSE:0.8543 0.9400 0.9028 Tau:0.4973 0.4591 0.4186\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 42 Step: 798 Index:-0.4387 R2:0.5635 0.2245 0.4488 RMSE:0.7681 0.8556 0.8254 Tau:0.4895 0.4169 0.4133\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 43 Step: 817 Index:-0.4796 R2:0.5682 0.2470 0.4456 RMSE:0.8072 0.9070 0.8589 Tau:0.5006 0.4274 0.4184\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 44 Step: 836 Index:-0.4086 R2:0.5709 0.2186 0.4371 RMSE:0.7586 0.8044 0.8271 Tau:0.4934 0.3958 0.4179\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 45 Step: 855 Index:-0.4045 R2:0.5705 0.2493 0.4556 RMSE:0.7706 0.8636 0.8246 Tau:0.4960 0.4591 0.4175\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 46 Step: 874 Index:-0.4472 R2:0.5813 0.2233 0.4429 RMSE:0.7405 0.8218 0.8178 Tau:0.4999 0.3747 0.4208\n",
      "Epoch: 47 Step: 893 Index:-0.3799 R2:0.5937 0.2706 0.4487 RMSE:0.7645 0.8602 0.8376 Tau:0.5090 0.4802 0.4259\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 48 Step: 912 Index:-0.4254 R2:0.5946 0.2664 0.4570 RMSE:0.7570 0.8740 0.8293 Tau:0.5012 0.4486 0.4252\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 49 Step: 931 Index:-0.3932 R2:0.5995 0.2536 0.4571 RMSE:0.7415 0.7890 0.8229 Tau:0.5045 0.3958 0.4255\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 50 Step: 950 Index:-0.4523 R2:0.5890 0.2475 0.4486 RMSE:0.7325 0.8269 0.8152 Tau:0.5071 0.3747 0.4299\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 51 Step: 969 Index:-0.4149 R2:0.6092 0.2630 0.4648 RMSE:0.7166 0.8107 0.8019 Tau:0.5129 0.3958 0.4280\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 52 Step: 988 Index:-0.4103 R2:0.6187 0.2917 0.4586 RMSE:0.7253 0.8272 0.8177 Tau:0.5260 0.4169 0.4360\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 53 Step: 1007 Index:-0.3870 R2:0.6240 0.2928 0.4573 RMSE:0.7110 0.8039 0.8112 Tau:0.5318 0.4169 0.4388\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 54 Step: 1026 Index:-0.4524 R2:0.6224 0.2498 0.4592 RMSE:0.7013 0.8270 0.8076 Tau:0.5266 0.3747 0.4373\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 55 Step: 1045 Index:-0.3965 R2:0.6310 0.3010 0.4632 RMSE:0.7062 0.8134 0.8105 Tau:0.5474 0.4169 0.4386\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 56 Step: 1064 Index:-0.4031 R2:0.6349 0.2608 0.4663 RMSE:0.7095 0.7883 0.8144 Tau:0.5396 0.3852 0.4365\n",
      "Epoch: 57 Step: 1083 Index:-0.3784 R2:0.6431 0.3066 0.4585 RMSE:0.6969 0.7953 0.8110 Tau:0.5546 0.4169 0.4440\n",
      "Epoch: 58 Step: 1102 Index:-0.3589 R2:0.6420 0.2946 0.4684 RMSE:0.6906 0.7652 0.7997 Tau:0.5474 0.4063 0.4488\n",
      "Epoch: 59 Step: 1121 Index:-0.3532 R2:0.6232 0.3084 0.4659 RMSE:0.7068 0.7807 0.8011 Tau:0.5455 0.4274 0.4428\n",
      "Epoch: 60 Step: 1140 Index:-0.3305 R2:0.6525 0.3104 0.4722 RMSE:0.6775 0.7790 0.7972 Tau:0.5520 0.4486 0.4434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 61 Step: 1159 Index:-0.4097 R2:0.6432 0.2480 0.4587 RMSE:0.7384 0.8055 0.8475 Tau:0.5494 0.3958 0.4443\n",
      "Epoch: 62 Step: 1178 Index:-0.3151 R2:0.6565 0.3091 0.4672 RMSE:0.6740 0.7636 0.8004 Tau:0.5553 0.4486 0.4429\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 63 Step: 1197 Index:-0.3426 R2:0.6602 0.3099 0.4717 RMSE:0.6726 0.8017 0.8017 Tau:0.5572 0.4591 0.4438\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 64 Step: 1216 Index:-0.3558 R2:0.6601 0.3131 0.4694 RMSE:0.6661 0.7832 0.8027 Tau:0.5624 0.4274 0.4496\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 65 Step: 1235 Index:-0.3674 R2:0.6625 0.3048 0.4762 RMSE:0.6654 0.7948 0.7960 Tau:0.5637 0.4274 0.4409\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 66 Step: 1254 Index:-0.4506 R2:0.6669 0.3313 0.4620 RMSE:0.7363 0.8992 0.8675 Tau:0.5683 0.4486 0.4514\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 67 Step: 1273 Index:-0.3507 R2:0.6517 0.2728 0.4631 RMSE:0.7404 0.7782 0.8486 Tau:0.5513 0.4274 0.4469\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 68 Step: 1292 Index:-0.3891 R2:0.6595 0.3154 0.4582 RMSE:0.6999 0.8482 0.8311 Tau:0.5767 0.4591 0.4388\n",
      "Epoch: 69 Step: 1311 Index:-0.2981 R2:0.6559 0.3045 0.4670 RMSE:0.6872 0.7466 0.8088 Tau:0.5605 0.4486 0.4490\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 70 Step: 1330 Index:-0.3702 R2:0.6590 0.3359 0.4589 RMSE:0.7112 0.8504 0.8365 Tau:0.5826 0.4802 0.4327\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 71 Step: 1349 Index:-0.3988 R2:0.6557 0.2491 0.4578 RMSE:0.7215 0.7946 0.8394 Tau:0.5670 0.3958 0.4458\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 72 Step: 1368 Index:-0.3497 R2:0.6258 0.3151 0.4318 RMSE:0.7310 0.8405 0.8452 Tau:0.5800 0.4908 0.4301\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 73 Step: 1387 Index:-0.3556 R2:0.6794 0.2726 0.4629 RMSE:0.6500 0.7936 0.8047 Tau:0.5748 0.4380 0.4487\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 74 Step: 1406 Index:-0.4125 R2:0.6833 0.2964 0.4622 RMSE:0.6892 0.8822 0.8444 Tau:0.5780 0.4697 0.4454\n",
      "Epoch: 75 Step: 1425 Index:-0.2874 R2:0.6872 0.3053 0.4685 RMSE:0.6989 0.7465 0.8346 Tau:0.5800 0.4591 0.4489\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 76 Step: 1444 Index:-0.3339 R2:0.6869 0.3256 0.4640 RMSE:0.6550 0.8035 0.8132 Tau:0.5780 0.4697 0.4432\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 77 Step: 1463 Index:-0.3275 R2:0.6877 0.2849 0.4675 RMSE:0.6545 0.7761 0.8142 Tau:0.5852 0.4486 0.4488\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 78 Step: 1482 Index:-0.3255 R2:0.6956 0.2962 0.4633 RMSE:0.6345 0.7740 0.8098 Tau:0.5884 0.4486 0.4452\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 79 Step: 1501 Index:-0.3734 R2:0.6974 0.3102 0.4574 RMSE:0.6415 0.8220 0.8265 Tau:0.6002 0.4486 0.4507\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 80 Step: 1520 Index:-0.3042 R2:0.6968 0.2987 0.4658 RMSE:0.6614 0.7527 0.8216 Tau:0.6028 0.4486 0.4490\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 81 Step: 1539 Index:-0.3719 R2:0.6975 0.3099 0.4601 RMSE:0.6510 0.8416 0.8302 Tau:0.5963 0.4697 0.4426\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 82 Step: 1558 Index:-0.3968 R2:0.7042 0.2953 0.4622 RMSE:0.6329 0.8348 0.8233 Tau:0.6002 0.4380 0.4464\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 83 Step: 1577 Index:-0.3467 R2:0.7069 0.2723 0.4624 RMSE:0.6288 0.7847 0.8110 Tau:0.6034 0.4380 0.4462\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 84 Step: 1596 Index:-0.3489 R2:0.7014 0.3072 0.4628 RMSE:0.6296 0.8186 0.8190 Tau:0.6080 0.4697 0.4479\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 85 Step: 1615 Index:-0.3216 R2:0.7108 0.3013 0.4658 RMSE:0.6212 0.7702 0.8114 Tau:0.6080 0.4486 0.4466\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 86 Step: 1634 Index:-0.3559 R2:0.7109 0.3028 0.4631 RMSE:0.6154 0.8044 0.8162 Tau:0.6060 0.4486 0.4452\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Epoch: 87 Step: 1653 Index:-0.3439 R2:0.7171 0.3145 0.4470 RMSE:0.6165 0.8030 0.8305 Tau:0.6242 0.4591 0.4481\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Epoch: 88 Step: 1672 Index:-0.3361 R2:0.7191 0.3103 0.4589 RMSE:0.6133 0.8057 0.8204 Tau:0.6203 0.4697 0.4468\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Epoch: 89 Step: 1691 Index:-0.3172 R2:0.7219 0.3207 0.4560 RMSE:0.6080 0.7869 0.8202 Tau:0.6164 0.4697 0.4488\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Epoch: 90 Step: 1710 Index:-0.3222 R2:0.7176 0.2923 0.4695 RMSE:0.6141 0.7813 0.8088 Tau:0.6158 0.4591 0.4452\n",
      "EarlyStopping counter: 16 out of 100\n",
      "Epoch: 91 Step: 1729 Index:-0.3572 R2:0.7224 0.3446 0.4526 RMSE:0.6488 0.8479 0.8504 Tau:0.6184 0.4908 0.4464\n",
      "EarlyStopping counter: 17 out of 100\n",
      "Epoch: 92 Step: 1748 Index:-0.3827 R2:0.7191 0.3404 0.4526 RMSE:0.6720 0.8735 0.8668 Tau:0.6151 0.4908 0.4490\n",
      "EarlyStopping counter: 18 out of 100\n",
      "Epoch: 93 Step: 1767 Index:-0.3238 R2:0.7291 0.2966 0.4573 RMSE:0.5978 0.7829 0.8175 Tau:0.6275 0.4591 0.4515\n",
      "EarlyStopping counter: 19 out of 100\n",
      "Epoch: 94 Step: 1786 Index:-0.4145 R2:0.7305 0.3123 0.4562 RMSE:0.6479 0.8948 0.8656 Tau:0.6262 0.4802 0.4506\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Epoch: 95 Step: 1805 Index:-0.3586 R2:0.7315 0.2850 0.4551 RMSE:0.5932 0.8177 0.8241 Tau:0.6282 0.4591 0.4459\n",
      "EarlyStopping counter: 21 out of 100\n",
      "Epoch: 96 Step: 1824 Index:-0.4087 R2:0.7360 0.3256 0.4461 RMSE:0.6721 0.9206 0.8921 Tau:0.6340 0.5119 0.4485\n",
      "EarlyStopping counter: 22 out of 100\n",
      "Epoch: 97 Step: 1843 Index:-0.3684 R2:0.7342 0.3068 0.4629 RMSE:0.5978 0.8275 0.8258 Tau:0.6249 0.4591 0.4456\n",
      "EarlyStopping counter: 23 out of 100\n",
      "Epoch: 98 Step: 1862 Index:-0.3365 R2:0.7344 0.2960 0.4553 RMSE:0.5945 0.7850 0.8165 Tau:0.6242 0.4486 0.4438\n",
      "EarlyStopping counter: 24 out of 100\n",
      "Epoch: 99 Step: 1881 Index:-0.3640 R2:0.7421 0.2968 0.4584 RMSE:0.5874 0.8125 0.8214 Tau:0.6295 0.4486 0.4469\n",
      "EarlyStopping counter: 25 out of 100\n",
      "Epoch: 100 Step: 1900 Index:-0.3657 R2:0.7425 0.3283 0.4467 RMSE:0.6258 0.8670 0.8634 Tau:0.6314 0.5013 0.4475\n",
      "EarlyStopping counter: 26 out of 100\n",
      "Epoch: 101 Step: 1919 Index:-0.3313 R2:0.7439 0.2924 0.4558 RMSE:0.5822 0.8115 0.8250 Tau:0.6366 0.4802 0.4496\n",
      "EarlyStopping counter: 27 out of 100\n",
      "Epoch: 102 Step: 1938 Index:-0.3959 R2:0.7475 0.3029 0.4432 RMSE:0.6006 0.8550 0.8558 Tau:0.6353 0.4591 0.4471\n",
      "EarlyStopping counter: 28 out of 100\n",
      "Epoch: 103 Step: 1957 Index:-0.3709 R2:0.7485 0.2655 0.4535 RMSE:0.5941 0.7878 0.8268 Tau:0.6425 0.4169 0.4442\n",
      "EarlyStopping counter: 29 out of 100\n",
      "Epoch: 104 Step: 1976 Index:-0.4522 R2:0.7416 0.3134 0.4392 RMSE:0.6722 0.9429 0.9063 Tau:0.6314 0.4908 0.4416\n",
      "EarlyStopping counter: 30 out of 100\n",
      "Epoch: 105 Step: 1995 Index:-0.3276 R2:0.7447 0.2575 0.4416 RMSE:0.6536 0.7761 0.8485 Tau:0.6327 0.4486 0.4439\n",
      "EarlyStopping counter: 31 out of 100\n",
      "Epoch: 106 Step: 2014 Index:-0.4116 R2:0.7360 0.2867 0.4446 RMSE:0.6281 0.8813 0.8650 Tau:0.6366 0.4697 0.4505\n",
      "EarlyStopping counter: 32 out of 100\n",
      "Epoch: 107 Step: 2033 Index:-0.4141 R2:0.7453 0.2938 0.4385 RMSE:0.6078 0.8627 0.8563 Tau:0.6340 0.4486 0.4468\n",
      "EarlyStopping counter: 33 out of 100\n",
      "Epoch: 108 Step: 2052 Index:-0.3654 R2:0.7499 0.2936 0.4462 RMSE:0.5733 0.8140 0.8363 Tau:0.6399 0.4486 0.4487\n",
      "EarlyStopping counter: 34 out of 100\n",
      "Epoch: 109 Step: 2071 Index:-0.3865 R2:0.7468 0.2794 0.4511 RMSE:0.5762 0.8350 0.8353 Tau:0.6327 0.4486 0.4434\n",
      "EarlyStopping counter: 35 out of 100\n",
      "Epoch: 110 Step: 2090 Index:-0.3452 R2:0.7530 0.2738 0.4546 RMSE:0.5913 0.7832 0.8339 Tau:0.6464 0.4380 0.4454\n",
      "EarlyStopping counter: 36 out of 100\n",
      "Epoch: 111 Step: 2109 Index:-0.3168 R2:0.7359 0.3124 0.4515 RMSE:0.5938 0.8181 0.8355 Tau:0.6301 0.5013 0.4358\n",
      "EarlyStopping counter: 37 out of 100\n",
      "Epoch: 112 Step: 2128 Index:-0.3463 R2:0.7381 0.2942 0.4543 RMSE:0.5846 0.8054 0.8304 Tau:0.6379 0.4591 0.4490\n",
      "EarlyStopping counter: 38 out of 100\n",
      "Epoch: 113 Step: 2147 Index:-0.3373 R2:0.7559 0.2929 0.4442 RMSE:0.5716 0.8175 0.8366 Tau:0.6418 0.4802 0.4400\n",
      "EarlyStopping counter: 39 out of 100\n",
      "Epoch: 114 Step: 2166 Index:-0.3884 R2:0.7629 0.2777 0.4597 RMSE:0.5671 0.8581 0.8396 Tau:0.6477 0.4697 0.4438\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Epoch: 115 Step: 2185 Index:-0.2952 R2:0.7687 0.2924 0.4516 RMSE:0.5601 0.7754 0.8260 Tau:0.6548 0.4802 0.4443\n",
      "EarlyStopping counter: 41 out of 100\n",
      "Epoch: 116 Step: 2204 Index:-0.4364 R2:0.7628 0.2798 0.4555 RMSE:0.5778 0.8850 0.8591 Tau:0.6503 0.4486 0.4437\n",
      "EarlyStopping counter: 42 out of 100\n",
      "Epoch: 117 Step: 2223 Index:-0.2938 R2:0.7672 0.3105 0.4510 RMSE:0.5526 0.7845 0.8303 Tau:0.6464 0.4908 0.4422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 43 out of 100\n",
      "Epoch: 118 Step: 2242 Index:-0.3860 R2:0.7704 0.3152 0.4493 RMSE:0.5928 0.8767 0.8760 Tau:0.6516 0.4908 0.4456\n",
      "EarlyStopping counter: 44 out of 100\n",
      "Epoch: 119 Step: 2261 Index:-0.4264 R2:0.7584 0.3212 0.4286 RMSE:0.6369 0.9066 0.9046 Tau:0.6529 0.4802 0.4344\n",
      "EarlyStopping counter: 45 out of 100\n",
      "Epoch: 120 Step: 2280 Index:-0.3433 R2:0.7723 0.3058 0.4401 RMSE:0.5613 0.8235 0.8496 Tau:0.6529 0.4802 0.4441\n",
      "EarlyStopping counter: 46 out of 100\n",
      "Epoch: 121 Step: 2299 Index:-0.3368 R2:0.7747 0.3100 0.4471 RMSE:0.5516 0.8382 0.8548 Tau:0.6509 0.5013 0.4415\n",
      "EarlyStopping counter: 47 out of 100\n",
      "Epoch: 122 Step: 2318 Index:-0.3454 R2:0.7665 0.2921 0.4542 RMSE:0.5611 0.8256 0.8369 Tau:0.6483 0.4802 0.4401\n",
      "Epoch: 123 Step: 2337 Index:-0.2846 R2:0.7345 0.3059 0.4360 RMSE:0.6093 0.7437 0.8340 Tau:0.6373 0.4591 0.4441\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 124 Step: 2356 Index:-0.3896 R2:0.7694 0.3150 0.4387 RMSE:0.6093 0.9015 0.8895 Tau:0.6542 0.5119 0.4375\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 125 Step: 2375 Index:-0.4316 R2:0.7756 0.2385 0.4456 RMSE:0.7452 0.8380 0.9272 Tau:0.6705 0.4063 0.4401\n",
      "Epoch: 126 Step: 2394 Index:-0.2811 R2:0.7674 0.3239 0.4519 RMSE:0.5820 0.7824 0.8225 Tau:0.6483 0.5013 0.4494\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 127 Step: 2413 Index:-0.3439 R2:0.7752 0.3030 0.4626 RMSE:0.5524 0.8136 0.8201 Tau:0.6548 0.4697 0.4422\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 128 Step: 2432 Index:-0.3199 R2:0.7808 0.3052 0.4531 RMSE:0.5520 0.8212 0.8374 Tau:0.6640 0.5013 0.4484\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 129 Step: 2451 Index:-0.4322 R2:0.7753 0.3100 0.4436 RMSE:0.6352 0.9335 0.9107 Tau:0.6516 0.5013 0.4412\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 130 Step: 2470 Index:-0.3389 R2:0.7861 0.2731 0.4497 RMSE:0.5334 0.7980 0.8296 Tau:0.6679 0.4591 0.4445\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 131 Step: 2489 Index:-0.4024 R2:0.7871 0.2823 0.4460 RMSE:0.5419 0.8404 0.8499 Tau:0.6731 0.4380 0.4443\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 132 Step: 2508 Index:-0.3789 R2:0.7921 0.2833 0.4513 RMSE:0.5287 0.8275 0.8397 Tau:0.6737 0.4486 0.4460\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 133 Step: 2527 Index:-0.4620 R2:0.7925 0.3086 0.4428 RMSE:0.6114 0.9317 0.9214 Tau:0.6737 0.4697 0.4471\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 134 Step: 2546 Index:-0.3522 R2:0.7953 0.2868 0.4490 RMSE:0.5201 0.8007 0.8342 Tau:0.6770 0.4486 0.4451\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 135 Step: 2565 Index:-0.3441 R2:0.7934 0.2769 0.4530 RMSE:0.5427 0.7821 0.8362 Tau:0.6750 0.4380 0.4469\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 136 Step: 2584 Index:-0.4136 R2:0.7787 0.3129 0.4339 RMSE:0.5994 0.8938 0.9002 Tau:0.6698 0.4802 0.4360\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 137 Step: 2603 Index:-0.3143 R2:0.7782 0.2839 0.4548 RMSE:0.5612 0.7734 0.8348 Tau:0.6770 0.4591 0.4458\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Epoch: 138 Step: 2622 Index:-0.3144 R2:0.8008 0.2892 0.4541 RMSE:0.5271 0.7735 0.8265 Tau:0.6802 0.4591 0.4412\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Epoch: 139 Step: 2641 Index:-0.4300 R2:0.7987 0.2786 0.4576 RMSE:0.5422 0.8786 0.8641 Tau:0.6789 0.4486 0.4472\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Epoch: 140 Step: 2660 Index:-0.4411 R2:0.7958 0.2916 0.4343 RMSE:0.6049 0.9213 0.9155 Tau:0.6828 0.4802 0.4377\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Epoch: 141 Step: 2679 Index:-0.2850 R2:0.7885 0.3017 0.4250 RMSE:0.5544 0.7441 0.8430 Tau:0.6841 0.4591 0.4479\n",
      "EarlyStopping counter: 16 out of 100\n",
      "Epoch: 142 Step: 2698 Index:-0.4629 R2:0.7699 0.2895 0.4353 RMSE:0.6101 0.9220 0.8991 Tau:0.6718 0.4591 0.4319\n",
      "EarlyStopping counter: 17 out of 100\n",
      "Epoch: 143 Step: 2717 Index:-0.4227 R2:0.8019 0.2644 0.4589 RMSE:0.5151 0.8080 0.8238 Tau:0.6854 0.3852 0.4452\n",
      "EarlyStopping counter: 18 out of 100\n",
      "Epoch: 144 Step: 2736 Index:-0.3436 R2:0.8096 0.2880 0.4503 RMSE:0.5040 0.8027 0.8333 Tau:0.6913 0.4591 0.4415\n",
      "EarlyStopping counter: 19 out of 100\n",
      "Epoch: 145 Step: 2755 Index:-0.4168 R2:0.8147 0.2987 0.4553 RMSE:0.5297 0.8759 0.8733 Tau:0.6959 0.4591 0.4446\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Epoch: 146 Step: 2774 Index:-0.4393 R2:0.8040 0.3267 0.4279 RMSE:0.5939 0.9090 0.9209 Tau:0.7030 0.4697 0.4379\n",
      "EarlyStopping counter: 21 out of 100\n",
      "Epoch: 147 Step: 2793 Index:-0.3427 R2:0.8197 0.2819 0.4558 RMSE:0.5361 0.7701 0.8369 Tau:0.6952 0.4274 0.4446\n",
      "EarlyStopping counter: 22 out of 100\n",
      "Epoch: 148 Step: 2812 Index:-0.3762 R2:0.8160 0.2646 0.4547 RMSE:0.4929 0.8142 0.8323 Tau:0.6985 0.4380 0.4450\n",
      "EarlyStopping counter: 23 out of 100\n",
      "Epoch: 149 Step: 2831 Index:-0.4253 R2:0.8195 0.3134 0.4386 RMSE:0.5616 0.9055 0.9095 Tau:0.7121 0.4802 0.4429\n",
      "EarlyStopping counter: 24 out of 100\n",
      "Epoch: 150 Step: 2850 Index:-0.4295 R2:0.8239 0.3106 0.4469 RMSE:0.5735 0.9308 0.9258 Tau:0.7050 0.5013 0.4409\n",
      "EarlyStopping counter: 25 out of 100\n",
      "Epoch: 151 Step: 2869 Index:-0.4235 R2:0.8245 0.2772 0.4493 RMSE:0.4885 0.8404 0.8571 Tau:0.6965 0.4169 0.4413\n",
      "EarlyStopping counter: 26 out of 100\n",
      "Epoch: 152 Step: 2888 Index:-0.3795 R2:0.8212 0.3147 0.4178 RMSE:0.5620 0.8702 0.9118 Tau:0.7024 0.4908 0.4328\n",
      "EarlyStopping counter: 27 out of 100\n",
      "Epoch: 153 Step: 2907 Index:-0.3066 R2:0.8074 0.2847 0.4039 RMSE:0.5139 0.7762 0.8651 Tau:0.7063 0.4697 0.4332\n",
      "EarlyStopping counter: 28 out of 100\n",
      "Epoch: 154 Step: 2926 Index:-0.4840 R2:0.8206 0.2766 0.4444 RMSE:0.5054 0.8587 0.8679 Tau:0.6991 0.3747 0.4492\n",
      "EarlyStopping counter: 29 out of 100\n",
      "Epoch: 155 Step: 2945 Index:-0.4378 R2:0.8303 0.2798 0.4254 RMSE:0.5357 0.8863 0.9046 Tau:0.7069 0.4486 0.4399\n",
      "EarlyStopping counter: 30 out of 100\n",
      "Epoch: 156 Step: 2964 Index:-0.4240 R2:0.8338 0.2653 0.4395 RMSE:0.4713 0.8092 0.8458 Tau:0.7089 0.3852 0.4413\n",
      "EarlyStopping counter: 31 out of 100\n",
      "Epoch: 157 Step: 2983 Index:-0.3892 R2:0.8342 0.2655 0.4498 RMSE:0.4781 0.7956 0.8327 Tau:0.7121 0.4063 0.4420\n",
      "EarlyStopping counter: 32 out of 100\n",
      "Epoch: 158 Step: 3002 Index:-0.3596 R2:0.8283 0.2858 0.4266 RMSE:0.4798 0.8081 0.8587 Tau:0.7212 0.4486 0.4363\n",
      "EarlyStopping counter: 33 out of 100\n",
      "Epoch: 159 Step: 3021 Index:-0.5917 R2:0.8261 0.2865 0.4398 RMSE:0.5966 0.9770 0.9716 Tau:0.7076 0.3852 0.4399\n",
      "EarlyStopping counter: 34 out of 100\n",
      "Epoch: 160 Step: 3040 Index:-0.4007 R2:0.8382 0.3266 0.4371 RMSE:0.5180 0.8492 0.8908 Tau:0.7121 0.4486 0.4401\n",
      "EarlyStopping counter: 35 out of 100\n",
      "Epoch: 161 Step: 3059 Index:-0.4157 R2:0.8426 0.2922 0.4344 RMSE:0.4945 0.8642 0.8913 Tau:0.7173 0.4486 0.4374\n",
      "EarlyStopping counter: 36 out of 100\n",
      "Epoch: 162 Step: 3078 Index:-0.4680 R2:0.8463 0.2767 0.4427 RMSE:0.4984 0.8849 0.9022 Tau:0.7193 0.4169 0.4400\n",
      "EarlyStopping counter: 37 out of 100\n",
      "Epoch: 163 Step: 3097 Index:-0.4190 R2:0.8464 0.3147 0.4367 RMSE:0.5053 0.8781 0.9089 Tau:0.7193 0.4591 0.4432\n",
      "EarlyStopping counter: 38 out of 100\n",
      "Epoch: 164 Step: 3116 Index:-0.4353 R2:0.8476 0.2550 0.4486 RMSE:0.4533 0.8310 0.8512 Tau:0.7245 0.3958 0.4375\n",
      "EarlyStopping counter: 39 out of 100\n",
      "Epoch: 165 Step: 3135 Index:-0.3991 R2:0.8437 0.2987 0.4432 RMSE:0.4734 0.8159 0.8634 Tau:0.7238 0.4169 0.4415\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Epoch: 166 Step: 3154 Index:-0.5222 R2:0.8478 0.2837 0.4631 RMSE:0.5018 0.9074 0.9052 Tau:0.7232 0.3852 0.4417\n",
      "Epoch: 167 Step: 3173 Index:-0.2418 R2:0.8214 0.3451 0.4106 RMSE:0.5088 0.7431 0.8595 Tau:0.7134 0.5013 0.4310\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch: 168 Step: 3192 Index:-0.4130 R2:0.8420 0.2495 0.4637 RMSE:0.4828 0.7983 0.8235 Tau:0.7134 0.3852 0.4353\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch: 169 Step: 3211 Index:-0.3518 R2:0.8540 0.2924 0.4376 RMSE:0.4472 0.7792 0.8431 Tau:0.7291 0.4274 0.4381\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch: 170 Step: 3230 Index:-0.4454 R2:0.8508 0.2462 0.4527 RMSE:0.4763 0.7884 0.8275 Tau:0.7212 0.3430 0.4396\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch: 171 Step: 3249 Index:-0.3886 R2:0.8617 0.2998 0.4398 RMSE:0.4486 0.8160 0.8652 Tau:0.7362 0.4274 0.4314\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch: 172 Step: 3268 Index:-0.4562 R2:0.8583 0.2594 0.4520 RMSE:0.4321 0.8204 0.8493 Tau:0.7375 0.3641 0.4390\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch: 173 Step: 3287 Index:-0.5203 R2:0.8599 0.3096 0.4422 RMSE:0.5372 0.9266 0.9558 Tau:0.7349 0.4063 0.4403\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch: 174 Step: 3306 Index:-0.4951 R2:0.8510 0.2166 0.4441 RMSE:0.4422 0.8487 0.8594 Tau:0.7245 0.3536 0.4415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch: 175 Step: 3325 Index:-0.3786 R2:0.8559 0.2720 0.4330 RMSE:0.4926 0.7638 0.8448 Tau:0.7375 0.3852 0.4327\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch: 176 Step: 3344 Index:-0.4389 R2:0.8427 0.2734 0.4609 RMSE:0.4768 0.8664 0.8737 Tau:0.7206 0.4274 0.4390\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch: 177 Step: 3363 Index:-0.4915 R2:0.8530 0.2120 0.4471 RMSE:0.5274 0.8028 0.8326 Tau:0.7297 0.3113 0.4348\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch: 178 Step: 3382 Index:-0.4648 R2:0.8422 0.3228 0.4232 RMSE:0.5157 0.8711 0.9222 Tau:0.7297 0.4063 0.4346\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Epoch: 179 Step: 3401 Index:-0.5094 R2:0.8655 0.2794 0.4467 RMSE:0.4821 0.9052 0.9225 Tau:0.7408 0.3958 0.4428\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Epoch: 180 Step: 3420 Index:-0.5390 R2:0.8486 0.2123 0.4389 RMSE:0.4507 0.8293 0.8542 Tau:0.7284 0.2902 0.4390\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Epoch: 181 Step: 3439 Index:-0.4665 R2:0.8661 0.2810 0.4214 RMSE:0.4434 0.8306 0.8868 Tau:0.7447 0.3641 0.4327\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Epoch: 182 Step: 3458 Index:-0.5279 R2:0.8763 0.2669 0.4384 RMSE:0.4687 0.9132 0.9338 Tau:0.7512 0.3852 0.4373\n",
      "EarlyStopping counter: 16 out of 100\n",
      "Epoch: 183 Step: 3477 Index:-0.4750 R2:0.8757 0.2386 0.4459 RMSE:0.4048 0.8286 0.8600 Tau:0.7518 0.3536 0.4381\n",
      "EarlyStopping counter: 17 out of 100\n",
      "Epoch: 184 Step: 3496 Index:-0.4904 R2:0.8473 0.2556 0.4402 RMSE:0.4522 0.8439 0.8823 Tau:0.7440 0.3536 0.4384\n",
      "EarlyStopping counter: 18 out of 100\n",
      "Epoch: 185 Step: 3515 Index:-0.4888 R2:0.8717 0.2076 0.4417 RMSE:0.4817 0.8107 0.8394 Tau:0.7525 0.3219 0.4297\n",
      "EarlyStopping counter: 19 out of 100\n",
      "Epoch: 186 Step: 3534 Index:-0.6189 R2:0.8480 0.2173 0.4607 RMSE:0.4967 0.9725 0.9418 Tau:0.7050 0.3536 0.4137\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Epoch: 187 Step: 3553 Index:-0.3968 R2:0.8623 0.2933 0.4367 RMSE:0.4364 0.7926 0.8661 Tau:0.7401 0.3958 0.4395\n",
      "EarlyStopping counter: 21 out of 100\n",
      "Epoch: 188 Step: 3572 Index:-0.4555 R2:0.8825 0.2426 0.4330 RMSE:0.4674 0.7880 0.8499 Tau:0.7583 0.3325 0.4354\n",
      "EarlyStopping counter: 22 out of 100\n",
      "Epoch: 189 Step: 3591 Index:-0.5226 R2:0.8764 0.2677 0.4232 RMSE:0.4752 0.8973 0.9471 Tau:0.7531 0.3747 0.4278\n",
      "EarlyStopping counter: 23 out of 100\n",
      "Epoch: 190 Step: 3610 Index:-0.4489 R2:0.8812 0.2510 0.4410 RMSE:0.3969 0.8130 0.8639 Tau:0.7499 0.3641 0.4378\n",
      "EarlyStopping counter: 24 out of 100\n",
      "Epoch: 191 Step: 3629 Index:-0.4806 R2:0.8849 0.2936 0.4338 RMSE:0.4345 0.8552 0.9181 Tau:0.7681 0.3747 0.4373\n",
      "EarlyStopping counter: 25 out of 100\n",
      "Epoch: 192 Step: 3648 Index:-0.5212 R2:0.8882 0.1868 0.4211 RMSE:0.4332 0.8220 0.8588 Tau:0.7714 0.3008 0.4305\n",
      "EarlyStopping counter: 26 out of 100\n",
      "Epoch: 193 Step: 3667 Index:-0.4659 R2:0.8899 0.2672 0.4228 RMSE:0.4048 0.7772 0.8580 Tau:0.7714 0.3113 0.4352\n",
      "EarlyStopping counter: 27 out of 100\n",
      "Epoch: 194 Step: 3686 Index:-0.5761 R2:0.8937 0.2503 0.4303 RMSE:0.4297 0.8874 0.9228 Tau:0.7759 0.3113 0.4349\n",
      "EarlyStopping counter: 28 out of 100\n",
      "Epoch: 195 Step: 3705 Index:-0.5312 R2:0.8849 0.2231 0.4340 RMSE:0.4041 0.8636 0.9016 Tau:0.7603 0.3325 0.4409\n",
      "EarlyStopping counter: 29 out of 100\n",
      "Epoch: 196 Step: 3724 Index:-0.5448 R2:0.8995 0.2550 0.4297 RMSE:0.4363 0.8879 0.9362 Tau:0.7785 0.3430 0.4374\n",
      "EarlyStopping counter: 30 out of 100\n",
      "Epoch: 197 Step: 3743 Index:-0.5558 R2:0.8967 0.2324 0.4257 RMSE:0.4111 0.8777 0.9248 Tau:0.7681 0.3219 0.4388\n",
      "EarlyStopping counter: 31 out of 100\n",
      "Epoch: 198 Step: 3762 Index:-0.4886 R2:0.8952 0.2158 0.4371 RMSE:0.4605 0.8105 0.8532 Tau:0.7668 0.3219 0.4373\n",
      "EarlyStopping counter: 32 out of 100\n",
      "Epoch: 199 Step: 3781 Index:-0.5816 R2:0.8997 0.2449 0.4211 RMSE:0.4550 0.9141 0.9709 Tau:0.7668 0.3325 0.4360\n",
      "EarlyStopping counter: 33 out of 100\n",
      "Epoch: 200 Step: 3800 Index:-0.5055 R2:0.8894 0.2584 0.4513 RMSE:0.4143 0.8907 0.9174 Tau:0.7557 0.3852 0.4280\n",
      "EarlyStopping counter: 34 out of 100\n",
      "Epoch: 201 Step: 3819 Index:-0.4845 R2:0.9009 0.2364 0.4274 RMSE:0.3918 0.7958 0.8541 Tau:0.7727 0.3113 0.4298\n",
      "EarlyStopping counter: 35 out of 100\n",
      "Epoch: 202 Step: 3838 Index:-0.6615 R2:0.8483 0.1213 0.4116 RMSE:0.5231 0.9200 0.9047 Tau:0.7460 0.2586 0.4208\n",
      "EarlyStopping counter: 36 out of 100\n",
      "Epoch: 203 Step: 3857 Index:-0.4204 R2:0.8915 0.3298 0.4220 RMSE:0.4327 0.8057 0.9074 Tau:0.7655 0.3852 0.4377\n",
      "EarlyStopping counter: 37 out of 100\n",
      "Epoch: 204 Step: 3876 Index:-0.4338 R2:0.8965 0.2451 0.4332 RMSE:0.3892 0.7979 0.8560 Tau:0.7701 0.3641 0.4282\n",
      "EarlyStopping counter: 38 out of 100\n",
      "Epoch: 205 Step: 3895 Index:-0.5371 R2:0.9030 0.2612 0.4304 RMSE:0.4108 0.8695 0.9429 Tau:0.7785 0.3325 0.4386\n",
      "EarlyStopping counter: 39 out of 100\n",
      "Epoch: 206 Step: 3914 Index:-0.5483 R2:0.8849 0.1819 0.4346 RMSE:0.4576 0.8491 0.8779 Tau:0.7688 0.3008 0.4282\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Epoch: 207 Step: 3933 Index:-0.4866 R2:0.9044 0.2689 0.4330 RMSE:0.3882 0.8296 0.9104 Tau:0.7818 0.3430 0.4361\n",
      "EarlyStopping counter: 41 out of 100\n",
      "Epoch: 208 Step: 3952 Index:-0.4566 R2:0.9052 0.2351 0.4260 RMSE:0.3701 0.8312 0.8950 Tau:0.7844 0.3747 0.4323\n",
      "EarlyStopping counter: 42 out of 100\n",
      "Epoch: 209 Step: 3971 Index:-0.5018 R2:0.9046 0.2669 0.4235 RMSE:0.3834 0.8554 0.9260 Tau:0.7707 0.3536 0.4340\n",
      "EarlyStopping counter: 43 out of 100\n",
      "Epoch: 210 Step: 3990 Index:-0.4954 R2:0.9133 0.2441 0.4195 RMSE:0.3577 0.8384 0.9123 Tau:0.7941 0.3430 0.4282\n",
      "EarlyStopping counter: 44 out of 100\n",
      "Epoch: 211 Step: 4009 Index:-0.5122 R2:0.9020 0.2011 0.4186 RMSE:0.3825 0.8235 0.8797 Tau:0.7811 0.3113 0.4371\n",
      "EarlyStopping counter: 45 out of 100\n",
      "Epoch: 212 Step: 4028 Index:-0.4878 R2:0.9113 0.2175 0.4080 RMSE:0.3572 0.8202 0.8875 Tau:0.7928 0.3325 0.4260\n",
      "EarlyStopping counter: 46 out of 100\n",
      "Epoch: 213 Step: 4047 Index:-0.5898 R2:0.8711 0.2030 0.4333 RMSE:0.4207 0.9011 0.9544 Tau:0.7570 0.3113 0.4259\n",
      "EarlyStopping counter: 47 out of 100\n",
      "Epoch: 214 Step: 4066 Index:-0.6409 R2:0.8752 0.1067 0.3766 RMSE:0.4719 0.8995 0.9162 Tau:0.7681 0.2586 0.4125\n",
      "EarlyStopping counter: 48 out of 100\n",
      "Epoch: 215 Step: 4085 Index:-0.5459 R2:0.9059 0.2868 0.4355 RMSE:0.4589 0.9205 0.9889 Tau:0.7883 0.3747 0.4322\n",
      "EarlyStopping counter: 49 out of 100\n",
      "Epoch: 216 Step: 4104 Index:-0.4892 R2:0.9092 0.2333 0.4273 RMSE:0.3616 0.8217 0.8965 Tau:0.7876 0.3325 0.4396\n",
      "EarlyStopping counter: 50 out of 100\n",
      "Epoch: 217 Step: 4123 Index:-0.4713 R2:0.9151 0.2231 0.4265 RMSE:0.3433 0.8249 0.8927 Tau:0.7994 0.3536 0.4293\n",
      "EarlyStopping counter: 51 out of 100\n",
      "Epoch: 218 Step: 4142 Index:-0.5001 R2:0.9023 0.2647 0.4169 RMSE:0.3839 0.8537 0.9388 Tau:0.7889 0.3536 0.4269\n",
      "EarlyStopping counter: 52 out of 100\n",
      "Epoch: 219 Step: 4161 Index:-0.4544 R2:0.9198 0.2453 0.4259 RMSE:0.3306 0.8079 0.8900 Tau:0.8065 0.3536 0.4323\n",
      "EarlyStopping counter: 53 out of 100\n",
      "Epoch: 220 Step: 4180 Index:-0.4805 R2:0.9215 0.2520 0.4148 RMSE:0.3409 0.8235 0.9217 Tau:0.8052 0.3430 0.4331\n",
      "EarlyStopping counter: 54 out of 100\n",
      "Epoch: 221 Step: 4199 Index:-0.4479 R2:0.9171 0.2716 0.4024 RMSE:0.3560 0.8120 0.9332 Tau:0.8072 0.3641 0.4297\n",
      "EarlyStopping counter: 55 out of 100\n",
      "Epoch: 222 Step: 4218 Index:-0.5640 R2:0.9190 0.2245 0.4201 RMSE:0.3445 0.8859 0.9502 Tau:0.8059 0.3219 0.4271\n",
      "EarlyStopping counter: 56 out of 100\n",
      "Epoch: 223 Step: 4237 Index:-0.5126 R2:0.9162 0.2037 0.4047 RMSE:0.3403 0.8451 0.9315 Tau:0.7935 0.3325 0.4408\n",
      "EarlyStopping counter: 57 out of 100\n",
      "Epoch: 224 Step: 4256 Index:-0.5306 R2:0.9109 0.2675 0.4312 RMSE:0.4041 0.8947 0.9711 Tau:0.7954 0.3641 0.4369\n",
      "EarlyStopping counter: 58 out of 100\n",
      "Epoch: 225 Step: 4275 Index:-0.5192 R2:0.9200 0.2511 0.4162 RMSE:0.3778 0.8622 0.9557 Tau:0.8098 0.3430 0.4311\n",
      "EarlyStopping counter: 59 out of 100\n",
      "Epoch: 226 Step: 4294 Index:-0.5157 R2:0.9164 0.1957 0.4162 RMSE:0.3348 0.8482 0.9018 Tau:0.8033 0.3325 0.4284\n",
      "EarlyStopping counter: 60 out of 100\n",
      "Epoch: 227 Step: 4313 Index:-0.5609 R2:0.9158 0.2029 0.4240 RMSE:0.3572 0.9039 0.9656 Tau:0.7961 0.3430 0.4303\n",
      "EarlyStopping counter: 61 out of 100\n",
      "Epoch: 228 Step: 4332 Index:-0.4577 R2:0.9104 0.2375 0.4068 RMSE:0.3805 0.7796 0.8801 Tau:0.7987 0.3219 0.4429\n",
      "EarlyStopping counter: 62 out of 100\n",
      "Epoch: 229 Step: 4351 Index:-0.5090 R2:0.9096 0.2029 0.4091 RMSE:0.5421 0.8414 0.8800 Tau:0.7994 0.3325 0.4197\n",
      "EarlyStopping counter: 63 out of 100\n",
      "Epoch: 230 Step: 4370 Index:-0.6738 R2:0.9001 0.2541 0.4357 RMSE:0.5694 1.0274 1.1145 Tau:0.7824 0.3536 0.4366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 64 out of 100\n",
      "Epoch: 231 Step: 4389 Index:-0.4931 R2:0.9038 0.2301 0.3966 RMSE:0.3698 0.8150 0.9094 Tau:0.8085 0.3219 0.4251\n",
      "EarlyStopping counter: 65 out of 100\n",
      "Epoch: 232 Step: 4408 Index:-0.5883 R2:0.9146 0.1841 0.4102 RMSE:0.3531 0.8891 0.9713 Tau:0.7928 0.3008 0.4375\n",
      "EarlyStopping counter: 66 out of 100\n",
      "Epoch: 233 Step: 4427 Index:-0.4615 R2:0.9230 0.2503 0.4249 RMSE:0.3264 0.8256 0.9122 Tau:0.8104 0.3641 0.4331\n",
      "EarlyStopping counter: 67 out of 100\n",
      "Epoch: 234 Step: 4446 Index:-0.5250 R2:0.9262 0.2303 0.4115 RMSE:0.3332 0.8469 0.9479 Tau:0.8150 0.3219 0.4384\n",
      "EarlyStopping counter: 68 out of 100\n",
      "Epoch: 235 Step: 4465 Index:-0.5059 R2:0.9265 0.2077 0.4146 RMSE:0.3135 0.8384 0.9235 Tau:0.8150 0.3325 0.4374\n",
      "EarlyStopping counter: 69 out of 100\n",
      "Epoch: 236 Step: 4484 Index:-0.5356 R2:0.9141 0.1634 0.3950 RMSE:0.3622 0.8470 0.9213 Tau:0.7994 0.3113 0.4316\n",
      "EarlyStopping counter: 70 out of 100\n",
      "Epoch: 237 Step: 4503 Index:-0.5503 R2:0.9278 0.2395 0.4049 RMSE:0.3811 0.8933 1.0018 Tau:0.8260 0.3430 0.4324\n",
      "EarlyStopping counter: 71 out of 100\n",
      "Epoch: 238 Step: 4522 Index:-0.4487 R2:0.9199 0.2518 0.3979 RMSE:0.3351 0.8023 0.9187 Tau:0.8137 0.3536 0.4280\n",
      "EarlyStopping counter: 72 out of 100\n",
      "Epoch: 239 Step: 4541 Index:-0.5071 R2:0.9290 0.2271 0.4119 RMSE:0.3179 0.8502 0.9671 Tau:0.8104 0.3430 0.4379\n",
      "EarlyStopping counter: 73 out of 100\n",
      "Epoch: 240 Step: 4560 Index:-0.4779 R2:0.9278 0.2067 0.4146 RMSE:0.3083 0.8420 0.9287 Tau:0.8189 0.3641 0.4323\n",
      "EarlyStopping counter: 74 out of 100\n",
      "Epoch: 241 Step: 4579 Index:-0.5214 R2:0.9249 0.1916 0.3803 RMSE:0.3384 0.8222 0.9328 Tau:0.8234 0.3008 0.4256\n",
      "EarlyStopping counter: 75 out of 100\n",
      "Epoch: 242 Step: 4598 Index:-0.5961 R2:0.9132 0.2559 0.4020 RMSE:0.4732 0.9391 1.0606 Tau:0.8078 0.3430 0.4273\n",
      "EarlyStopping counter: 76 out of 100\n",
      "Epoch: 243 Step: 4617 Index:-0.5642 R2:0.8916 0.1641 0.3723 RMSE:0.3946 0.8649 0.9906 Tau:0.7583 0.3008 0.4294\n",
      "EarlyStopping counter: 77 out of 100\n",
      "Epoch: 244 Step: 4636 Index:-0.5252 R2:0.9179 0.2097 0.4095 RMSE:0.3294 0.8471 0.9503 Tau:0.8046 0.3219 0.4339\n",
      "EarlyStopping counter: 78 out of 100\n",
      "Epoch: 245 Step: 4655 Index:-0.5093 R2:0.9209 0.2209 0.4175 RMSE:0.3359 0.8312 0.9181 Tau:0.8091 0.3219 0.4337\n",
      "EarlyStopping counter: 79 out of 100\n",
      "Epoch: 246 Step: 4674 Index:-0.5190 R2:0.9168 0.2542 0.3842 RMSE:0.3903 0.8514 1.0022 Tau:0.8189 0.3325 0.4208\n",
      "EarlyStopping counter: 80 out of 100\n",
      "Epoch: 247 Step: 4693 Index:-0.4860 R2:0.9332 0.2408 0.4026 RMSE:0.3056 0.8185 0.9487 Tau:0.8293 0.3325 0.4354\n",
      "EarlyStopping counter: 81 out of 100\n",
      "Epoch: 248 Step: 4712 Index:-0.6303 R2:0.9325 0.1937 0.3920 RMSE:0.3882 0.9417 1.0827 Tau:0.8182 0.3113 0.4337\n",
      "EarlyStopping counter: 82 out of 100\n",
      "Epoch: 249 Step: 4731 Index:-0.6136 R2:0.9328 0.2285 0.4141 RMSE:0.3901 0.9460 1.0660 Tau:0.8156 0.3325 0.4217\n",
      "EarlyStopping counter: 83 out of 100\n",
      "Epoch: 250 Step: 4750 Index:-0.6082 R2:0.9070 0.2593 0.3998 RMSE:0.5762 1.0145 1.2351 Tau:0.7649 0.4063 0.4462\n",
      "EarlyStopping counter: 84 out of 100\n",
      "Epoch: 251 Step: 4769 Index:-0.6782 R2:0.8801 0.1443 0.4037 RMSE:0.4459 0.9895 1.0673 Tau:0.7707 0.3113 0.4187\n",
      "EarlyStopping counter: 85 out of 100\n",
      "Epoch: 252 Step: 4788 Index:-0.4349 R2:0.9078 0.2904 0.3830 RMSE:0.6004 0.7885 0.9098 Tau:0.7850 0.3536 0.4354\n",
      "EarlyStopping counter: 86 out of 100\n",
      "Epoch: 253 Step: 4807 Index:-0.6784 R2:0.9212 0.2648 0.4300 RMSE:0.5626 1.0320 1.1406 Tau:0.7974 0.3536 0.4399\n",
      "EarlyStopping counter: 87 out of 100\n",
      "Epoch: 254 Step: 4826 Index:-0.4539 R2:0.9065 0.2445 0.3946 RMSE:0.4055 0.7863 0.8845 Tau:0.8000 0.3325 0.4196\n",
      "EarlyStopping counter: 88 out of 100\n",
      "Epoch: 255 Step: 4845 Index:-0.5900 R2:0.9078 0.1879 0.4132 RMSE:0.3605 0.9013 0.9947 Tau:0.7922 0.3113 0.4396\n",
      "EarlyStopping counter: 89 out of 100\n",
      "Epoch: 256 Step: 4864 Index:-0.5162 R2:0.9236 0.2604 0.4098 RMSE:0.3776 0.8698 1.0036 Tau:0.8091 0.3536 0.4379\n",
      "EarlyStopping counter: 90 out of 100\n",
      "Epoch: 257 Step: 4883 Index:-0.5285 R2:0.9356 0.2082 0.3988 RMSE:0.3118 0.8610 0.9810 Tau:0.8221 0.3325 0.4314\n",
      "EarlyStopping counter: 91 out of 100\n",
      "Epoch: 258 Step: 4902 Index:-0.5531 R2:0.9351 0.2223 0.3986 RMSE:0.3322 0.8750 1.0131 Tau:0.8215 0.3219 0.4318\n",
      "EarlyStopping counter: 92 out of 100\n",
      "Epoch: 259 Step: 4921 Index:-0.5091 R2:0.9376 0.2014 0.3892 RMSE:0.3211 0.8204 0.9278 Tau:0.8254 0.3113 0.4306\n",
      "EarlyStopping counter: 93 out of 100\n",
      "Epoch: 260 Step: 4940 Index:-0.5556 R2:0.9314 0.1927 0.3919 RMSE:0.3002 0.8670 1.0043 Tau:0.8130 0.3113 0.4387\n",
      "EarlyStopping counter: 94 out of 100\n",
      "Epoch: 261 Step: 4959 Index:-0.5151 R2:0.9368 0.2102 0.3820 RMSE:0.2909 0.8264 0.9770 Tau:0.8228 0.3113 0.4323\n",
      "EarlyStopping counter: 95 out of 100\n",
      "Epoch: 262 Step: 4978 Index:-0.4670 R2:0.9328 0.2745 0.3958 RMSE:0.3329 0.8522 1.0367 Tau:0.8247 0.3852 0.4246\n",
      "EarlyStopping counter: 96 out of 100\n",
      "Epoch: 263 Step: 4997 Index:-0.5008 R2:0.9296 0.2427 0.3703 RMSE:0.3216 0.8122 0.9894 Tau:0.8234 0.3113 0.4251\n",
      "EarlyStopping counter: 97 out of 100\n",
      "Epoch: 264 Step: 5016 Index:-0.5388 R2:0.9205 0.2115 0.3994 RMSE:0.3259 0.8607 0.9987 Tau:0.8130 0.3219 0.4358\n",
      "EarlyStopping counter: 98 out of 100\n",
      "Epoch: 265 Step: 5035 Index:-0.5254 R2:0.9359 0.2462 0.3876 RMSE:0.3341 0.8684 1.0303 Tau:0.8280 0.3430 0.4285\n",
      "EarlyStopping counter: 99 out of 100\n",
      "Epoch: 266 Step: 5054 Index:-0.4827 R2:0.9381 0.2336 0.3725 RMSE:0.3005 0.8363 0.9975 Tau:0.8384 0.3536 0.4222\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Epoch: 267 Step: 5073 Index:-0.5571 R2:0.9362 0.2598 0.3890 RMSE:0.3689 0.9001 1.0669 Tau:0.8241 0.3430 0.4258\n"
     ]
    }
   ],
   "source": [
    "# train_f_list=[]\n",
    "# train_mse_list=[]\n",
    "# train_r2_list=[]\n",
    "# test_f_list=[]\n",
    "# test_mse_list=[]\n",
    "# test_r2_list=[]\n",
    "# val_f_list=[]\n",
    "# val_mse_list=[]\n",
    "# val_r2_list=[]\n",
    "# epoch_list=[]\n",
    "# train_predict_list=[]\n",
    "# test_predict_list=[]\n",
    "# val_predict_list=[]\n",
    "# train_y_list=[]\n",
    "# test_y_list=[]\n",
    "# val_y_list=[]\n",
    "# train_d_list=[]\n",
    "# test_d_list=[]\n",
    "# val_d_list=[]\n",
    "\n",
    "epoch = 0\n",
    "optimizer_list = [optimizer, optimizer_AFSE, optimizer_GRN]\n",
    "max_epoch = 1000\n",
    "while epoch < max_epoch:\n",
    "    train(model, amodel, gmodel, train_df, test_df, optimizer_list, loss_function, epoch)\n",
    "#     print(train_df.shape,test_df.shape)\n",
    "    train_d, train_f, train_r2, train_MSE, train_predict, reconstruction_loss, one_hot_loss, interger_loss,binary_loss = eval(model, amodel, gmodel, train_df,output_feature=True,return_GRN_loss=True)\n",
    "    train_predict = np.array(train_predict)\n",
    "    train_WTI = weighted_top_index(train_df, train_predict, len(train_df))\n",
    "    train_tau, _ = scipy.stats.kendalltau(train_predict,train_df[tasks[0]].values.astype(float).tolist())\n",
    "    val_d, val_f, val_r2, val_MSE, val_predict, val_reconstruction_loss, val_one_hot_loss, val_interger_loss,val_binary_loss = eval(model, amodel, gmodel, val_df,output_feature=True,return_GRN_loss=True)\n",
    "    val_predict = np.array(val_predict)\n",
    "    val_WTI = weighted_top_index(val_df, val_predict, len(val_df))\n",
    "    val_AP = AP(val_df, val_predict, len(val_df))\n",
    "    val_tau, _ = scipy.stats.kendalltau(val_predict,val_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "    test_d, test_f, test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df,output_feature=True)\n",
    "    test_predict = np.array(test_predict)\n",
    "    test_WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "#     test_AP = AP(test_df, test_predict, test_active)\n",
    "    test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "    \n",
    "    k_list = [int(len(test_df)*0.01),int(len(test_df)*0.03),int(len(test_df)*0.1),10,30,100]\n",
    "    topk_list =[]\n",
    "    false_positive_rate_list = []\n",
    "    for k in k_list:\n",
    "        a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "        topk_list.append(a)\n",
    "        false_positive_rate_list.append(b)\n",
    "    \n",
    "    epoch = epoch + 1\n",
    "    global_step = epoch * int(np.max([len(train_df),len(test_df)])/batch_size)\n",
    "    logger.add_scalar('val/WTI', val_WTI, global_step)\n",
    "    logger.add_scalar('val/AP', val_AP, global_step)\n",
    "    logger.add_scalar('val/r2', val_r2, global_step)\n",
    "    logger.add_scalar('val/RMSE', val_MSE**0.5, global_step)\n",
    "    logger.add_scalar('val/Tau', val_tau, global_step)\n",
    "#     logger.add_scalar('test/TAP', test_AP, global_step)\n",
    "    logger.add_scalar('test/r2', test_r2_a, global_step)\n",
    "    logger.add_scalar('test/RMSE', test_MSE_a**0.5, global_step)\n",
    "    logger.add_scalar('test/Tau', test_tau, global_step)\n",
    "    logger.add_scalar('val/GRN', reconstruction_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_one_hot', one_hot_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_interger', interger_loss, global_step)\n",
    "    logger.add_scalar('val/GRN_binary', binary_loss, global_step)\n",
    "    logger.add_scalar('test/EF0.01', topk_list[0], global_step)\n",
    "    logger.add_scalar('test/EF0.03', topk_list[1], global_step)\n",
    "    logger.add_scalar('test/EF0.1', topk_list[2], global_step)\n",
    "    logger.add_scalar('test/EF10', topk_list[3], global_step)\n",
    "    logger.add_scalar('test/EF30', topk_list[4], global_step)\n",
    "    logger.add_scalar('test/EF100', topk_list[5], global_step)\n",
    "    \n",
    "#     train_mse_list.append(train_MSE**0.5)\n",
    "#     train_r2_list.append(train_r2)\n",
    "#     val_mse_list.append(val_MSE**0.5)  \n",
    "#     val_r2_list.append(val_r2)\n",
    "#     train_f_list.append(train_f)\n",
    "#     val_f_list.append(val_f)\n",
    "#     test_f_list.append(test_f)\n",
    "#     epoch_list.append(epoch)\n",
    "#     train_predict_list.append(train_predict.flatten())\n",
    "#     test_predict_list.append(test_predict.flatten())\n",
    "#     val_predict_list.append(val_predict.flatten())\n",
    "#     train_y_list.append(train_df[tasks[0]].values)\n",
    "#     val_y_list.append(val_df[tasks[0]].values)\n",
    "#     test_y_list.append(test_df[tasks[0]].values)\n",
    "#     train_d_list.append(train_d)\n",
    "#     val_d_list.append(val_d)\n",
    "#     test_d_list.append(test_d)\n",
    "\n",
    "    stop_index = - val_MSE**0.5 + val_tau \n",
    "    early_stop = stopper.step(stop_index, model)\n",
    "    early_stop = stopper_afse.step(stop_index, amodel, if_print=False)\n",
    "    early_stop = stopper_generate.step(stop_index, gmodel, if_print=False)\n",
    "#     print('epoch {:d}/{:d}, validation {} {:.4f}, {} {:.4f},best validation {r2} {:.4f}'.format(epoch, total_epoch, 'r2', val_r2, 'mse:',val_MSE, stopper.best_score))\n",
    "    print('Epoch:',epoch, 'Step:', global_step, 'Index:%.4f'%stop_index, 'R2:%.4f'%train_r2,'%.4f'%val_r2,'%.4f'%test_r2_a, 'RMSE:%.4f'%train_MSE**0.5, '%.4f'%val_MSE**0.5, \n",
    "          '%.4f'%test_MSE_a**0.5, 'Tau:%.4f'%train_tau,'%.4f'%val_tau,'%.4f'%test_tau)#, 'Tau:%.4f'%val_tau,'%.4f'%test_tau,'GRN:%.4f'%reconstruction_loss,'%.4f'%val_reconstruction_loss\n",
    "    if early_stop:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)\n",
    "stopper_afse.load_checkpoint(amodel)\n",
    "stopper_generate.load_checkpoint(gmodel)\n",
    "    \n",
    "test_r2, test_MSE, test_predict = eval(model, amodel, gmodel, test_df)\n",
    "test_r2_a, test_MSE_a, test_predict_a = eval(model, amodel, gmodel, test_df[:test_active])\n",
    "test_r2_ina, test_MSE_ina, test_predict_ina = eval(model, amodel, gmodel, test_df[test_active:].reset_index(drop=True))\n",
    "    \n",
    "test_predict = np.array(test_predict)\n",
    "test_tau, _ = scipy.stats.kendalltau(test_predict,test_df[tasks[0]].values.astype(float).tolist())\n",
    "\n",
    "k_list = [int(len(test_df)*0.01),int(len(test_df)*0.05),int(len(test_df)*0.1),int(len(test_df)*0.15),int(len(test_df)*0.2),int(len(test_df)*0.25),\n",
    "          int(len(test_df)*0.3),int(len(test_df)*0.4),int(len(test_df)*0.5),50,100,150,200,250,300]\n",
    "topk_list =[]\n",
    "false_positive_rate_list = []\n",
    "for k in k_list:\n",
    "    a,b = topk_acc_recall(test_df, test_predict, k, test_active, False, epoch)\n",
    "    topk_list.append(a)\n",
    "    false_positive_rate_list.append(b)\n",
    "WTI = weighted_top_index(test_df, test_predict, test_active)\n",
    "ap = AP(test_df, test_predict, test_active)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 267 r2:0.4106 RMSE:0.8595 WTI:0.3241 AP:0.6447 Tau:0.4310 \n",
      " \n",
      " Top-1:0.0000 Top-1-fp:0.0000 \n",
      " Top-5:0.5556 Top-5-fp:0.0000 \n",
      " Top-10:0.5263 Top-10-fp:0.1053 \n",
      " Top-15:0.5172 Top-15-fp:0.0690 \n",
      " Top-20:0.6667 Top-20-fp:0.1026 \n",
      " Top-25:0.6667 Top-25-fp:0.2083 \n",
      " Top-30:0.6552 Top-30-fp:0.2586 \n",
      " Top-40:0.6538 Top-40-fp:0.3462 \n",
      " Top-50:0.7375 Top-50-fp:0.3918 \n",
      " \n",
      " Top50:0.6600 Top50-fp:0.2200 \n",
      " Top100:0.7750 Top100-fp:0.3800 \n",
      " Top150:0.9000 Top150-fp:0.5200 \n",
      " Top200:1.0000 Top200-fp:0.5750 \n",
      " Top250:1.0000 Top250-fp:0.4600 \n",
      " Top300:1.0000 Top300-fp:0.3833 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(' epoch:',epoch,'r2:%.4f'%test_r2_a,'RMSE:%.4f'%test_MSE_a**0.5,'WTI:%.4f'%WTI,'AP:%.4f'%ap,'Tau:%.4f'%test_tau,'\\n','\\n',\n",
    "      'Top-1:%.4f'%topk_list[0],'Top-1-fp:%.4f'%false_positive_rate_list[0],'\\n',\n",
    "      'Top-5:%.4f'%topk_list[1],'Top-5-fp:%.4f'%false_positive_rate_list[1],'\\n',\n",
    "      'Top-10:%.4f'%topk_list[2],'Top-10-fp:%.4f'%false_positive_rate_list[2],'\\n',\n",
    "      'Top-15:%.4f'%topk_list[3],'Top-15-fp:%.4f'%false_positive_rate_list[3],'\\n',\n",
    "      'Top-20:%.4f'%topk_list[4],'Top-20-fp:%.4f'%false_positive_rate_list[4],'\\n',\n",
    "      'Top-25:%.4f'%topk_list[5],'Top-25-fp:%.4f'%false_positive_rate_list[5],'\\n',\n",
    "      'Top-30:%.4f'%topk_list[6],'Top-30-fp:%.4f'%false_positive_rate_list[6],'\\n',\n",
    "      'Top-40:%.4f'%topk_list[7],'Top-40-fp:%.4f'%false_positive_rate_list[7],'\\n',\n",
    "      'Top-50:%.4f'%topk_list[8],'Top-50-fp:%.4f'%false_positive_rate_list[8],'\\n','\\n',\n",
    "      'Top50:%.4f'%topk_list[9],'Top50-fp:%.4f'%false_positive_rate_list[9],'\\n',\n",
    "      'Top100:%.4f'%topk_list[10],'Top100-fp:%.4f'%false_positive_rate_list[10],'\\n',\n",
    "      'Top150:%.4f'%topk_list[11],'Top150-fp:%.4f'%false_positive_rate_list[11],'\\n',\n",
    "      'Top200:%.4f'%topk_list[12],'Top200-fp:%.4f'%false_positive_rate_list[12],'\\n',\n",
    "      'Top250:%.4f'%topk_list[13],'Top250-fp:%.4f'%false_positive_rate_list[13],'\\n',\n",
    "      'Top300:%.4f'%topk_list[14],'Top300-fp:%.4f'%false_positive_rate_list[14],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('target_file:',train_filename)\n",
    "# print('inactive_file:',test_filename)\n",
    "# np.savez(result_dir, epoch_list, train_f_list, train_d_list, \n",
    "#          train_predict_list, train_y_list, val_f_list, val_d_list, val_predict_list, val_y_list, test_f_list, \n",
    "#          test_d_list, test_predict_list, test_y_list)\n",
    "# sim_space = np.load(result_dir+'.npz')\n",
    "# print(sim_space['arr_10'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = loss_function(mol_prediction,y)\n",
    "#             loss.backward(retain_graph=True)\n",
    "#             optimizer_AFSE.zero_grad()\n",
    "#             punish_lr = torch.norm(torch.mean(eps.grad,0))\n",
    "\n",
    "# loss =  regression_loss + vat_loss + test_vat_loss\n",
    "\n",
    "#         init_lr = 1e-4\n",
    "#         max_lr = 10**-(init_lr-1)\n",
    "#         conv_lr = conv_lr - conv_lr**2 + 0.1 * punish_lr\n",
    "#         if conv_lr < max_lr:\n",
    "#             for param_group in optimizer_AFSE.param_groups:\n",
    "#                 param_group[\"lr\"] = conv_lr.detach()\n",
    "#                 AFSE_lr = conv_lr    \n",
    "#         else:\n",
    "#             for param_group in optimizer_AFSE.param_groups:\n",
    "#                 param_group[\"lr\"] = max_lr\n",
    "#                 AFSE_lr = max_lr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
